# x86 instruction set description version 0.2, 2023-03-05
# Based on Intel Instruction Set Reference #325383-078US, December 2022.
# https://golang.org/x/arch/x86/x86spec
"AAA","AAA","aaa","37","V","I","","","","","","ASCII adjust AL after addition."
"AAD","AAD","aad","D5 0A","V","I","","pseudo","","","","ASCII adjust AX before division."
"AAD imm8","AAD imm8","aad imm8","D5 ib","V","I","","","r","","","Adjust AX before division to number base imm8."
"AAM","AAM","aam","D4 0A","V","I","","pseudo","","","","ASCII adjust AX after multiply."
"AAM imm8","AAM imm8","aam imm8","D4 ib","V","I","","","r","","","Adjust AX after multiply to number base imm8."
"AAS","AAS","aas","3F","V","I","","","","","","ASCII adjust AL after subtraction."
"ADC AL, imm8","ADCB imm8, AL","adcb imm8, AL","14 ib","V","V","","","","Y","8","Add with carry imm8 to AL."
"ADC AX, imm16","ADCW imm16, AX","adcw imm16, AX","15 iw","V","V","","operand16","","Y","16","Add with carry imm16 to AX."
"ADC EAX, imm32","ADCL imm32, EAX","adcl imm32, EAX","15 id","V","V","","operand32","","Y","32","Add with carry imm32 to EAX."
"ADC RAX, imm32","ADCQ imm32, RAX","adcq imm32, RAX","REX.W 15 id","N.E.","V","","","","Y","64","Add with carry imm32 sign extended to 64- bits to RAX."
"ADC r/m16, imm16","ADCW imm16, r/m16","adcw imm16, r/m16","81 /2 iw","V","V","","operand16","","Y","16","Add with carry imm16 to r/m16."
"ADC r/m16, imm8","ADCW imm8, r/m16","adcw imm8, r/m16","83 /2 ib","V","V","","operand16","","Y","16","Add with CF sign-extended imm8 to r/m16."
"ADC r/m16, r16","ADCW r16, r/m16","adcw r16, r/m16","11 /r","V","V","","operand16","","Y","16","Add with carry r16 to r/m16."
"ADC r/m32, imm32","ADCL imm32, r/m32","adcl imm32, r/m32","81 /2 id","V","V","","operand32","","Y","32","Add with CF imm32 to r/m32."
"ADC r/m32, imm8","ADCL imm8, r/m32","adcl imm8, r/m32","83 /2 ib","V","V","","operand32","","Y","32","Add with CF sign-extended imm8 into r/m32."
"ADC r/m32, r32","ADCL r32, r/m32","adcl r32, r/m32","11 /r","V","V","","operand32","","Y","32","Add with CF r32 to r/m32."
"ADC r/m64, imm32","ADCQ imm32, r/m64","adcq imm32, r/m64","REX.W 81 /2 id","N.E.","V","","","","Y","64","Add with CF imm32 sign extended to 64-bits to r/m64."
"ADC r/m64, imm8","ADCQ imm8, r/m64","adcq imm8, r/m64","REX.W 83 /2 ib","N.E.","V","","","","Y","64","Add with CF sign-extended imm8 into r/m64."
"ADC r/m64, r64","ADCQ r64, r/m64","adcq r64, r/m64","REX.W 11 /r","N.E.","V","","","","Y","64","Add with CF r64 to r/m64."
"ADC r/m8, imm8","ADCB imm8, r/m8","adcb imm8, r/m8","80 /2 ib","V","V","","","","Y","8","Add with carry imm8 to r/m8."
"ADC r/m8, imm8","ADCB imm8, r/m8","adcb imm8, r/m8","REX 80 /2 ib","N.E.","V","","pseudo64","","Y","8","Add with carry imm8 to r/m8."
"ADC r/m8, r8","ADCB r8, r/m8","adcb r8, r/m8","10 /r","V","V","","","","Y","8","Add with carry byte register to r/m8."
"ADC r/m8, r8","ADCB r8, r/m8","adcb r8, r/m8","REX 10 /r","N.E.","V","","pseudo64","","Y","8","Add with carry byte register to r/m64."
"ADC r16, r/m16","ADCW r/m16, r16","adcw r/m16, r16","13 /r","V","V","","operand16","","Y","16","Add with carry r/m16 to r16."
"ADC r32, r/m32","ADCL r/m32, r32","adcl r/m32, r32","13 /r","V","V","","operand32","","Y","32","Add with CF r/m32 to r32."
"ADC r64, r/m64","ADCQ r/m64, r64","adcq r/m64, r64","REX.W 13 /r","N.E.","V","","","","Y","64","Add with CF r/m64 to r64."
"ADC r8, r/m8","ADCB r/m8, r8","adcb r/m8, r8","12 /r","V","V","","","","Y","8","Add with carry r/m8 to byte register."
"ADC r8, r/m8","ADCB r/m8, r8","adcb r/m8, r8","REX 12 /r","N.E.","V","","pseudo64","","Y","8","Add with carry r/m64 to byte register."
"ADCX r32, r/m32","ADCXL r/m32, r32","adcxl r/m32, r32","66 0F 38 F6 /r","V","V","ADX","operand16,operand32","","Y","32","Unsigned addition of r32 with CF, r/m32 to r32, writes CF."
"ADCX r64, r/m64","ADCXQ r/m64, r64","adcxq r/m64, r64","66 REX.W 0F 38 F6 /r","N.E.","V","ADX","","","Y","64","Unsigned addition of r64 with CF, r/m64 to r64, writes CF."
"ADD AL, imm8","ADDB imm8, AL","addb imm8, AL","04 ib","V","V","","","","Y","8","Add imm8 to AL."
"ADD AX, imm16","ADDW imm16, AX","addw imm16, AX","05 iw","V","V","","operand16","","Y","16","Add imm16 to AX."
"ADD EAX, imm32","ADDL imm32, EAX","addl imm32, EAX","05 id","V","V","","operand32","","Y","32","Add imm32 to EAX."
"ADD RAX, imm32","ADDQ imm32, RAX","addq imm32, RAX","REX.W 05 id","N.E.","V","","","","Y","64","Add imm32 sign-extended to 64-bits to RAX."
"ADD r/m16, imm16","ADDW imm16, r/m16","addw imm16, r/m16","81 /0 iw","V","V","","operand16","","Y","16","Add imm16 to r/m16."
"ADD r/m16, imm8","ADDW imm8, r/m16","addw imm8, r/m16","83 /0 ib","V","V","","operand16","","Y","16","Add sign-extended imm8 to r/m16."
"ADD r/m16, r16","ADDW r16, r/m16","addw r16, r/m16","01 /r","V","V","","operand16","","Y","16","Add r16 to r/m16."
"ADD r/m32, imm32","ADDL imm32, r/m32","addl imm32, r/m32","81 /0 id","V","V","","operand32","","Y","32","Add imm32 to r/m32."
"ADD r/m32, imm8","ADDL imm8, r/m32","addl imm8, r/m32","83 /0 ib","V","V","","operand32","","Y","32","Add sign-extended imm8 to r/m32."
"ADD r/m32, r32","ADDL r32, r/m32","addl r32, r/m32","01 /r","V","V","","operand32","","Y","32","Add r32 to r/m32."
"ADD r/m64, imm32","ADDQ imm32, r/m64","addq imm32, r/m64","REX.W 81 /0 id","N.E.","V","","","","Y","64","Add imm32 sign-extended to 64-bits to r/m64."
"ADD r/m64, imm8","ADDQ imm8, r/m64","addq imm8, r/m64","REX.W 83 /0 ib","N.E.","V","","","","Y","64","Add sign-extended imm8 to r/m64."
"ADD r/m64, r64","ADDQ r64, r/m64","addq r64, r/m64","REX.W 01 /r","N.E.","V","","","","Y","64","Add r64 to r/m64."
"ADD r/m8, imm8","ADDB imm8, r/m8","addb imm8, r/m8","80 /0 ib","V","V","","","","Y","8","Add imm8 to r/m8."
"ADD r/m8, imm8","ADDB imm8, r/m8","addb imm8, r/m8","REX 80 /0 ib","N.E.","V","","pseudo64","","Y","8","Add sign-extended imm8 to r/m8."
"ADD r/m8, r8","ADDB r8, r/m8","addb r8, r/m8","00 /r","V","V","","","","Y","8","Add r8 to r/m8."
"ADD r/m8, r8","ADDB r8, r/m8","addb r8, r/m8","REX 00 /r","N.E.","V","","pseudo64","","Y","8","Add r8 to r/m8."
"ADD r16, r/m16","ADDW r/m16, r16","addw r/m16, r16","03 /r","V","V","","operand16","","Y","16","Add r/m16 to r16."
"ADD r32, r/m32","ADDL r/m32, r32","addl r/m32, r32","03 /r","V","V","","operand32","","Y","32","Add r/m32 to r32."
"ADD r64, r/m64","ADDQ r/m64, r64","addq r/m64, r64","REX.W 03 /r","N.E.","V","","","","Y","64","Add r/m64 to r64."
"ADD r8, r/m8","ADDB r/m8, r8","addb r/m8, r8","02 /r","V","V","","","","Y","8","Add r/m8 to r8."
"ADD r8, r/m8","ADDB r/m8, r8","addb r/m8, r8","REX 02 /r","N.E.","V","","pseudo64","","Y","8","Add r/m8 to r8."
"ADDPD xmm1, xmm2/m128","ADDPD xmm2/m128, xmm1","addpd xmm2/m128, xmm1","66 0F 58 /r","V","V","SSE2","","","","","Add packed double precision floating-point values from xmm2/mem to xmm1 and store result in xmm1."
"ADDPS xmm1, xmm2/m128","ADDPS xmm2/m128, xmm1","addps xmm2/m128, xmm1","NP 0F 58 /r","V","V","SSE","","","","","Add packed single precision floating-point values from xmm2/m128 to xmm1 and store result in xmm1."
"ADDSD xmm1, xmm2/m64","ADDSD xmm2/m64, xmm1","addsd xmm2/m64, xmm1","F2 0F 58 /r","V","V","SSE2","","","","","Add the low double precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1."
"ADDSS xmm1, xmm2/m32","ADDSS xmm2/m32, xmm1","addss xmm2/m32, xmm1","F3 0F 58 /r","V","V","SSE","","","","","Add the low single precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1."
"ADDSUBPD xmm1, xmm2/m128","ADDSUBPD xmm2/m128, xmm1","addsubpd xmm2/m128, xmm1","66 0F D0 /r","V","V","SSE3","","","","","Add/subtract double precision floating-point values from xmm2/m128 to xmm1."
"ADDSUBPS xmm1, xmm2/m128","ADDSUBPS xmm2/m128, xmm1","addsubps xmm2/m128, xmm1","F2 0F D0 /r","V","V","SSE3","","","","","Add/subtract single precision floating-point values from xmm2/m128 to xmm1."
"ADOX r32, r/m32","ADOXL r/m32, r32","adoxl r/m32, r32","F3 0F 38 F6 /r","V","V","ADX","operand16,operand32","","Y","32","Unsigned addition of r32 with OF, r/m32 to r32, writes OF."
"ADOX r64, r/m64","ADOXQ r/m64, r64","adoxq r/m64, r64","F3 REX.W 0F 38 F6 /r","N.E.","V","ADX","","","Y","64","Unsigned addition of r64 with OF, r/m64 to r64, writes OF."
"AESDEC xmm1, xmm2/m128","AESDEC xmm2/m128, xmm1","aesdec xmm2/m128, xmm1","66 0F 38 DE /r","V","V","AES","","","","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm1 with one 128-bit round key from xmm2/m128."
"AESDEC128KL xmm, m384","AESDEC128KL m384, xmm","aesdec128kl m384, xmm","F3 0F 38 DD !(11):rrr:bbb","V","V","AESKLE","","","","","Decrypt xmm using 128-bit AES key indicated by han- dle at m384 and store result in xmm."
"AESDEC256KL xmm, m512","AESDEC256KL m512, xmm","aesdec256kl m512, xmm","F3 0F 38 DF !(11):rrr:bbb","V","V","AESKLE","","","","","Decrypt xmm using 256-bit AES key indicated by han- dle at m512 and store result in xmm."
"AESDECLAST xmm1, xmm2/m128","AESDECLAST xmm2/m128, xmm1","aesdeclast xmm2/m128, xmm1","66 0F 38 DF /r","V","V","AES","","","","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm1 with one 128-bit round key from xmm2/m128."
"AESENC xmm1, xmm2/m128","AESENC xmm2/m128, xmm1","aesenc xmm2/m128, xmm1","66 0F 38 DC /r","V","V","AES","","","","","Perform one round of an AES encryption flow, using one 128-bit data (state) from xmm1 with one 128-bit round key from xmm2/m128."
"AESENC128KL xmm, m384","AESENC128KL m384, xmm","aesenc128kl m384, xmm","F3 0F 38 DC !(11):rrr:bbb","V","V","AESKLE","","","","","Encrypt xmm using 128-bit AES key indicated by han- dle at m384 and store result in xmm."
"AESENC256KL xmm, m512","AESENC256KL m512, xmm","aesenc256kl m512, xmm","F3 0F 38 DE !(11):rrr:bbb","V","V","AESKLE","","","","","Encrypt xmm using 256-bit AES key indicated by han- dle at m512 and store result in xmm."
"AESENCLAST xmm1, xmm2/m128","AESENCLAST xmm2/m128, xmm1","aesenclast xmm2/m128, xmm1","66 0F 38 DD /r","V","V","AES","","","","","Perform the last round of an AES encryption flow, using one 128-bit data (state) from xmm1 with one 128-bit round key from xmm2/m128."
"AESIMC xmm1, xmm2/m128","AESIMC xmm2/m128, xmm1","aesimc xmm2/m128, xmm1","66 0F 38 DB /r","V","V","AES","","","","","Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1."
"AESKEYGENASSIST xmm1, xmm2/m128, imm8","AESKEYGENASSIST imm8, xmm2/m128, xmm1","aeskeygenassist imm8, xmm2/m128, xmm1","66 0F 3A DF /r ib","V","V","AES","","","","","Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1."
"AND AL, imm8","ANDB imm8, AL","andb imm8, AL","24 ib","V","V","","","","Y","8","AL AND imm8."
"AND AX, imm16","ANDW imm16, AX","andw imm16, AX","25 iw","V","V","","operand16","","Y","16","AX AND imm16."
"AND EAX, imm32","ANDL imm32, EAX","andl imm32, EAX","25 id","V","V","","operand32","","Y","32","EAX AND imm32."
"AND RAX, imm32","ANDQ imm32, RAX","andq imm32, RAX","REX.W 25 id","N.E.","V","","","","Y","64","RAX AND imm32 sign-extended to 64-bits."
"AND r/m16, imm16","ANDW imm16, r/m16","andw imm16, r/m16","81 /4 iw","V","V","","operand16","","Y","16","r/m16 AND imm16."
"AND r/m16, imm8","ANDW imm8, r/m16","andw imm8, r/m16","83 /4 ib","V","V","","operand16","","Y","16","r/m16 AND imm8 (sign-extended)."
"AND r/m16, r16","ANDW r16, r/m16","andw r16, r/m16","21 /r","V","V","","operand16","","Y","16","r/m16 AND r16."
"AND r/m32, imm32","ANDL imm32, r/m32","andl imm32, r/m32","81 /4 id","V","V","","operand32","","Y","32","r/m32 AND imm32."
"AND r/m32, imm8","ANDL imm8, r/m32","andl imm8, r/m32","83 /4 ib","V","V","","operand32","","Y","32","r/m32 AND imm8 (sign-extended)."
"AND r/m32, r32","ANDL r32, r/m32","andl r32, r/m32","21 /r","V","V","","operand32","","Y","32","r/m32 AND r32."
"AND r/m64, imm32","ANDQ imm32, r/m64","andq imm32, r/m64","REX.W 81 /4 id","N.E.","V","","","","Y","64","r/m64 AND imm32 sign extended to 64-bits."
"AND r/m64, imm8","ANDQ imm8, r/m64","andq imm8, r/m64","REX.W 83 /4 ib","N.E.","V","","","","Y","64","r/m64 AND imm8 (sign-extended)."
"AND r/m64, r64","ANDQ r64, r/m64","andq r64, r/m64","REX.W 21 /r","N.E.","V","","","","Y","64","r/m64 AND r32."
"AND r/m8, imm8","ANDB imm8, r/m8","andb imm8, r/m8","80 /4 ib","V","V","","","","Y","8","r/m8 AND imm8."
"AND r/m8, imm8","ANDB imm8, r/m8","andb imm8, r/m8","REX 80 /4 ib","N.E.","V","","pseudo64","","Y","8","r/m8 AND imm8."
"AND r/m8, r8","ANDB r8, r/m8","andb r8, r/m8","20 /r","V","V","","","","Y","8","r/m8 AND r8."
"AND r/m8, r8","ANDB r8, r/m8","andb r8, r/m8","REX 20 /r","N.E.","V","","pseudo64","","Y","8","r/m64 AND r8 (sign-extended)."
"AND r16, r/m16","ANDW r/m16, r16","andw r/m16, r16","23 /r","V","V","","operand16","","Y","16","r16 AND r/m16."
"AND r32, r/m32","ANDL r/m32, r32","andl r/m32, r32","23 /r","V","V","","operand32","","Y","32","r32 AND r/m32."
"AND r64, r/m64","ANDQ r/m64, r64","andq r/m64, r64","REX.W 23 /r","N.E.","V","","","","Y","64","r64 AND r/m64."
"AND r8, r/m8","ANDB r/m8, r8","andb r/m8, r8","22 /r","V","V","","","","Y","8","r8 AND r/m8."
"AND r8, r/m8","ANDB r/m8, r8","andb r/m8, r8","REX 22 /r","N.E.","V","","pseudo64","","Y","8","r/m64 AND r8 (sign-extended)."
"ANDN r32a, r32b, r/m32","ANDNL r/m32, r32b, r32a","andnl r/m32, r32b, r32a","VEX.LZ.0F38.W0 F2 /r","V","V","BMI1","","","Y","32","Bitwise AND of inverted r32b with r/m32, store result in r32a."
"ANDN r64a, r64b, r/m64","ANDNQ r/m64, r64b, r64a","andnq r/m64, r64b, r64a","VEX.LZ.0F38.W1 F2 /r","N.E.","V","BMI1","","","Y","64","Bitwise AND of inverted r64b with r/m64, store result in r64a."
"ANDNPD xmm1, xmm2/m128","ANDNPD xmm2/m128, xmm1","andnpd xmm2/m128, xmm1","66 0F 55 /r","V","V","SSE2","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in xmm1 and xmm2/mem."
"ANDNPS xmm1, xmm2/m128","ANDNPS xmm2/m128, xmm1","andnps xmm2/m128, xmm1","NP 0F 55 /r","V","V","SSE","","","","","Return the bitwise logical AND NOT of packed single precision floating-point values in xmm1 and xmm2/mem."
"ANDPD xmm1, xmm2/m128","ANDPD xmm2/m128, xmm1","andpd xmm2/m128, xmm1","66 0F 54 /r","V","V","SSE2","","","","","Return the bitwise logical AND of packed double precision floating-point values in xmm1 and xmm2/mem."
"ANDPS xmm1, xmm2/m128","ANDPS xmm2/m128, xmm1","andps xmm2/m128, xmm1","NP 0F 54 /r","V","V","SSE","","","","","Return the bitwise logical AND of packed single precision floating-point values in xmm1 and xmm2/mem."
"ARPL r/m16, r16","ARPL r16, r/m16","arpl r16, r/m16","63 /r","V","N.E.","","","","","","Adjust RPL of r/m16 to not less than RPL of r16."
"BEXTR r32a, r/m32, r32b","BEXTRL r32b, r/m32, r32a","bextrl r32b, r/m32, r32a","VEX.LZ.0F38.W0 F7 /r","V","V","BMI1","","","Y","32","Contiguous bitwise extract from r/m32 using r32b as control; store result in r32a."
"BEXTR r64a, r/m64, r64b","BEXTRQ r64b, r/m64, r64a","bextrq r64b, r/m64, r64a","VEX.LZ.0F38.W1 F7 /r","N.E.","V","BMI1","","","Y","64","Contiguous bitwise extract from r/m64 using r64b as control; store result in r64a."
"BLENDPD xmm1, xmm2/m128, imm8","BLENDPD imm8, xmm2/m128, xmm1","blendpd imm8, xmm2/m128, xmm1","66 0F 3A 0D /r ib","V","V","SSE4_1","","","","","Select packed double precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"BLENDPS xmm1, xmm2/m128, imm8","BLENDPS imm8, xmm2/m128, xmm1","blendps imm8, xmm2/m128, xmm1","66 0F 3A 0C /r ib","V","V","SSE4_1","","","","","Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"BLENDVPD xmm1, xmm2/m128, <XMM0>","BLENDVPD <XMM0>, xmm2/m128, xmm1","blendvpd <XMM0>, xmm2/m128, xmm1","66 0F 38 15 /r","V","V","SSE4_1","","","","","Select packed double precision floating-point values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1."
"BLENDVPS xmm1, xmm2/m128, <XMM0>","BLENDVPS <XMM0>, xmm2/m128, xmm1","blendvps <XMM0>, xmm2/m128, xmm1","66 0F 38 14 /r","V","V","SSE4_1","","","","","Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1."
"BLSI r32, r/m32","BLSIL r/m32, r32","blsil r/m32, r32","VEX.LZ.0F38.W0 F3 /3","V","V","BMI1","","","Y","32","Extract lowest set bit from r/m32 and set that bit in r32."
"BLSI r64, r/m64","BLSIQ r/m64, r64","blsiq r/m64, r64","VEX.LZ.0F38.W1 F3 /3","N.E.","V","BMI1","","","Y","64","Extract lowest set bit from r/m64, and set that bit in r64."
"BLSMSK r32, r/m32","BLSMSKL r/m32, r32","blsmskl r/m32, r32","VEX.LZ.0F38.W0 F3 /2","V","V","BMI1","","","Y","32","Set all lower bits in r32 to “1” starting from bit 0 to lowest set bit in r/m32."
"BLSMSK r64, r/m64","BLSMSKQ r/m64, r64","blsmskq r/m64, r64","VEX.LZ.0F38.W1 F3 /2","N.E.","V","BMI1","","","Y","64","Set all lower bits in r64 to “1” starting from bit 0 to lowest set bit in r/m64."
"BLSR r32, r/m32","BLSRL r/m32, r32","blsrl r/m32, r32","VEX.LZ.0F38.W0 F3 /1","V","V","BMI1","","","Y","32","Reset lowest set bit of r/m32, keep all other bits of r/m32 and write result to r32."
"BLSR r64, r/m64","BLSRQ r/m64, r64","blsrq r/m64, r64","VEX.LZ.0F38.W1 F3 /1","N.E.","V","BMI1","","","Y","64","Reset lowest set bit of r/m64, keep all other bits of r/m64 and write result to r64."
"BNDCL bnd, r/m32","BNDCL r/m32, bnd","bndcl r/m32, bnd","F3 0F 1A /r","V","N.E.","MPX","","","","","Generate a #BR if the address in r/m32 is lower than the lower bound in bnd.LB."
"BNDCL bnd, r/m64","BNDCL r/m64, bnd","bndcl r/m64, bnd","F3 0F 1A /r","N.E.","V","MPX","","","","","Generate a #BR if the address in r/m64 is lower than the lower bound in bnd.LB."
"BNDCN bnd, r/m32","BNDCN r/m32, bnd","bndcn r/m32, bnd","F2 0F 1B /r","V","N.E.","MPX","","","","","Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form)."
"BNDCN bnd, r/m64","BNDCN r/m64, bnd","bndcn r/m64, bnd","F2 0F 1B /r","N.E.","V","MPX","","","","","Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form)."
"BNDCU bnd, r/m32","BNDCU r/m32, bnd","bndcu r/m32, bnd","F2 0F 1A /r","V","N.E.","MPX","","","","","Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form)."
"BNDCU bnd, r/m64","BNDCU r/m64, bnd","bndcu r/m64, bnd","F2 0F 1A /r","N.E.","V","MPX","","","","","Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form)."
"BNDLDX bnd1, mib","BNDLDX mib, bnd1","bndldx mib, bnd1","NP 0F 1A /r","V","V","MPX","","w,r","","","Load the bounds stored in a bound table entry (BTE) into bnd with address translation using the base of mib and conditional on the index of mib matching the pointer value in the BTE."
"BNDMK bnd, m32","BNDMK m32, bnd","bndmk m32, bnd","F3 0F 1B /r","V","N.E.","MPX","","","","","Make lower and upper bounds from m32 and store them in bnd."
"BNDMK bnd, m64","BNDMK m64, bnd","bndmk m64, bnd","F3 0F 1B /r","N.E.","V","MPX","","","","","Make lower and upper bounds from m64 and store them in bnd."
"BNDMOV bnd1, bnd2/m128","BNDMOV bnd2/m128, bnd1","bndmov bnd2/m128, bnd1","66 0F 1A /r","N.E.","V","MPX","","","","","Move lower and upper bound from bnd2/m128 to bound register bnd1."
"BNDMOV bnd1, bnd2/m64","BNDMOV bnd2/m64, bnd1","bndmov bnd2/m64, bnd1","66 0F 1A /r","V","N.E.","MPX","","","","","Move lower and upper bound from bnd2/m64 to bound register bnd1."
"BNDMOV bnd1/m128, bnd2","BNDMOV bnd2, bnd1/m128","bndmov bnd2, bnd1/m128","66 0F 1B /r","N.E.","V","MPX","","","","","Move lower and upper bound from bnd2 to bound register bnd1/m128."
"BNDMOV bnd1/m64, bnd2","BNDMOV bnd2, bnd1/m64","bndmov bnd2, bnd1/m64","66 0F 1B /r","V","N.E.","MPX","","","","","Move lower and upper bound from bnd2 to bnd1/m64."
"BNDSTX mib, bnd1","BNDSTX bnd1, mib","bndstx bnd1, mib","NP 0F 1B /r","V","V","MPX","","r,r","","","Store the bounds in bnd and the pointer value in the index regis- ter of mib to a bound table entry (BTE) with address translation using the base of mib."
"BOUND r16, m16&16","BOUNDW m16&16, r16","boundw r16, m16&16","62 /r","V","I","","operand16","","Y","16","Check if r16 (array index) is within bounds specified by m16&16."
"BOUND r32, m32&32","BOUNDL m32&32, r32","boundl r32, m32&32","62 /r","V","I","","operand32","","Y","32","Check if r32 (array index) is within bounds specified by m32&32."
"BSF r16, r/m16","BSFW r/m16, r16","bsfw r/m16, r16","0F BC /r","V","V","","operand16","","Y","16","Bit scan forward on r/m16."
"BSF r32, r/m32","BSFL r/m32, r32","bsfl r/m32, r32","0F BC /r","V","V","","operand32","","Y","32","Bit scan forward on r/m32."
"BSF r64, r/m64","BSFQ r/m64, r64","bsfq r/m64, r64","REX.W 0F BC /r","N.E.","V","","","","Y","64","Bit scan forward on r/m64."
"BSR r16, r/m16","BSRW r/m16, r16","bsrw r/m16, r16","0F BD /r","V","V","","operand16","","Y","16","Bit scan reverse on r/m16."
"BSR r32, r/m32","BSRL r/m32, r32","bsrl r/m32, r32","0F BD /r","V","V","","operand32","","Y","32","Bit scan reverse on r/m32."
"BSR r64, r/m64","BSRQ r/m64, r64","bsrq r/m64, r64","REX.W 0F BD /r","N.E.","V","","","","Y","64","Bit scan reverse on r/m64."
"BSWAP r16op","BSWAPW r16op","bswap r16op","0F C8+rd","V","V","","operand16","rw","Y","16",""
"BSWAP r32","BSWAPL r32","bswap r32","0F C8+rd","V","V","","operand16,operand32","","Y","32","Reverses the byte order of a 32-bit register."
"BSWAP r64","BSWAPQ r64","bswap r64","REX.W 0F C8+rd","N.E.","V","","","","Y","64","Reverses the byte order of a 64-bit register."
"BT r/m16, imm8","BTW imm8, r/m16","btw imm8, r/m16","0F BA /4 ib","V","V","","operand16","","Y","16","Store selected bit in CF flag."
"BT r/m16, r16","BTW r16, r/m16","btw r16, r/m16","0F A3 /r","V","V","","operand16","","Y","16","Store selected bit in CF flag."
"BT r/m32, imm8","BTL imm8, r/m32","btl imm8, r/m32","0F BA /4 ib","V","V","","operand32","","Y","32","Store selected bit in CF flag."
"BT r/m32, r32","BTL r32, r/m32","btl r32, r/m32","0F A3 /r","V","V","","operand32","","Y","32","Store selected bit in CF flag."
"BT r/m64, imm8","BTQ imm8, r/m64","btq imm8, r/m64","REX.W 0F BA /4 ib","N.E.","V","","","","Y","64","Store selected bit in CF flag."
"BT r/m64, r64","BTQ r64, r/m64","btq r64, r/m64","REX.W 0F A3 /r","N.E.","V","","","","Y","64","Store selected bit in CF flag."
"BTC r/m16, imm8","BTCW imm8, r/m16","btcw imm8, r/m16","0F BA /7 ib","V","V","","operand16","","Y","16","Store selected bit in CF flag and complement."
"BTC r/m16, r16","BTCW r16, r/m16","btcw r16, r/m16","0F BB /r","V","V","","operand16","","Y","16","Store selected bit in CF flag and complement."
"BTC r/m32, imm8","BTCL imm8, r/m32","btcl imm8, r/m32","0F BA /7 ib","V","V","","operand32","","Y","32","Store selected bit in CF flag and complement."
"BTC r/m32, r32","BTCL r32, r/m32","btcl r32, r/m32","0F BB /r","V","V","","operand32","","Y","32","Store selected bit in CF flag and complement."
"BTC r/m64, imm8","BTCQ imm8, r/m64","btcq imm8, r/m64","REX.W 0F BA /7 ib","N.E.","V","","","","Y","64","Store selected bit in CF flag and complement."
"BTC r/m64, r64","BTCQ r64, r/m64","btcq r64, r/m64","REX.W 0F BB /r","N.E.","V","","","","Y","64","Store selected bit in CF flag and complement."
"BTR r/m16, imm8","BTRW imm8, r/m16","btrw imm8, r/m16","0F BA /6 ib","V","V","","operand16","","Y","16","Store selected bit in CF flag and clear."
"BTR r/m16, r16","BTRW r16, r/m16","btrw r16, r/m16","0F B3 /r","V","V","","operand16","","Y","16","Store selected bit in CF flag and clear."
"BTR r/m32, imm8","BTRL imm8, r/m32","btrl imm8, r/m32","0F BA /6 ib","V","V","","operand32","","Y","32","Store selected bit in CF flag and clear."
"BTR r/m32, r32","BTRL r32, r/m32","btrl r32, r/m32","0F B3 /r","V","V","","operand32","","Y","32","Store selected bit in CF flag and clear."
"BTR r/m64, imm8","BTRQ imm8, r/m64","btrq imm8, r/m64","REX.W 0F BA /6 ib","N.E.","V","","","","Y","64","Store selected bit in CF flag and clear."
"BTR r/m64, r64","BTRQ r64, r/m64","btrq r64, r/m64","REX.W 0F B3 /r","N.E.","V","","","","Y","64","Store selected bit in CF flag and clear."
"BTS r/m16, imm8","BTSW imm8, r/m16","btsw imm8, r/m16","0F BA /5 ib","V","V","","operand16","","Y","16","Store selected bit in CF flag and set."
"BTS r/m16, r16","BTSW r16, r/m16","btsw r16, r/m16","0F AB /r","V","V","","operand16","","Y","16","Store selected bit in CF flag and set."
"BTS r/m32, imm8","BTSL imm8, r/m32","btsl imm8, r/m32","0F BA /5 ib","V","V","","operand32","","Y","32","Store selected bit in CF flag and set."
"BTS r/m32, r32","BTSL r32, r/m32","btsl r32, r/m32","0F AB /r","V","V","","operand32","","Y","32","Store selected bit in CF flag and set."
"BTS r/m64, imm8","BTSQ imm8, r/m64","btsq imm8, r/m64","REX.W 0F BA /5 ib","N.E.","V","","","","Y","64","Store selected bit in CF flag and set."
"BTS r/m64, r64","BTSQ r64, r/m64","btsq r64, r/m64","REX.W 0F AB /r","N.E.","V","","","","Y","64","Store selected bit in CF flag and set."
"BZHI r32a, r/m32, r32b","BZHIL r32b, r/m32, r32a","bzhil r32b, r/m32, r32a","VEX.LZ.0F38.W0 F5 /r","V","V","BMI2","","","Y","32","Zero bits in r/m32 starting with the position in r32b, write result to r32a."
"BZHI r64a, r/m64, r64b","BZHIQ r64b, r/m64, r64a","bzhiq r64b, r/m64, r64a","VEX.LZ.0F38.W1 F5 /r","N.E.","V","BMI2","","","Y","64","Zero bits in r/m64 starting with the position in r64b, write result to r64a."
"CALL r/m16","CALLW* r/m16","callw* r/m16","FF /2","V","N.E.","","operand16","","Y","16","Call near, absolute indirect, address given in r/m16."
"CALL r/m32","CALLL* r/m32","calll* r/m32","FF /2","V","N.E.","","operand32","","Y","32","Call near, absolute indirect, address given in r/m32."
"CALL r/m64","CALLQ* r/m64","callq* r/m64","FF /2","N.E.","V","","","","Y","64","Call near, absolute indirect, address given in r/m64."
"CALL rel16","CALL rel16","call rel16","E8 cw","V","N.S.","","operand16","r","Y","","Call near, relative, displacement relative to next instruction."
"CALL rel32","CALL rel32","call rel32","E8 cd","V","V","","operand32","r","Y","","Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode."
"CALL rel32","CALL rel32","call rel32","E8 cd","N.S.","V","","operand16,operand64","r","Y","",""
"CALL_FAR m16:16","LCALLW* m16:16","lcallw* m16:16","FF /3","V","V","","operand16","","Y","","Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16-bit offset from far pointer referenced in the instruction."
"CALL_FAR m16:32","LCALLL* m16:32","lcalll* m16:32","FF /3","V","V","","operand32","","Y","","In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction."
"CALL_FAR m16:64","LCALLQ* m16:64","lcallq* m16:64","REX.W FF /3","N.E.","V","","","","Y","","In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction."
"CALL_FAR ptr16:16","LCALLW ptr16:16","lcallw ptr16:16","9A cd","V","I","","operand16","","Y","","Call far, absolute, address given in operand."
"CALL_FAR ptr16:32","LCALLL ptr16:32","lcalll ptr16:32","9A cp","V","I","","operand32","","Y","","Call far, absolute, address given in operand."
"CBW","CBW","cbtw","98","V","V","","operand16","","","","AX := sign-extend of AL."
"CDQ","CDQ","cltd","99","V","V","","operand32","","","","EDX:EAX := sign-extend of EAX."
"CDQE","CDQE","cltq","REX.W 98","N.E.","V","","","","","","RAX := sign-extend of EAX."
"CLAC","CLAC","clac","NP 0F 01 CA","V","V","SMAP","","","","","Clear the AC flag in the EFLAGS register."
"CLC","CLC","clc","F8","V","V","","","","","","Clear CF flag."
"CLD","CLD","cld","FC","V","V","","","","","","Clear DF flag."
"CLDEMOTE m8","CLDEMOTE m8","cldemote m8","NP 0F 1C /0","V","V","CLDEMOTE","","","","","Hint to hardware to move the cache line containing m8 to a more distant level of the cache without writing back to mem- ory."
"CLI","CLI","cli","FA","V","V","","","","","","Clear interrupt flag; interrupts disabled when interrupt flag cleared."
"CLRSSBSY m64","CLRSSBSY m64","clrssbsy m64","F3 0F AE /6","V","V","CET_SS","","","","","Clear busy flag in supervisor shadow stack token reference by m64."
"CLTS","CLTS","clts","0F 06","V","V","","","","","","Clears TS flag in CR0."
"CLWB m8","CLWB m8","clwb m8","66 0F AE /6","V","V","CLWB","","","","","Writes back modified cache line containing m8, and may retain the line in cache hierarchy in non-modified state."
"CMC","CMC","cmc","F5","V","V","","","","","","Complement CF flag."
"CMOVA r16, r/m16","CMOVWHI r/m16, r16","cmovaw r/m16, r16","0F 47 /r","V","V","","operand16","","Y","16","Move if above (CF=0 and ZF=0)."
"CMOVA r32, r/m32","CMOVLHI r/m32, r32","cmoval r/m32, r32","0F 47 /r","V","V","","operand32","","Y","32","Move if above (CF=0 and ZF=0)."
"CMOVA r64, r/m64","CMOVQHI r/m64, r64","cmovaq r/m64, r64","REX.W 0F 47 /r","N.E.","V","","","","Y","64","Move if above (CF=0 and ZF=0)."
"CMOVAE r16, r/m16","CMOVWCC r/m16, r16","cmovaew r/m16, r16","0F 43 /r","V","V","","operand16","","Y","16","Move if above or equal (CF=0)."
"CMOVAE r32, r/m32","CMOVLCC r/m32, r32","cmovael r/m32, r32","0F 43 /r","V","V","","operand32","","Y","32","Move if above or equal (CF=0)."
"CMOVAE r64, r/m64","CMOVQCC r/m64, r64","cmovaeq r/m64, r64","REX.W 0F 43 /r","N.E.","V","","","","Y","64","Move if above or equal (CF=0)."
"CMOVB r16, r/m16","CMOVWCS r/m16, r16","cmovbw r/m16, r16","0F 42 /r","V","V","","operand16","","Y","16","Move if below (CF=1)."
"CMOVB r32, r/m32","CMOVLCS r/m32, r32","cmovbl r/m32, r32","0F 42 /r","V","V","","operand32","","Y","32","Move if below (CF=1)."
"CMOVB r64, r/m64","CMOVQCS r/m64, r64","cmovbq r/m64, r64","REX.W 0F 42 /r","N.E.","V","","","","Y","64","Move if below (CF=1)."
"CMOVBE r16, r/m16","CMOVWLS r/m16, r16","cmovbew r/m16, r16","0F 46 /r","V","V","","operand16","","Y","16","Move if below or equal (CF=1 or ZF=1)."
"CMOVBE r32, r/m32","CMOVLLS r/m32, r32","cmovbel r/m32, r32","0F 46 /r","V","V","","operand32","","Y","32","Move if below or equal (CF=1 or ZF=1)."
"CMOVBE r64, r/m64","CMOVQLS r/m64, r64","cmovbeq r/m64, r64","REX.W 0F 46 /r","N.E.","V","","","","Y","64","Move if below or equal (CF=1 or ZF=1)."
"CMOVC r16, r/m16","CMOVC r/m16, r16","cmovc r/m16, r16","0F 42 /r","V","V","","operand16,pseudo","","","","Move if carry (CF=1)."
"CMOVC r32, r/m32","CMOVC r/m32, r32","cmovc r/m32, r32","0F 42 /r","V","V","","operand32,pseudo","","","","Move if carry (CF=1)."
"CMOVC r64, r/m64","CMOVC r/m64, r64","cmovc r/m64, r64","REX.W 0F 42 /r","N.E.","V","","pseudo","","","","Move if carry (CF=1)."
"CMOVE r16, r/m16","CMOVWEQ r/m16, r16","cmovew r/m16, r16","0F 44 /r","V","V","","operand16","","Y","16","Move if equal (ZF=1)."
"CMOVE r32, r/m32","CMOVLEQ r/m32, r32","cmovel r/m32, r32","0F 44 /r","V","V","","operand32","","Y","32","Move if equal (ZF=1)."
"CMOVE r64, r/m64","CMOVQEQ r/m64, r64","cmoveq r/m64, r64","REX.W 0F 44 /r","N.E.","V","","","","Y","64","Move if equal (ZF=1)."
"CMOVG r16, r/m16","CMOVWGT r/m16, r16","cmovgw r/m16, r16","0F 4F /r","V","V","","operand16","","Y","16","Move if greater (ZF=0 and SF=OF)."
"CMOVG r32, r/m32","CMOVLGT r/m32, r32","cmovgl r/m32, r32","0F 4F /r","V","V","","operand32","","Y","32","Move if greater (ZF=0 and SF=OF)."
"CMP AL, imm8","CMPB AL, imm8","cmpb imm8, AL","3C ib","V","V","","","","Y","8","Compare imm8 with AL."
"CMP AX, imm16","CMPW AX, imm16","cmpw imm16, AX","3D iw","V","V","","operand16","","Y","16","Compare imm16 with AX."
"CMP EAX, imm32","CMPL EAX, imm32","cmpl imm32, EAX","3D id","V","V","","operand32","","Y","32","Compare imm32 with EAX."
"CMP RAX, imm32","CMPQ RAX, imm32","cmpq imm32, RAX","REX.W 3D id","N.E.","V","","","","Y","64","Compare imm32 sign-extended to 64-bits with RAX."
"CMP r/m16, imm16","CMPW r/m16, imm16","cmpw imm16, r/m16","81 /7 iw","V","V","","operand16","","Y","16","Compare imm16 with r/m16."
"CMP r/m16, imm8","CMPW r/m16, imm8","cmpw imm8, r/m16","83 /7 ib","V","V","","operand16","","Y","16","Compare imm8 with r/m16."
"CMP r/m16, r16","CMPW r/m16, r16","cmpw r16, r/m16","39 /r","V","V","","operand16","","Y","16","Compare r16 with r/m16."
"CMP r/m32, imm32","CMPL r/m32, imm32","cmpl imm32, r/m32","81 /7 id","V","V","","operand32","","Y","32","Compare imm32 with r/m32."
"CMP r/m32, imm8","CMPL r/m32, imm8","cmpl imm8, r/m32","83 /7 ib","V","V","","operand32","","Y","32","Compare imm8 with r/m32."
"CMP r/m32, r32","CMPL r/m32, r32","cmpl r32, r/m32","39 /r","V","V","","operand32","","Y","32","Compare r32 with r/m32."
"CMP r/m64, imm32","CMPQ r/m64, imm32","cmpq imm32, r/m64","REX.W 81 /7 id","N.E.","V","","","","Y","64","Compare imm32 sign-extended to 64-bits with r/m64."
"CMP r/m64, imm8","CMPQ r/m64, imm8","cmpq imm8, r/m64","REX.W 83 /7 ib","N.E.","V","","","","Y","64","Compare imm8 with r/m64."
"CMP r/m64, r64","CMPQ r/m64, r64","cmpq r64, r/m64","REX.W 39 /r","N.E.","V","","","","Y","64","Compare r64 with r/m64."
"CMP r/m8, imm8","CMPB r/m8, imm8","cmpb imm8, r/m8","80 /7 ib","V","V","","","","Y","8","Compare imm8 with r/m8."
"CMP r/m8, imm8","CMPB r/m8, imm8","cmpb imm8, r/m8","REX 80 /7 ib","N.E.","V","","pseudo64","","Y","8","Compare imm8 with r/m8."
"CMP r/m8, r8","CMPB r/m8, r8","cmpb r8, r/m8","38 /r","V","V","","","","Y","8","Compare r8 with r/m8."
"CMP r/m8, r8","CMPB r/m8, r8","cmpb r8, r/m8","REX 38 /r","N.E.","V","","pseudo64","","Y","8","Compare r8 with r/m8."
"CMP r16, r/m16","CMPW r16, r/m16","cmpw r/m16, r16","3B /r","V","V","","operand16","","Y","16","Compare r/m16 with r16."
"CMP r32, r/m32","CMPL r32, r/m32","cmpl r/m32, r32","3B /r","V","V","","operand32","","Y","32","Compare r/m32 with r32."
"CMP r64, r/m64","CMPQ r64, r/m64","cmpq r/m64, r64","REX.W 3B /r","N.E.","V","","","","Y","64","Compare r/m64 with r64."
"CMP r8, r/m8","CMPB r8, r/m8","cmpb r/m8, r8","3A /r","V","V","","","","Y","8","Compare r/m8 with r8."
"CMP r8, r/m8","CMPB r8, r/m8","cmpb r/m8, r8","REX 3A /r","N.E.","V","","pseudo64","","Y","8","Compare r/m8 with r8."
"CMPPD xmm1, xmm2/m128, imm8","CMPPD imm8, xmm1, xmm2/m128","cmppd imm8, xmm2/m128, xmm1","66 0F C2 /r ib","V","V","SSE2","","","","","Compare packed double precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate."
"CMPPS xmm1, xmm2/m128, imm8","CMPPS imm8, xmm1, xmm2/m128","cmpps imm8, xmm2/m128, xmm1","NP 0F C2 /r ib","V","V","SSE","","","","","Compare packed single precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate."
"CMPSB","CMPSB","cmpsb","A6","V","V","","","","","","For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly."
"CMPSD","CMPSL","cmpsl","A7","V","V","","operand32","","","","For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly."
"CMPSD xmm1, xmm2/m64, imm8","CMPSD imm8, xmm1, xmm2/m64","cmpsd imm8, xmm2/m64, xmm1","F2 0F C2 /r ib","V","V","SSE2","","","","","Compare low double precision floating-point value in xmm2/m64 and xmm1 using bits 2:0 of imm8 as comparison predicate."
"CMPSQ","CMPSQ","cmpsq","REX.W A7","N.E.","V","","","","","","Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly."
"CMPSS xmm1, xmm2/m32, imm8","CMPSS imm8, xmm1, xmm2/m32","cmpss imm8, xmm2/m32, xmm1","F3 0F C2 /r ib","V","V","SSE","","","","","Compare low single precision floating-point value in xmm2/m32 and xmm1 using bits 2:0 of imm8 as comparison predicate."
"CMPSW","CMPSW","cmpsw","A7","V","V","","operand16","","","","For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly."
"CMPXCHG r/m16, r16","CMPXCHGW r16, r/m16","cmpxchgw r16, r/m16","0F B1 /r","V","V","","operand16","","Y","16","Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX."
"CMPXCHG r/m32, r32","CMPXCHGL r32, r/m32","cmpxchgl r32, r/m32","0F B1 /r","V","V","","operand32","","Y","32","Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX."
"CMPXCHG r/m64, r64","CMPXCHGQ r64, r/m64","cmpxchgq r64, r/m64","REX.W 0F B1 /r","N.E.","V","","","","Y","64","Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX."
"CMPXCHG r/m8, r8","CMPXCHGB r8, r/m8","cmpxchgb r8, r/m8","0F B0 /r","V","V","","","","Y","8","Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL."
"CMPXCHG r/m8, r8","CMPXCHGB r8, r/m8","cmpxchgb r8, r/m8","REX 0F B0 /r","N.E.","V","","pseudo64","","Y","8","Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL."
"CMPXCHG16B m128","CMPXCHG16B m128","cmpxchg16b m128","REX.W 0F C7 /1","N.E.","V","","","","","","Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX."
"CMPXCHG8B m64","CMPXCHG8B m64","cmpxchg8b m64","0F C7 /1","V","V","","operand16,operand32","","","","Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX."
"COMISD xmm1, xmm2/m64","COMISD xmm2/m64, xmm1","comisd xmm2/m64, xmm1","66 0F 2F /r","V","V","SSE2","","","","","Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"COMISS xmm1, xmm2/m32","COMISS xmm2/m32, xmm1","comiss xmm2/m32, xmm1","NP 0F 2F /r","V","V","SSE","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"CPUID","CPUID","cpuid","0F A2","V","V","486","","","","","Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well)."
"CQO","CQO","cqto","REX.W 99","N.E.","V","","","","","","RDX:RAX:= sign-extend of RAX."
"CRC32 r32, r/m16","CRC32W r/m16, r32","crc32w r/m16, r32","F2 0F 38 F1 /r","V","V","","operand16","","Y","16","Accumulate CRC32 on r/m16."
"CRC32 r32, r/m32","CRC32L r/m32, r32","crc32l r/m32, r32","F2 0F 38 F1 /r","V","V","","operand32","","Y","32","Accumulate CRC32 on r/m32."
"CRC32 r32, r/m8","CRC32B r/m8, r32","crc32b r/m8, r32","F2 0F 38 F0 /r","V","V","","operand16,operand32","","Y","8","Accumulate CRC32 on r/m8."
"CRC32 r32, r/m8","CRC32B r/m8, r32","crc32b r/m8, r32","F2 REX 0F 38 F0 /r","N.E.","V","","pseudo64","","Y","8","Accumulate CRC32 on r/m8."
"CRC32 r64, r/m64","CRC32Q r/m64, r64","crc32q r/m64, r64","F2 REX.W 0F 38 F1 /r","N.E.","V","","","","Y","64","Accumulate CRC32 on r/m64."
"CRC32 r64, r/m8","CRC32B r/m8, r64","crc32b r/m8, r64","F2 REX.W 0F 38 F0 /r","N.E.","V","","","","Y","8","Accumulate CRC32 on r/m8."
"CVTDQ2PD xmm1, xmm2/m64","CVTPL2PD xmm2/m64, xmm1","cvtdq2pd xmm2/m64, xmm1","F3 0F E6 /r","V","V","SSE2","","","","","Convert two packed signed doubleword integers from xmm2/mem to two packed double precision floating- point values in xmm1."
"CVTPD2PI mm, xmm/m128","CVTPD2PI xmm/m128, mm","cvtpd2pi xmm/m128, mm","66 0F 2D /r","V","V","SSE2","","","","","Convert two packed double precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm."
"CVTPD2PS xmm1, xmm2/m128","CVTPD2PS xmm2/m128, xmm1","cvtpd2ps xmm2/m128, xmm1","66 0F 5A /r","V","V","SSE2","","","","","Convert two packed double precision floating-point values in xmm2/mem to two single precision floating-point values in xmm1."
"CVTPI2PD xmm, mm/m64","CVTPI2PD mm/m64, xmm","cvtpi2pd mm/m64, xmm","66 0F 2A /r","V","V","","","","","","Convert two packed signed doubleword integers from mm/mem64 to two packed double precision floating-point values in xmm."
"CVTPI2PS xmm, mm/m64","CVTPI2PS mm/m64, xmm","cvtpi2ps mm/m64, xmm","NP 0F 2A /r","V","V","","","","","","Convert two signed doubleword integers from mm/m64 to two single precision floating-point values in xmm."
"CVTPS2DQ xmm1, xmm2/m128","CVTPS2PL xmm2/m128, xmm1","cvtps2dq xmm2/m128, xmm1","66 0F 5B /r","V","V","SSE2","","","","","Convert four packed single precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1."
"CVTPS2PD xmm1, xmm2/m64","CVTPS2PD xmm2/m64, xmm1","cvtps2pd xmm2/m64, xmm1","NP 0F 5A /r","V","V","SSE2","","","","","Convert two packed single precision floating-point values in xmm2/m64 to two packed double precision floating-point values in xmm1."
"CVTPS2PI mm, xmm/m64","CVTPS2PI xmm/m64, mm","cvtps2pi xmm/m64, mm","NP 0F 2D /r","V","V","","","","","","Convert two packed single precision floating- point values from xmm/m64 to two packed signed doubleword integers in mm."
"CVTSD2SI r32, xmm1/m64","CVTSD2SL xmm1/m64, r32","cvtsd2si xmm1/m64, r32","F2 0F 2D /r","V","V","SSE2","operand16,operand32","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer r32."
"CVTSD2SI r64, xmm1/m64","CVTSD2SL xmm1/m64, r64","cvtsd2siq xmm1/m64, r64","F2 REX.W 0F 2D /r","N.E.","V","SSE2","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer sign- extended into r64."
"CVTSD2SS xmm1, xmm2/m64","CVTSD2SS xmm2/m64, xmm1","cvtsd2ss xmm2/m64, xmm1","F2 0F 5A /r","V","V","SSE2","","","","","Convert one double precision floating-point value in xmm2/m64 to one single precision floating-point value in xmm1."
"CVTSI2SD xmm1, r/m32","CVTSL2SD r/m32, xmm1","cvtsi2sdl r/m32, xmm1","F2 0F 2A /r","V","V","SSE2","operand16,operand32","","Y","32","Convert one signed doubleword integer from r32/m32 to one double precision floating-point value in xmm1."
"CVTSI2SD xmm1, r/m64","CVTSQ2SD r/m64, xmm1","cvtsi2sdq r/m64, xmm1","F2 REX.W 0F 2A /r","N.E.","V","SSE2","","","Y","64","Convert one signed quadword integer from r/m64 to one double precision floating-point value in xmm1."
"CVTSI2SS xmm1, r/m32","CVTSL2SS r/m32, xmm1","cvtsi2ssl r/m32, xmm1","F3 0F 2A /r","V","V","SSE","operand16,operand32","","Y","32","Convert one signed doubleword integer from r/m32 to one single precision floating-point value in xmm1."
"CVTSI2SS xmm1, r/m64","CVTSQ2SS r/m64, xmm1","cvtsi2ssq r/m64, xmm1","F3 REX.W 0F 2A /r","N.E.","V","SSE","","","Y","64","Convert one signed quadword integer from r/m64 to one single precision floating-point value in xmm1."
"CVTSS2SD xmm1, xmm2/m32","CVTSS2SD xmm2/m32, xmm1","cvtss2sd xmm2/m32, xmm1","F3 0F 5A /r","V","V","SSE2","","","","","Convert one single precision floating-point value in xmm2/m32 to one double precision floating-point value in xmm1."
"CVTSS2SI r32, xmm1/m32","CVTSS2SL xmm1/m32, r32","cvtss2si xmm1/m32, r32","F3 0F 2D /r","V","V","SSE","operand16,operand32","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32."
"CVTSS2SI r64, xmm1/m32","CVTSS2SL xmm1/m32, r64","cvtss2siq xmm1/m32, r64","F3 REX.W 0F 2D /r","N.E.","V","SSE","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64."
"CVTTPD2DQ xmm1, xmm2/m128","CVTTPD2PL xmm2/m128, xmm1","cvttpd2dq xmm2/m128, xmm1","66 0F E6 /r","V","V","SSE2","","","","","Convert two packed double precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation."
"CVTTPD2PI mm, xmm/m128","CVTTPD2PI xmm/m128, mm","cvttpd2pi xmm/m128, mm","66 0F 2C /r","V","V","","","","","","Convert two packer double precision floating- point values from xmm/m128 to two packed signed doubleword integers in mm using truncation."
"CVTTPS2DQ xmm1, xmm2/m128","CVTTPS2PL xmm2/m128, xmm1","cvttps2dq xmm2/m128, xmm1","F3 0F 5B /r","V","V","SSE2","","","","","Convert four packed single precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation."
"CVTTPS2PI mm, xmm/m64","CVTTPS2PI xmm/m64, mm","cvttps2pi xmm/m64, mm","NP 0F 2C /r","V","V","","","","","","Convert two single precision floating-point values from xmm/m64 to two signed doubleword signed integers in mm using truncation."
"CVTTSD2SI r32, xmm1/m64","CVTTSD2SL xmm1/m64, r32","cvttsd2si xmm1/m64, r32","F2 0F 2C /r","V","V","SSE2","operand16,operand32","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation."
"CVTTSD2SI r64, xmm1/m64","CVTTSD2SL xmm1/m64, r64","cvttsd2siq xmm1/m64, r64","F2 REX.W 0F 2C /r","N.E.","V","SSE2","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation."
"CVTTSS2SI r32, xmm1/m32","CVTTSS2SL xmm1/m32, r32","cvttss2si xmm1/m32, r32","F3 0F 2C /r","V","V","SSE","operand16,operand32","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation."
"CVTTSS2SI r64, xmm1/m32","CVTTSS2SL xmm1/m32, r64","cvttss2siq xmm1/m32, r64","F3 REX.W 0F 2C /r","N.E.","V","SSE","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation."
"CWD","CWD","cwtd","99","V","V","","operand16","","","","DX:AX := sign-extend of AX."
"CWDE","CWDE","cwtl","98","V","V","","operand32","","","","EAX := sign-extend of AX."
"DAA","DAA","daa","27","V","I","","","","","","Decimal adjust AL after addition."
"DAS","DAS","das","2F","V","I","","","","","","Decimal adjust AL after subtraction."
"DEC r/m16","DECW r/m16","decw r/m16","FF /1","V","V","","operand16","","Y","16","Decrement r/m16 by 1."
"DEC r/m32","DECL r/m32","decl r/m32","FF /1","V","V","","operand32","","Y","32","Decrement r/m32 by 1."
"DEC r/m64","DECQ r/m64","decq r/m64","REX.W FF /1","N.E.","V","","","","Y","64","Decrement r/m64 by 1."
"DEC r/m8","DECB r/m8","decb r/m8","FE /1","V","V","","","","Y","8","Decrement r/m8 by 1."
"DEC r/m8","DECB r/m8","decb r/m8","REX FE /1","N.E.","V","","pseudo64","","Y","8","Decrement r/m8 by 1."
"DEC r16","DECW r16","decw r16","48+rw","V","N.E.","","operand16","","Y","16","Decrement r16 by 1."
"DEC r32","DECL r32","decl r32","48+rd","V","N.E.","","operand32","","Y","32","Decrement r32 by 1."
"DIV r/m16","DIVW r/m16","divw r/m16","F7 /6","V","V","","operand16","","Y","16","Unsigned divide DX:AX by r/m16, with result stored in AX := Quotient, DX := Remainder."
"DIV r/m32","DIVL r/m32","divl r/m32","F7 /6","V","V","","operand32","","Y","32","Unsigned divide EDX:EAX by r/m32, with result stored in EAX := Quotient, EDX := Remainder."
"DIV r/m64","DIVQ r/m64","divq r/m64","REX.W F7 /6","N.E.","V","","","","Y","64","Unsigned divide RDX:RAX by r/m64, with result stored in RAX := Quotient, RDX := Remainder."
"DIV r/m8","DIVB r/m8","divb r/m8","F6 /6","V","V","","","","Y","8","Unsigned divide AX by r/m8, with result stored in AL := Quotient, AH := Remainder."
"DIV r/m8","DIVB r/m8","divb r/m8","REX F6 /6","N.E.","V","","pseudo64","","Y","8","Unsigned divide AX by r/m8, with result stored in AL := Quotient, AH := Remainder."
"DIVPD xmm1, xmm2/m128","DIVPD xmm2/m128, xmm1","divpd xmm2/m128, xmm1","66 0F 5E /r","V","V","SSE2","","","","","Divide packed double precision floating-point values in xmm1 by packed double precision floating-point values in xmm2/mem."
"DIVPS xmm1, xmm2/m128","DIVPS xmm2/m128, xmm1","divps xmm2/m128, xmm1","NP 0F 5E /r","V","V","SSE","","","","","Divide packed single precision floating-point values in xmm1 by packed single precision floating-point values in xmm2/mem."
"DIVSD xmm1, xmm2/m64","DIVSD xmm2/m64, xmm1","divsd xmm2/m64, xmm1","F2 0F 5E /r","V","V","SSE2","","","","","Divide low double precision floating-point value in xmm1 by low double precision floating-point value in xmm2/m64."
"DIVSS xmm1, xmm2/m32","DIVSS xmm2/m32, xmm1","divss xmm2/m32, xmm1","F3 0F 5E /r","V","V","SSE","","","","","Divide low single precision floating-point value in xmm1 by low single precision floating-point value in xmm2/m32."
"DPPD xmm1, xmm2/m128, imm8","DPPD imm8, xmm2/m128, xmm1","dppd imm8, xmm2/m128, xmm1","66 0F 3A 41 /r ib","V","V","SSE4_1","","","","","Selectively multiply packed double precision floating-point values from xmm1 with packed double precision floating-point values from xmm2, add and selectively store the packed double precision floating-point values to xmm1."
"DPPS xmm1, xmm2/m128, imm8","DPPS imm8, xmm2/m128, xmm1","dpps imm8, xmm2/m128, xmm1","66 0F 3A 40 /r ib","V","V","SSE4_1","","","","","Selectively multiply packed single precision floating-point values from xmm1 with packed single precision floating-point values from xmm2, add and selectively store the packed single precision floating-point values or zero values to xmm1."
"EMMS","EMMS","emms","NP 0F 77","V","V","","","","","","Set the x87 FPU tag word to empty."
"ENCODEKEY128 r32, r32, <XMM0-2>, <XMM4-6>","ENCODEKEY128 <XMM4-6>, <XMM0-2>, r32, r32","encodekey128 <XMM4-6>, <XMM0-2>, r32, r32","F3 0F 38 FA 11:rrr:bbb","V","V","AESKLE","","","","","Wrap a 128-bit AES key from XMM0 into a key handle and output handle in XMM0—2."
"ENCODEKEY256 r32, r32 <XMM0-6>","ENCODEKEY256 r32 <XMM0-6>, r32","encodekey256 r32 <XMM0-6>, r32","F3 0F 38 FB 11:rrr:bbb","V","V","AESKLE","","","","","Wrap a 256-bit AES key from XMM1:XMM0 into a key handle and store it in XMM0—3."
"ENDBR32","ENDBR32","endbr32","F3 0F 1E FB","V","V","CET_IBT","","","","","Terminate indirect branch in 32-bit and compatibility mode."
"ENDBR64","ENDBR64","endbr64","F3 0F 1E FA","V","V","CET_IBT","","","Y","","Terminate indirect branch in 64-bit mode."
"ENTER imm16, 0","ENTER 0, imm16","enter imm16, 0","C8 iw 00","V","V","","pseudo","","","","Create a stack frame for a procedure."
"ENTER imm16, 1","ENTER 1, imm16","enter imm16, 1","C8 iw 01","V","V","","pseudo","","","","Create a stack frame with a nested pointer for a procedure."
"ENTER imm16, imm8b","ENTERW/ENTERL/ENTERQ imm8b, imm16","enterw/enterl/enterq imm16, imm8b","C8 iw ib","V","V","","","","","","Create a stack frame with nested pointers for a procedure."
"F2XM1","F2XM1","f2xm1","D9 F0","V","V","","","","","","Replace ST(0) with (2 – 1)."
"FABS","FABS","fabs","D9 E1","V","V","","","","","","Replace ST with its absolute value."
"FADD ST(0), ST(i)","FADDD ST(i), ST(0)","fadd ST(i), ST(0)","D8 C0+i","V","V","","","rw,r","Y","","Add ST(0) to ST(i) and store result in ST(0)."
"FADD ST(i), ST(0)","FADDD ST(0), ST(i)","fadd ST(0), ST(i)","DC C0+i","V","V","","","rw,r","Y","","Add ST(i) to ST(0) and store result in ST(i)."
"FADD m32fp","FADDD m32fp","fadds m32fp","D8 /0","V","V","","","r","Y","32","Add m32fp to ST(0) and store result in ST(0)."
"FADD m64fp","FADDD m64fp","faddl m64fp","DC /0","V","V","","","r","Y","64","Add m64fp to ST(0) and store result in ST(0)."
"FADDP","FADDDP","faddp","DE C1","V","V","","pseudo","","","","Add ST(0) to ST(1), store result in ST(1), and pop the register stack."
"FADDP ST(i), ST(0)","FADDDP ST(0), ST(i)","faddp ST(0), ST(i)","DE C0+i","V","V","","","rw,r","","","Add ST(0) to ST(i), store result in ST(i), and pop the register stack."
"FBLD m80bcd","FBLD m80bcd","fbld m80bcd","DF /4","V","V","","","","","","Convert BCD value to floating-point and push onto the FPU stack."
"FBSTP m80bcd","FBSTP m80bcd","fbstp m80bcd","DF /6","V","V","","","w","","","Store ST(0) in m80bcd and pop ST(0)."
"FCHS","FCHS","fchs","D9 E0","V","V","","","","","","Complements sign of ST(0)."
"FCOM","FCOMD","fcom","D8 D1","V","V","","pseudo","","Y","","Compare ST(0) with ST(1)."
"FCOM ST(i)","FCOMD ST(i)","fcom ST(i)","D8 D0+i","V","V","","","r","Y","","Compare ST(0) with ST(i)."
"FCOM m32fp","FCOMD m32fp","fcoms m32fp","D8 /2","V","V","","","r","Y","32","Compare ST(0) with m32fp."
"FCOM m64fp","FCOMD m64fp","fcoml m64fp","DC /2","V","V","","","r","Y","64","Compare ST(0) with m64fp."
"FCOMI ST(0), ST(i)","FCOMI ST(i), ST(0)","fcomi ST(i), ST(0)","DB F0+i","V","V","","P6","r,r","","","Compare ST(0) with ST(i) and set status flags accordingly."
"FCOMIP ST(0), ST(i)","FCOMIP ST(i), ST(0)","fcomip ST(i), ST(0)","DF F0+i","V","V","","P6","r,r","","","Compare ST(0) with ST(i), set status flags accordingly, and pop register stack."
"FCOMP","FCOMP","fcomp","D8 D9","V","V","","pseudo","","Y","","Compare ST(0) with ST(1) and pop register stack."
"FCOMP ST(i)","FCOMP ST(i)","fcomp ST(i)","D8 D8+i","V","V","","","r","Y","","Compare ST(0) with ST(i) and pop register stack."
"FCOMP m32fp","FCOMFP m32fp","fcomps m32fp","D8 /3","V","V","","","r","Y","32","Compare ST(0) with m32fp and pop register stack."
"FCOMP m64fp","FCOMPL m64fp","fcompl m64fp","DC /3","V","V","","","r","Y","64","Compare ST(0) with m64fp and pop register stack."
"FCOMPP","FCOMPP","fcompp","DE D9","V","V","","","","","","Compare ST(0) with ST(1) and pop register stack twice."
"FCOS","FCOS","fcos","D9 FF","V","V","","","","","","Replace ST(0) with its approximate cosine."
"FDECSTP","FDECSTP","fdecstp","D9 F6","V","V","","","","","","Decrement TOP field in FPU status word."
"FDIV ST(0), ST(i)","FDIVD ST(i), ST(0)","fdiv ST(i), ST(0)","D8 F0+i","V","V","","","rw,r","Y","","Divide ST(0) by ST(i) and store result in ST(0)."
"FDIV ST(i), ST(0)","FDIVD ST(0), ST(i)","fdivr ST(0), ST(i)","DC F8+i","V","V","","","rw,r","Y","","Divide ST(i) by ST(0) and store result in ST(i)."
"FDIV m32fp","FDIVD m32fp","fdivs m32fp","D8 /6","V","V","","","r","Y","32","Divide ST(0) by m32fp and store result in ST(0)."
"FDIV m64fp","FDIVD m64fp","fdivl m64fp","DC /6","V","V","","","r","Y","64","Divide ST(0) by m64fp and store result in ST(0)."
"FDIVP","FDIVP","fdivp","DE F9","V","V","","pseudo","","","","Divide ST(1) by ST(0), store result in ST(1), and pop the register stack."
"FDIVP ST(i), ST(0)","FDIVRP ST(0), ST(i)","fdivrp ST(0), ST(i)","DE F8+i","V","V","","","rw,r","","","Divide ST(i) by ST(0), store result in ST(i), and pop the register stack."
"FDIVR ST(0), ST(i)","FDIVR ST(i), ST(0)","fdivr ST(i), ST(0)","D8 F8+i","V","V","","","rw,r","Y","","Divide ST(i) by ST(0) and store result in ST(0)."
"FDIVR ST(i), ST(0)","FDIVD ST(0), ST(i)","fdiv ST(0), ST(i)","DC F0+i","V","V","","","rw,r","Y","","Divide ST(0) by ST(i) and store result in ST(i)."
"FDIVR m32fp","FDIVFR m32fp","fdivrs m32fp","D8 /7","V","V","","","r","Y","32","Divide m32fp by ST(0) and store result in ST(0)."
"FDIVR m64fp","FDIVRL m64fp","fdivrl m64fp","DC /7","V","V","","","r","Y","64","Divide m64fp by ST(0) and store result in ST(0)."
"FDIVRP","FDIVRP","fdivrp","DE F1","V","V","","pseudo","","","","Divide ST(0) by ST(1), store result in ST(1), and pop the register stack."
"FDIVRP ST(i), ST(0)","FDIVP ST(0), ST(i)","fdivp ST(0), ST(i)","DE F0+i","V","V","","","rw,r","","","Divide ST(0) by ST(i), store result in ST(i), and pop the register stack."
"FFREE ST(i)","FFREE ST(i)","ffree ST(i)","DD C0+i","V","V","","","w","","","Sets tag for ST(i) to empty."
"FFREEP ST(i)","FFREEP ST(i)","ffreep ST(i)","DF C0+i","V","V","","","w","","",""
"FIADD m16int","FIADD m16int","fiadd m16int","DE /0","V","V","","","r","Y","","Add m16int to ST(0) and store result in ST(0)."
"FIADD m32int","FIADDL m32int","fiaddl m32int","DA /0","V","V","","","r","Y","32","Add m32int to ST(0) and store result in ST(0)."
"FICOM m16int","FICOM m16int","ficom m16int","DE /2","V","V","","","r","Y","","Compare ST(0) with m16int."
"FICOM m32int","FICOML m32int","ficoml m32int","DA /2","V","V","","","r","Y","32","Compare ST(0) with m32int."
"FICOMP m16int","FICOMP m16int","ficomp m16int","DE /3","V","V","","","r","Y","","Compare ST(0) with m16int and pop stack register."
"FICOMP m32int","FICOMPL m32int","ficompl m32int","DA /3","V","V","","","r","Y","32","Compare ST(0) with m32int and pop stack register."
"FIDIV m16int","FIDIV m16int","fidiv m16int","DE /6","V","V","","","r","Y","","Divide ST(0) by m16int and store result in ST(0)."
"FIDIV m32int","FIDIVL m32int","fidivl m32int","DA /6","V","V","","","r","Y","32","Divide ST(0) by m32int and store result in ST(0)."
"FIDIVR m16int","FIDIVR m16int","fidivr m16int","DE /7","V","V","","","r","Y","","Divide m16int by ST(0) and store result in ST(0)."
"FIDIVR m32int","FIDIVRL m32int","fidivrl m32int","DA /7","V","V","","","r","Y","32","Divide m32int by ST(0) and store result in ST(0)."
"FILD m16int","FILD m16int","fild m16int","DF /0","V","V","","","r","Y","","Push m16int onto the FPU register stack."
"FILD m32int","FILDL m32int","fildl m32int","DB /0","V","V","","","r","Y","32","Push m32int onto the FPU register stack."
"FILD m64int","FILDLL m64int","fildll m64int","DF /5","V","V","","","r","Y","64","Push m64int onto the FPU register stack."
"FIMUL m16int","FIMUL m16int","fimul m16int","DE /1","V","V","","","r","Y","","Multiply ST(0) by m16int and store result in ST(0)."
"FIMUL m32int","FIMULL m32int","fimull m32int","DA /1","V","V","","","r","Y","32","Multiply ST(0) by m32int and store result in ST(0)."
"FINCSTP","FINCSTP","fincstp","D9 F7","V","V","","","","","","Increment the TOP field in the FPU status register."
"FINIT","FINIT","finit","9B DB E3","V","V","","pseudo","","","","Initialize FPU after checking for pending unmasked floating-point exceptions."
"FIST m16int","FIST m16int","fist m16int","DF /2","V","V","","","w","Y","","Store ST(0) in m16int."
"FIST m32int","FISTL m32int","fistl m32int","DB /2","V","V","","","w","Y","32","Store ST(0) in m32int."
"FISTP m16int","FISTP m16int","fistp m16int","DF /3","V","V","","","w","Y","","Store ST(0) in m16int and pop register stack."
"FISTP m32int","FISTPL m32int","fistpl m32int","DB /3","V","V","","","w","Y","32","Store ST(0) in m32int and pop register stack."
"FISTP m64int","FISTPLL m64int","fistpll m64int","DF /7","V","V","","","w","Y","64","Store ST(0) in m64int and pop register stack."
"FISTTP m16int","FISTTP m16int","fisttp m16int","DF /1","V","V","","","w","Y","","Store ST(0) in m16int with truncation."
"FISTTP m32int","FISTTPL m32int","fisttpl m32int","DB /1","V","V","","","w","Y","32","Store ST(0) in m32int with truncation."
"FISTTP m64int","FISTTPLL m64int","fisttpll m64int","DD /1","V","V","","","w","Y","64","Store ST(0) in m64int with truncation."
"FISUB m16int","FISUB m16int","fisub m16int","DE /4","V","V","","","r","Y","","Subtract m16int from ST(0) and store result in ST(0)."
"FISUB m32int","FISUBL m32int","fisubl m32int","DA /4","V","V","","","r","Y","32","Subtract m32int from ST(0) and store result in ST(0)."
"FISUBR m16int","FISUBR m16int","fisubr m16int","DE /5","V","V","","","r","Y","","Subtract ST(0) from m16int and store result in ST(0)."
"FISUBR m32int","FISUBRL m32int","fisubrl m32int","DA /5","V","V","","","r","Y","32","Subtract ST(0) from m32int and store result in ST(0)."
"FLD ST(i)","FLD ST(i)","fld ST(i)","D9 C0+i","V","V","","","r","Y","","Push ST(i) onto the FPU register stack."
"FLD m32fp","FLDS m32fp","flds m32fp","D9 /0","V","V","","","r","Y","32","Push m32fp onto the FPU register stack."
"FLD m64fp","FLDL m64fp","fldl m64fp","DD /0","V","V","","","r","Y","64","Push m64fp onto the FPU register stack."
"FLD m80fp","FLDT m80fp","fldt m80fp","DB /5","V","V","","","r","Y","80","Push m80fp onto the FPU register stack."
"FLD1","FLD1","fld1","D9 E8","V","V","","","","","","Push +1.0 onto the FPU register stack."
"FLDCW m2byte","FLDCW m2byte","fldcw m2byte","D9 /5","V","V","","","r","","","Load FPU control word from m2byte."
"FLDENV m14/28byte","FLDENVS/FLDENVL m14/28byte","fldenvs/fldenvl m14/28byte","D9 /4","V","V","","","r","","","Load FPU environment from m14byte or m28byte."
"FLDL2E","FLDL2E","fldl2e","D9 EA","V","V","","","","","","Push log e onto the FPU register stack."
"FLDL2T","FLDL2T","fldl2t","D9 E9","V","V","","","","","","Push log 10 onto the FPU register stack."
"FLDLG2","FLDLG2","fldlg2","D9 EC","V","V","","","","","","Push log 2 onto the FPU register stack."
"FLDPI","FLDPI","fldpi","D9 EB","V","V","","","","","","Push π onto the FPU register stack."
"FMUL ST(0), ST(i)","FMUL ST(i), ST(0)","fmul ST(i), ST(0)","D8 C8+i","V","V","","","rw,r","Y","","Multiply ST(0) by ST(i) and store result in ST(0)."
"FMUL ST(i), ST(0)","FMUL ST(0), ST(i)","fmul ST(0), ST(i)","DC C8+i","V","V","","","rw,r","Y","","Multiply ST(i) by ST(0) and store result in ST(i)."
"FMUL m32fp","FMULS m32fp","fmuls m32fp","D8 /1","V","V","","","r","Y","32","Multiply ST(0) by m32fp and store result in ST(0)."
"FMUL m64fp","FMULL m64fp","fmull m64fp","DC /1","V","V","","","r","Y","64","Multiply ST(0) by m64fp and store result in ST(0)."
"FMULP","FMULP","fmulp","DE C9","V","V","","pseudo","","","","Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack."
"FMULP ST(i), ST(0)","FMULP ST(0), ST(i)","fmulp ST(0), ST(i)","DE C8+i","V","V","","","rw,r","","","Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack."
"FNINIT","FNINIT","fninit","DB E3","V","V","","","","","","Initialize FPU without checking for pending unmasked floating-point exceptions."
"FNOP","FNOP","fnop","D9 D0","V","V","","","","","","No operation is performed."
"FNSAVE m94/108byte","FNSAVES/FNSAVEL m94/108byte","fnsaves/fnsavel m94/108byte","DD /6","V","V","","","w","","","Store FPU environment to m94byte or m108byte without checking for pending unmasked floating- point exceptions. Then re-initialize the FPU."
"FNSTCW m2byte","FNSTCW m2byte","fnstcw m2byte","D9 /7","V","V","","","w","","","Store FPU control word to m2byte without checking for pending unmasked floating-point exceptions."
"FNSTENV m14/28byte","FNSTENVS/FNSTENVL m14/28byte","fnstenvs/fnstenvl m14/28byte","D9 /6","V","V","","","w","","","Store FPU environment to m14byte or m28byte without checking for pending unmasked floating- point exceptions. Then mask all floating- point exceptions."
"FNSTSW AX","FNSTSW AX","fnstsw AX","DF E0","V","V","","","w","","","Store FPU status word in AX register without checking for pending unmasked floating-point exceptions."
"FNSTSW m2byte","FNSTSW m2byte","fnstsw m2byte","DD /7","V","V","","","w","","","Store FPU status word at m2byte without checking for pending unmasked floating-point exceptions."
"FPREM","FPREM","fprem","D9 F8","V","V","","","","","","Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1)."
"FPREM1","FPREM1","fprem1","D9 F5","V","V","","","","","","Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1)."
"FPTAN","FPTAN","fptan","D9 F2","V","V","","","","","","Replace ST(0) with its approximate tangent and push 1 onto the FPU stack."
"FRNDINT","FRNDINT","frndint","D9 FC","V","V","","","","","","Round ST(0) to an integer."
"FRSTOR m94/108byte","FRSTORS/FRSTORL m94/108byte","frstors/frstorl m94/108byte","DD /4","V","V","","","r","","","Load FPU state from m94byte or m108byte."
"FSAVE m94/108byte","FSAVE m94/108byte","fsave m94/108byte","9B DD /6","V","V","","pseudo","w","","","Store FPU state to m94byte or m108byte after checking for pending unmasked floating-point exceptions. Then re-initialize the FPU."
"FSCALE","FSCALE","fscale","D9 FD","V","V","","","","","","Scale ST(0) by ST(1)."
"FSIN","FSIN","fsin","D9 FE","V","V","","","","","","Replace ST(0) with the approximate of its sine."
"FSINCOS","FSINCOS","fsincos","D9 FB","V","V","","","","","","Compute the sine and cosine of ST(0); replace ST(0) with the approximate sine, and push the approximate cosine onto the register stack."
"FSQRT","FSQRT","fsqrt","D9 FA","V","V","","","","","","Computes square root of ST(0) and stores the result in ST(0)."
"FST ST(i)","FST ST(i)","fst ST(i)","DD D0+i","V","V","","","w","Y","","Copy ST(0) to ST(i)."
"FST m32fp","FSTS m32fp","fsts m32fp","D9 /2","V","V","","","w","Y","32","Copy ST(0) to m32fp."
"FST m64fp","FSTL m64fp","fstl m64fp","DD /2","V","V","","","w","Y","64","Copy ST(0) to m64fp."
"FSTCW m2byte","FSTCW m2byte","fstcw m2byte","9B D9 /7","V","V","","pseudo","w","","","Store FPU control word to m2byte after checking for pending unmasked floating-point exceptions."
"FSTENV m14/28byte","FSTENV m14/28byte","fstenv m14/28byte","9B D9 /6","V","V","","pseudo","w","","","Store FPU environment to m14byte or m28byte after checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions."
"FSTP ST(i)","FSTP ST(i)","fstp ST(i)","DD D8+i","V","V","","","w","Y","","Copy ST(0) to ST(i) and pop register stack."
"FSTP m32fp","FSTPS m32fp","fstps m32fp","D9 /3","V","V","","","w","Y","32","Copy ST(0) to m32fp and pop register stack."
"FSTP m64fp","FSTPL m64fp","fstpl m64fp","DD /3","V","V","","","w","Y","64","Copy ST(0) to m64fp and pop register stack."
"FSTP m80fp","FSTPT m80fp","fstpt m80fp","DB /7","V","V","","","w","Y","80","Copy ST(0) to m80fp and pop register stack."
"FSTSW AX","FSTSW AX","fstsw AX","9B DF E0","V","V","","pseudo","w","","","Store FPU status word in AX register after checking for pending unmasked floating-point exceptions."
"FSTSW m2byte","FSTSW m2byte","fstsw m2byte","9B DD /7","V","V","","pseudo","w","","","Store FPU status word at m2byte after checking for pending unmasked floating-point exceptions."
"FSUB ST(0), ST(i)","FSUB ST(i), ST(0)","fsub ST(i), ST(0)","D8 E0+i","V","V","","","rw,r","Y","","Subtract ST(i) from ST(0) and store result in ST(0)."
"FSUB ST(i), ST(0)","FSUBR ST(0), ST(i)","fsubr ST(0), ST(i)","DC E8+i","V","V","","","rw,r","Y","","Subtract ST(0) from ST(i) and store result in ST(i)."
"FSUB m32fp","FSUBS m32fp","fsubs m32fp","D8 /4","V","V","","","r","Y","32","Subtract m32fp from ST(0) and store result in ST(0)."
"FSUB m64fp","FSUBL m64fp","fsubl m64fp","DC /4","V","V","","","r","Y","64","Subtract m64fp from ST(0) and store result in ST(0)."
"FSUBP","FSUBP","fsubp","DE E9","V","V","","pseudo","","","","Subtract ST(0) from ST(1), store result in ST(1), and pop register stack."
"FSUBP ST(i), ST(0)","FSUBRP ST(0), ST(i)","fsubrp ST(0), ST(i)","DE E8+i","V","V","","","rw,r","","","Subtract ST(0) from ST(i), store result in ST(i), and pop register stack."
"FSUBR ST(0), ST(i)","FSUBR ST(i), ST(0)","fsubr ST(i), ST(0)","D8 E8+i","V","V","","","rw,r","Y","","Subtract ST(0) from ST(i) and store result in ST(0)."
"FSUBR ST(i), ST(0)","FSUB ST(0), ST(i)","fsub ST(0), ST(i)","DC E0+i","V","V","","","rw,r","Y","","Subtract ST(i) from ST(0) and store result in ST(i)."
"FSUBR m32fp","FSUBRS m32fp","fsubrs m32fp","D8 /5","V","V","","","r","Y","32","Subtract ST(0) from m32fp and store result in ST(0)."
"FSUBR m64fp","FSUBRL m64fp","fsubrl m64fp","DC /5","V","V","","","r","Y","64","Subtract ST(0) from m64fp and store result in ST(0)."
"FSUBRP","FSUBRP","fsubrp","DE E1","V","V","","pseudo","","","","Subtract ST(1) from ST(0), store result in ST(1), and pop register stack."
"FSUBRP ST(i), ST(0)","FSUBP ST(0), ST(i)","fsubp ST(0), ST(i)","DE E0+i","V","V","","","rw,r","","","Subtract ST(i) from ST(0), store result in ST(i), and pop register stack."
"FTST","FTST","ftst","D9 E4","V","V","","","","","","Compare ST(0) with 0.0."
"FUCOM","FUCOM","fucom","DD E1","V","V","","pseudo","","","","Compare ST(0) with ST(1)."
"FUCOM ST(i)","FUCOM ST(i)","fucom ST(i)","DD E0+i","V","V","","","r","","","Compare ST(0) with ST(i)."
"FUCOMI ST(0), ST(i)","FUCOMI ST(i), ST(0)","fucomi ST(i), ST(0)","DB E8+i","V","V","","P6","r,r","","","Compare ST(0) with ST(i), check for ordered values, and set status flags accordingly."
"FUCOMIP ST(0), ST(i)","FUCOMIP ST(i), ST(0)","fucomip ST(i), ST(0)","DF E8+i","V","V","","P6","r,r","","","Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack."
"FUCOMP","FUCOMP","fucomp","DD E9","V","V","","pseudo","","","","Compare ST(0) with ST(1) and pop register stack."
"FUCOMP ST(i)","FUCOMP ST(i)","fucomp ST(i)","DD E8+i","V","V","","","r","","","Compare ST(0) with ST(i) and pop register stack."
"FUCOMPP","FUCOMPP","fucompp","DA E9","V","V","","","","","","Compare ST(0) with ST(1) and pop register stack twice."
"FWAIT","FWAIT","fwait","9B","V","V","","","","","","Check pending unmasked floating-point exceptions."
"FXAM","FXAM","fxam","D9 E5","V","V","","","","","","Classify value or number in ST(0)."
"FXCH","FXCH","fxch","D9 C9","V","V","","pseudo","","","","Exchange the contents of ST(0) and ST(1)."
"FXCH ST(i)","FXCH ST(i)","fxch ST(i)","D9 C8+i","V","V","","","rw","","","Exchange the contents of ST(0) and ST(i)."
"FXRSTOR m512byte","FXRSTOR m512byte","fxrstor m512byte","NP 0F AE /1","V","V","","operand16,operand32","","","","Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte."
"FXRSTOR64 m512byte","FXRSTOR64 m512byte","fxrstor64 m512byte","NP REX.W 0F AE /1","N.E.","V","","","","","","Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte."
"FXSAVE m512byte","FXSAVE m512byte","fxsave m512byte","NP 0F AE /0","V","V","","operand16,operand32","","","","Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte."
"FXSAVE64 m512byte","FXSAVE64 m512byte","fxsave64 m512byte","NP REX.W 0F AE /0","N.E.","V","","","","","","Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte."
"FXTRACT","FXTRACT","fxtract","D9 F4","V","V","","","","","","Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack."
"FYL2X","FYL2X","fyl2x","D9 F1","V","V","","","","","","Replace ST(1) with (ST(1) ∗ log ST(0)) and pop the register stack."
"FYL2XP1","FYL2XP1","fyl2xp1","D9 F9","V","V","","","","","","Replace ST(1) with ST(1) ∗ log (ST(0) + 1.0) and pop the register stack."
"HADDPD xmm1, xmm2/m128","HADDPD xmm2/m128, xmm1","haddpd xmm2/m128, xmm1","66 0F 7C /r","V","V","SSE3","","","","","Horizontal add packed double precision floating-point values from xmm2/m128 to xmm1."
"HADDPS xmm1, xmm2/m128","HADDPS xmm2/m128, xmm1","haddps xmm2/m128, xmm1","F2 0F 7C /r","V","V","SSE3","","","","","Horizontal add packed single precision floating-point values from xmm2/m128 to xmm1."
"HLT","HLT","hlt","F4","V","V","","","","","","Halt"
"HSUBPD xmm1, xmm2/m128","HSUBPD xmm2/m128, xmm1","hsubpd xmm2/m128, xmm1","66 0F 7D /r","V","V","SSE3","","","","","Horizontal subtract packed double precision floating-point values from xmm2/m128 to xmm1."
"HSUBPS xmm1, xmm2/m128","HSUBPS xmm2/m128, xmm1","hsubps xmm2/m128, xmm1","F2 0F 7D /r","V","V","SSE3","","","","","Horizontal subtract packed single precision floating-point values from xmm2/m128 to xmm1."
"ICEBP","ICEBP","icebp","F1","V","V","","","","","",""
"IDIV r/m16","IDIVW r/m16","idivw r/m16","F7 /7","V","V","","operand16","","Y","16","Signed divide DX:AX by r/m16, with result stored in AX := Quotient, DX := Remainder."
"IDIV r/m32","IDIVL r/m32","idivl r/m32","F7 /7","V","V","","operand32","","Y","32","Signed divide EDX:EAX by r/m32, with result stored in EAX := Quotient, EDX := Remainder."
"IDIV r/m64","IDIVQ r/m64","idivq r/m64","REX.W F7 /7","N.E.","V","","","","Y","64","Signed divide RDX:RAX by r/m64, with result stored in RAX := Quotient, RDX := Remainder."
"IDIV r/m8","IDIVB r/m8","idivb r/m8","F6 /7","V","V","","","","Y","8","Signed divide AX by r/m8, with result stored in: AL := Quotient, AH := Remainder."
"IDIV r/m8","IDIVB r/m8","idivb r/m8","REX F6 /7","N.E.","V","","pseudo64","","Y","8","Signed divide AX by r/m8, with result stored in AL := Quotient, AH := Remainder."
"IMUL r/m16","IMULW r/m16","imulw r/m16","F7 /5","V","V","","operand16","","Y","16","DX:AX := AX ∗ r/m word."
"IMUL r/m32","IMULL r/m32","imull r/m32","F7 /5","V","V","","operand32","","Y","32","EDX:EAX := EAX ∗ r/m32."
"IMUL r/m64","IMULQ r/m64","imulq r/m64","REX.W F7 /5","N.E.","V","","","","Y","64","RDX:RAX := RAX ∗ r/m64."
"IMUL r/m8","IMULB r/m8","imulb r/m8","F6 /5","V","V","","","","Y","8","AX:= AL ∗ r/m byte."
"IMUL r16, r/m16","IMULW r/m16, r16","imulw r/m16, r16","0F AF /r","V","V","","operand16","","Y","16","word register := word register ∗ r/m16."
"IMUL r16, r/m16, imm16","IMULW imm16, r/m16, r16","imulw imm16, r/m16, r16","69 /r iw","V","V","","operand16","","Y","16","word register := r/m16 ∗ immediate word."
"IMUL r16, r/m16, imm8","IMULW imm8, r/m16, r16","imulw imm8, r/m16, r16","6B /r ib","V","V","","operand16","","Y","16","word register := r/m16 ∗ sign-extended immediate byte."
"IMUL r32, r/m32","IMULL r/m32, r32","imull r/m32, r32","0F AF /r","V","V","","operand32","","Y","32","doubleword register := doubleword register ∗ r/m32."
"IMUL r32, r/m32, imm32","IMULL imm32, r/m32, r32","imull imm32, r/m32, r32","69 /r id","V","V","","operand32","","Y","32","doubleword register := r/m32 ∗ immediate doubleword."
"IMUL r32, r/m32, imm8","IMULL imm8, r/m32, r32","imull imm8, r/m32, r32","6B /r ib","V","V","","operand32","","Y","32","doubleword register := r/m32 ∗ sign- extended immediate byte."
"IMUL r64, r/m64","IMULQ r/m64, r64","imulq r/m64, r64","REX.W 0F AF /r","N.E.","V","","","","Y","64","Quadword register := Quadword register ∗ r/m64."
"IMUL r64, r/m64, imm32","IMULQ imm32, r/m64, r64","imulq imm32, r/m64, r64","REX.W 69 /r id","N.E.","V","","","","Y","64","Quadword register := r/m64 ∗ immediate doubleword."
"IMUL r64, r/m64, imm8","IMULQ imm8, r/m64, r64","imulq imm8, r/m64, r64","REX.W 6B /r ib","N.E.","V","","","","Y","64","Quadword register := r/m64 ∗ sign-extended immediate byte."
"IN AL, DX","INB DX, AL","inb DX, AL","EC","V","V","","","w,r","Y","8","Input byte from I/O port in DX into AL."
"IN AL, imm8u","INB imm8u, AL","inb imm8u, AL","E4 ib","V","V","","","w,r","Y","8","Input byte from imm8 I/O port address into AL."
"IN AX, DX","INW DX, AX","inw DX, AX","ED","V","V","","operand16","w,r","Y","16","Input word from I/O port in DX into AX."
"IN AX, imm8u","INW imm8u, AX","inw imm8u, AX","E5 ib","V","V","","operand16","w,r","Y","16","Input word from imm8 I/O port address into AX."
"IN EAX, DX","INL DX, EAX","inl DX, EAX","ED","V","V","","operand32,operand64","w,r","Y","32","Input doubleword from I/O port in DX into EAX."
"IN EAX, imm8u","INL imm8u, EAX","inl imm8u, EAX","E5 ib","V","V","","operand32,operand64","w,r","Y","32","Input dword from imm8 I/O port address into EAX."
"INC r/m16","INCW r/m16","incw r/m16","FF /0","V","V","","operand16","","Y","16","Increment r/m word by 1."
"INC r/m32","INCL r/m32","incl r/m32","FF /0","V","V","","operand32","","Y","32","Increment r/m doubleword by 1."
"INC r/m64","INCQ r/m64","incq r/m64","REX.W FF /0","N.E.","V","","","","Y","64","Increment r/m quadword by 1."
"INC r/m8","INCB r/m8","incb r/m8","FE /0","V","V","","","","Y","8","Increment r/m byte by 1."
"INC r/m8","INCB r/m8","incb r/m8","REX FE /0","N.E.","V","","pseudo64","","Y","8","Increment r/m byte by 1."
"INC r16","INCW r16","incw r16","40+rw","V","N.E.","","operand16","","Y","16","Increment word register by 1."
"INC r32","INCL r32","incl r32","40+rd","V","N.E.","","operand32","","Y","32","Increment doubleword register by 1."
"INCSSPD r32","INCSSPD r32","incsspd r32","F3 0F AE /05","V","V","CET_SS","operand16,operand32","","","","Increment SSP by 4 * r32[7:0]."
"INCSSPQ r64","INCSSPQ r64","incsspq r64","F3 REX.W 0F AE /05","N.E.","V","CET_SS","","","","","Increment SSP by 8 * r64[7:0]."
"INSB","INSB","insb","6C","V","V","","","","","","Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI."
"INSD","INSL","insl","6D","V","V","","operand32,operand64","","","","Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INSERTPS xmm1, xmm2/m32, imm8","INSERTPS imm8, xmm2/m32, xmm1","insertps imm8, xmm2/m32, xmm1","66 0F 3A 21 /r ib","V","V","SSE4_1","","","","","Insert a single precision floating-point value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8."
"INSW","INSW","insw","6D","V","V","","operand16","","","","Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INT imm8","INT imm8","int imm8","CD ib","V","V","","","r","","","Generate software interrupt with vector specified by immediate byte."
"INT1","INT1","int1","F1","V","V","","","","","","Generate debug trap."
"INT3","INT3","int3","CC","V","V","","","","","","Generate breakpoint trap."
"INTO","INTO","into","CE","V","I","","","","","","Generate overflow trap if overflow flag is 1."
"INVPCID r32, m128","INVPCID m128, r32","invpcid m128, r32","66 0F 38 82 /r","V","N.E.","INVPCID","","","","","Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descrip- tor in m128."
"INVPCID r64, m128","INVPCID m128, r64","invpcid m128, r64","66 0F 38 82 /r","N.E.","V","INVPCID","","","","","Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descrip- tor in m128."
"IRET","IRETW","iretw","CF","V","V","","operand16","","","","Interrupt return (16-bit operand size)."
"IRETD","IRETL","iretl","CF","V","V","","operand32","","","","Interrupt return (32-bit operand size)."
"IRETQ","IRETQ","iretq","REX.W CF","N.E.","V","","","","","","Interrupt return (64-bit operand size)."
"JA rel16","JA rel16","ja rel16","0F 87 cw","V","N.S.","","operand16","","Y","","Jump near if above (CF=0 and ZF=0). Not supported in 64-bit mode."
"JA rel32","JA rel32","ja rel32","0F 87 cd","V","V","","operand32","","Y","","Jump near if above (CF=0 and ZF=0)."
"JA rel32","JA rel32","ja rel32","0F 87 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JA rel8","JA rel8","ja rel8","77 cb","V","V","","","","Y","","Jump short if above (CF=0 and ZF=0)."
"JAE rel16","JAE rel16","jae rel16","0F 83 cw","V","N.S.","","operand16","","Y","","Jump near if above or equal (CF=0). Not supported in 64-bit mode."
"JAE rel32","JAE rel32","jae rel32","0F 83 cd","V","V","","operand32","","Y","","Jump near if above or equal (CF=0)."
"JAE rel32","JAE rel32","jae rel32","0F 83 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JAE rel8","JAE rel8","jae rel8","73 cb","V","V","","","","Y","","Jump short if above or equal (CF=0)."
"JB rel16","JB rel16","jb rel16","0F 82 cw","V","N.S.","","operand16","","Y","","Jump near if below (CF=1). Not supported in 64-bit mode."
"JB rel32","JB rel32","jb rel32","0F 82 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JB rel32","JB rel32","jb rel32","0F 82 cd","V","V","","operand32","","Y","","Jump near if below (CF=1)."
"JB rel8","JB rel8","jb rel8","72 cb","V","V","","","","Y","","Jump short if below (CF=1)."
"JBE rel16","JBE rel16","jbe rel16","0F 86 cw","V","N.S.","","operand16","","Y","","Jump near if below or equal (CF=1 or ZF=1). Not supported in 64-bit mode."
"JBE rel32","JBE rel32","jbe rel32","0F 86 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JBE rel32","JBE rel32","jbe rel32","0F 86 cd","V","V","","operand32","","Y","","Jump near if below or equal (CF=1 or ZF=1)."
"JBE rel8","JBE rel8","jbe rel8","76 cb","V","V","","","","Y","","Jump short if below or equal (CF=1 or ZF=1)."
"JC rel16","JC rel16","jc rel16","0F 82 cw","V","N.S.","","pseudo","","","","Jump near if carry (CF=1). Not supported in 64-bit mode."
"JC rel32","JC rel32","jc rel32","0F 82 cd","V","V","","pseudo","","","","Jump near if carry (CF=1)."
"JC rel8","JC rel8","jc rel8","72 cb","V","V","","pseudo","","","","Jump short if carry (CF=1)."
"JCXZ rel8","JCXZ rel8","jcxz rel8","E3 cb","V","N.E.","","address16","","","","Jump short if CX register is 0."
"JE rel16","JE rel16","je rel16","0F 84 cw","V","N.S.","","operand16","","Y","","Jump near if equal (ZF=1). Not supported in 64-bit mode."
"JE rel32","JE rel32","je rel32","0F 84 cd","V","V","","operand32","","Y","","Jump near if equal (ZF=1)."
"JE rel32","JE rel32","je rel32","0F 84 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JE rel8","JE rel8","je rel8","74 cb","V","V","","","","Y","","Jump short if equal (ZF=1)."
"JECXZ rel8","JECXZ rel8","jecxz rel8","E3 cb","V","V","","address32","","","","Jump short if ECX register is 0."
"JG rel16","JG rel16","jg rel16","0F 8F cw","V","N.S.","","operand16","","Y","","Jump near if greater (ZF=0 and SF=OF). Not supported in 64-bit mode."
"JG rel32","JG rel32","jg rel32","0F 8F cd","V","V","","operand32","","Y","","Jump near if greater (ZF=0 and SF=OF)."
"JG rel32","JG rel32","jg rel32","0F 8F cd","N.S.","V","","operand16,operand64","r","Y","",""
"JG rel8","JG rel8","jg rel8","7F cb","V","V","","","","Y","","Jump short if greater (ZF=0 and SF=OF)."
"JGE rel16","JGE rel16","jge rel16","0F 8D cw","V","N.S.","","operand16","","Y","","Jump near if greater or equal (SF=OF). Not supported in 64-bit mode."
"JGE rel32","JGE rel32","jge rel32","0F 8D cd","N.S.","V","","operand16,operand64","r","Y","",""
"JGE rel32","JGE rel32","jge rel32","0F 8D cd","V","V","","operand32","","Y","","Jump near if greater or equal (SF=OF)."
"JGE rel8","JGE rel8","jge rel8","7D cb","V","V","","","","Y","","Jump short if greater or equal (SF=OF)."
"JL rel16","JL rel16","jl rel16","0F 8C cw","V","N.S.","","operand16","","Y","","Jump near if less (SF≠ OF). Not supported in 64-bit mode."
"JL rel32","JL rel32","jl rel32","0F 8C cd","N.S.","V","","operand16,operand64","r","Y","",""
"JL rel32","JL rel32","jl rel32","0F 8C cd","V","V","","operand32","","Y","","Jump near if less (SF≠ OF)."
"JL rel8","JL rel8","jl rel8","7C cb","V","V","","","","Y","","Jump short if less (SF≠ OF)."
"JLE rel16","JLE rel16","jle rel16","0F 8E cw","V","N.S.","","operand16","","Y","","Jump near if less or equal (ZF=1 or SF≠ OF). Not supported in 64-bit mode."
"JLE rel32","JLE rel32","jle rel32","0F 8E cd","N.S.","V","","operand16,operand64","r","Y","",""
"JLE rel32","JLE rel32","jle rel32","0F 8E cd","V","V","","operand32","","Y","","Jump near if less or equal (ZF=1 or SF≠ OF)."
"JLE rel8","JLE rel8","jle rel8","7E cb","V","V","","","","Y","","Jump short if less or equal (ZF=1 or SF≠ OF)."
"JMP r/m16","JMPW* r/m16","jmpw* r/m16","FF /4","V","N.S.","","operand16","","Y","16","Jump near, absolute indirect, address = zero- extended r/m16. Not supported in 64-bit mode."
"JMP r/m32","JMPL* r/m32","jmpl* r/m32","FF /4","V","N.S.","","operand32","","Y","32","Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode."
"JMP r/m64","JMPQ* r/m64","jmpq* r/m64","FF /4","N.E.","V","","","","Y","64","Jump near, absolute indirect, RIP = 64-Bit offset from register or memory."
"JMP rel16","JMP rel16","jmp rel16","E9 cw","V","N.S.","","operand16","","Y","","Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode."
"JMP rel32","JMP rel32","jmp rel32","E9 cd","V","V","","operand32","","Y","","Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits."
"JMP rel32","JMP rel32","jmp rel32","E9 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JMP rel8","JMP rel8","jmp rel8","EB cb","V","V","","","","Y","","Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits."
"JMP_FAR m16:16","LJMPW* m16:16","ljmpw* m16:16","FF /5","V","V","","operand16","","Y","","Jump far, absolute indirect, address given in m16:16."
"JMP_FAR m16:32","LJMPL* m16:32","ljmpl* m16:32","FF /5","V","V","","operand32","","Y","","Jump far, absolute indirect, address given in m16:32."
"JMP_FAR m16:64","LJMPQ* m16:64","ljmpq* m16:64","REX.W FF /5","N.E.","V","","","","Y","","Jump far, absolute indirect, address given in m16:64."
"JMP_FAR ptr16:16","LJMPW ptr16:16","ljmpw ptr16:16","EA cd","V","I","","operand16","","Y","","Jump far, absolute, address given in operand."
"JMP_FAR ptr16:32","LJMPL ptr16:32","ljmpl ptr16:32","EA cp","V","I","","operand32","","Y","","Jump far, absolute, address given in operand."
"JNA rel16","JNA rel16","jna rel16","0F 86 cw","V","N.S.","","pseudo","","","","Jump near if not above (CF=1 or ZF=1). Not supported in 64-bit mode."
"JNA rel32","JNA rel32","jna rel32","0F 86 cd","V","V","","pseudo","","","","Jump near if not above (CF=1 or ZF=1)."
"JNA rel8","JNA rel8","jna rel8","76 cb","V","V","","pseudo","","","","Jump short if not above (CF=1 or ZF=1)."
"JNAE rel16","JNAE rel16","jnae rel16","0F 82 cw","V","N.S.","","pseudo","","","","Jump near if not above or equal (CF=1). Not supported in 64-bit mode."
"JNAE rel32","JNAE rel32","jnae rel32","0F 82 cd","V","V","","pseudo","","","","Jump near if not above or equal (CF=1)."
"JNAE rel8","JNAE rel8","jnae rel8","72 cb","V","V","","pseudo","","","","Jump short if not above or equal (CF=1)."
"JNB rel16","JNB rel16","jnb rel16","0F 83 cw","V","N.S.","","pseudo","","","","Jump near if not below (CF=0). Not supported in 64-bit mode."
"JNB rel32","JNB rel32","jnb rel32","0F 83 cd","V","V","","pseudo","","","","Jump near if not below (CF=0)."
"JNB rel8","JNB rel8","jnb rel8","73 cb","V","V","","pseudo","","","","Jump short if not below (CF=0)."
"JNBE rel16","JNBE rel16","jnbe rel16","0F 87 cw","V","N.S.","","pseudo","","","","Jump near if not below or equal (CF=0 and ZF=0). Not supported in 64-bit mode."
"JNBE rel32","JNBE rel32","jnbe rel32","0F 87 cd","V","V","","pseudo","","","","Jump near if not below or equal (CF=0 and ZF=0)."
"JNBE rel8","JNBE rel8","jnbe rel8","77 cb","V","V","","pseudo","","","","Jump short if not below or equal (CF=0 and ZF=0)."
"JNC rel16","JNC rel16","jnc rel16","0F 83 cw","V","N.S.","","pseudo","","","","Jump near if not carry (CF=0). Not supported in 64-bit mode."
"JNC rel32","JNC rel32","jnc rel32","0F 83 cd","V","V","","pseudo","","","","Jump near if not carry (CF=0)."
"JNC rel8","JNC rel8","jnc rel8","73 cb","V","V","","pseudo","","","","Jump short if not carry (CF=0)."
"JNE rel16","JNE rel16","jne rel16","0F 85 cw","V","N.S.","","operand16","","Y","","Jump near if not equal (ZF=0). Not supported in 64-bit mode."
"JNE rel32","JNE rel32","jne rel32","0F 85 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JNE rel32","JNE rel32","jne rel32","0F 85 cd","V","V","","operand32","","Y","","Jump near if not equal (ZF=0)."
"JNE rel8","JNE rel8","jne rel8","75 cb","V","V","","","","Y","","Jump short if not equal (ZF=0)."
"JNG rel16","JNG rel16","jng rel16","0F 8E cw","V","N.S.","","pseudo","","","","Jump near if not greater (ZF=1 or SF≠ OF). Not supported in 64-bit mode."
"JNG rel32","JNG rel32","jng rel32","0F 8E cd","V","V","","pseudo","","","","Jump near if not greater (ZF=1 or SF≠ OF)."
"JNG rel8","JNG rel8","jng rel8","7E cb","V","V","","pseudo","","","","Jump short if not greater (ZF=1 or SF≠ OF)."
"JNGE rel16","JNGE rel16","jnge rel16","0F 8C cw","V","N.S.","","pseudo","","","","Jump near if not greater or equal (SF≠ OF). Not supported in 64-bit mode."
"JNGE rel32","JNGE rel32","jnge rel32","0F 8C cd","V","V","","pseudo","","","","Jump near if not greater or equal (SF≠ OF)."
"JNGE rel8","JNGE rel8","jnge rel8","7C cb","V","V","","pseudo","","","","Jump short if not greater or equal (SF≠ OF)."
"JNL rel16","JNL rel16","jnl rel16","0F 8D cw","V","N.S.","","pseudo","","","","Jump near if not less (SF=OF). Not supported in 64-bit mode."
"JNL rel32","JNL rel32","jnl rel32","0F 8D cd","V","V","","pseudo","","","","Jump near if not less (SF=OF)."
"JNL rel8","JNL rel8","jnl rel8","7D cb","V","V","","pseudo","","","","Jump short if not less (SF=OF)."
"JNLE rel16","JNLE rel16","jnle rel16","0F 8F cw","V","N.S.","","pseudo","","","","Jump near if not less or equal (ZF=0 and SF=OF). Not supported in 64-bit mode."
"JNLE rel32","JNLE rel32","jnle rel32","0F 8F cd","V","V","","pseudo","","","","Jump near if not less or equal (ZF=0 and SF=OF)."
"JNLE rel8","JNLE rel8","jnle rel8","7F cb","V","V","","pseudo","","","","Jump short if not less or equal (ZF=0 and SF=OF)."
"JNO rel16","JNO rel16","jno rel16","0F 81 cw","V","N.S.","","operand16","","Y","","Jump near if not overflow (OF=0). Not supported in 64-bit mode."
"JNO rel32","JNO rel32","jno rel32","0F 81 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JNO rel32","JNO rel32","jno rel32","0F 81 cd","V","V","","operand32","","Y","","Jump near if not overflow (OF=0)."
"JNO rel8","JNO rel8","jno rel8","71 cb","V","V","","","","Y","","Jump short if not overflow (OF=0)."
"JNP rel16","JNP rel16","jnp rel16","0F 8B cw","V","N.S.","","operand16","","Y","","Jump near if not parity (PF=0). Not supported in 64-bit mode."
"JNP rel32","JNP rel32","jnp rel32","0F 8B cd","V","V","","operand32","","Y","","Jump near if not parity (PF=0)."
"JNP rel32","JNP rel32","jnp rel32","0F 8B cd","N.S.","V","","operand16,operand64","r","Y","",""
"JNP rel8","JNP rel8","jnp rel8","7B cb","V","V","","","","Y","","Jump short if not parity (PF=0)."
"JNS rel16","JNS rel16","jns rel16","0F 89 cw","V","N.S.","","operand16","","Y","","Jump near if not sign (SF=0). Not supported in 64-bit mode."
"JNS rel32","JNS rel32","jns rel32","0F 89 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JNS rel32","JNS rel32","jns rel32","0F 89 cd","V","V","","operand32","","Y","","Jump near if not sign (SF=0)."
"JNS rel8","JNS rel8","jns rel8","79 cb","V","V","","","","Y","","Jump short if not sign (SF=0)."
"JNZ rel16","JNZ rel16","jnz rel16","0F 85 cw","V","N.S.","","pseudo","","","","Jump near if not zero (ZF=0). Not supported in 64-bit mode."
"JNZ rel32","JNZ rel32","jnz rel32","0F 85 cd","V","V","","pseudo","","","","Jump near if not zero (ZF=0)."
"JNZ rel8","JNZ rel8","jnz rel8","75 cb","V","V","","pseudo","","","","Jump short if not zero (ZF=0)."
"JO rel16","JO rel16","jo rel16","0F 80 cw","V","N.S.","","operand16","","Y","","Jump near if overflow (OF=1). Not supported in 64-bit mode."
"JO rel32","JO rel32","jo rel32","0F 80 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JO rel32","JO rel32","jo rel32","0F 80 cd","V","V","","operand32","","Y","","Jump near if overflow (OF=1)."
"JO rel8","JO rel8","jo rel8","70 cb","V","V","","","","Y","","Jump short if overflow (OF=1)."
"JP rel16","JP rel16","jp rel16","0F 8A cw","V","N.S.","","operand16","","Y","","Jump near if parity (PF=1). Not supported in 64-bit mode."
"JP rel32","JP rel32","jp rel32","0F 8A cd","N.S.","V","","operand16,operand64","r","Y","",""
"JP rel32","JP rel32","jp rel32","0F 8A cd","V","V","","operand32","","Y","","Jump near if parity (PF=1)."
"JP rel8","JP rel8","jp rel8","7A cb","V","V","","","","Y","","Jump short if parity (PF=1)."
"JPE rel16","JPE rel16","jpe rel16","0F 8A cw","V","N.S.","","pseudo","","","","Jump near if parity even (PF=1). Not supported in 64-bit mode."
"JPE rel32","JPE rel32","jpe rel32","0F 8A cd","V","V","","pseudo","","","","Jump near if parity even (PF=1)."
"JPE rel8","JPE rel8","jpe rel8","7A cb","V","V","","pseudo","","","","Jump short if parity even (PF=1)."
"JPO rel16","JPO rel16","jpo rel16","0F 8B cw","V","N.S.","","pseudo","","","","Jump near if parity odd (PF=0). Not supported in 64-bit mode."
"JPO rel32","JPO rel32","jpo rel32","0F 8B cd","V","V","","pseudo","","","","Jump near if parity odd (PF=0)."
"JPO rel8","JPO rel8","jpo rel8","7B cb","V","V","","pseudo","","","","Jump short if parity odd (PF=0)."
"JRCXZ rel8","JRCXZ rel8","jrcxz rel8","E3 cb","N.E.","V","","address64","","","","Jump short if RCX register is 0."
"JS rel16","JS rel16","js rel16","0F 88 cw","V","N.S.","","operand16","","Y","","Jump near if sign (SF=1). Not supported in 64- bit mode."
"JS rel32","JS rel32","js rel32","0F 88 cd","V","V","","operand32","","Y","","Jump near if sign (SF=1)."
"JS rel32","JS rel32","js rel32","0F 88 cd","N.S.","V","","operand16,operand64","r","Y","",""
"JS rel8","JS rel8","js rel8","78 cb","V","V","","","","Y","","Jump short if sign (SF=1)."
"JZ rel16","JZ rel16","jz rel16","0F 84 cw","V","N.S.","","operand16,pseudo","","","","Jump near if 0 (ZF=1). Not supported in 64-bit mode."
"JZ rel32","JZ rel32","jz rel32","0F 84 cd","V","V","","operand32,pseudo","","","","Jump near if 0 (ZF=1)."
"JZ rel8","JZ rel8","jz rel8","74 cb","V","V","","pseudo","","","","Jump short if zero (ZF = 1)."
"KADDB k1, k2, k3","KADDB k3, k2, k1","kaddb k3, k2, k1","VEX.L1.66.0F.W0 4A /r","V","V","AVX512DQ","","","","","Add 8 bits masks in k2 and k3 and place result in k1."
"KADDD k1, k2, k3","KADDD k3, k2, k1","kaddd k3, k2, k1","VEX.L1.66.0F.W1 4A /r","V","V","AVX512BW","","","","","Add 32 bits masks in k2 and k3 and place result in k1."
"KADDQ k1, k2, k3","KADDQ k3, k2, k1","kaddq k3, k2, k1","VEX.L1.0F.W1 4A /r","V","V","AVX512BW","","","","","Add 64 bits masks in k2 and k3 and place result in k1."
"KADDW k1, k2, k3","KADDW k3, k2, k1","kaddw k3, k2, k1","VEX.L1.0F.W0 4A /r","V","V","AVX512DQ","","","","","Add 16 bits masks in k2 and k3 and place result in k1."
"KANDB k1, k2, k3","KANDB k3, k2, k1","kandb k3, k2, k1","VEX.L1.66.0F.W0 41 /r","V","V","AVX512DQ","","","","","Bitwise AND 8 bits masks k2 and k3 and place result in k1."
"KANDD k1, k2, k3","KANDD k3, k2, k1","kandd k3, k2, k1","VEX.L1.66.0F.W1 41 /r","V","V","AVX512BW","","","","","Bitwise AND 32 bits masks k2 and k3 and place result in k1."
"KANDNB k1, k2, k3","KANDNB k3, k2, k1","kandnb k3, k2, k1","VEX.L1.66.0F.W0 42 /r","V","V","AVX512DQ","","","","","Bitwise AND NOT 8 bits masks k1 and k2 and place result in k1."
"KANDND k1, k2, k3","KANDND k3, k2, k1","kandnd k3, k2, k1","VEX.L1.66.0F.W1 42 /r","V","V","AVX512BW","","","","","Bitwise AND NOT 32 bits masks k2 and k3 and place result in k1."
"KANDNQ k1, k2, k3","KANDNQ k3, k2, k1","kandnq k3, k2, k1","VEX.L1.0F.W1 42 /r","V","V","AVX512BW","","","","","Bitwise AND NOT 64 bits masks k2 and k3 and place result in k1."
"KANDNW k1, k2, k3","KANDNW k3, k2, k1","kandnw k3, k2, k1","VEX.L1.0F.W0 42 /r","V","V","AVX512F","","","","","Bitwise AND NOT 16 bits masks k2 and k3 and place result in k1."
"KANDQ k1, k2, k3","KANDQ k3, k2, k1","kandq k3, k2, k1","VEX.L1.0F.W1 41 /r","V","V","AVX512BW","","","","","Bitwise AND 64 bits masks k2 and k3 and place result in k1."
"KANDW k1, k2, k3","KANDW k3, k2, k1","kandw k3, k2, k1","VEX.L1.0F.W0 41 /r","V","V","AVX512F","","","","","Bitwise AND 16 bits masks k2 and k3 and place result in k1."
"KMOVB k1, k2/m8","KMOVB k2/m8, k1","kmovb k2/m8, k1","VEX.L0.66.0F.W0 90 /r","V","V","AVX512DQ","","","","","Move 8 bits mask from k2/m8 and store the result in k1."
"KMOVB k1, r32","KMOVB r32, k1","kmovb r32, k1","VEX.L0.66.0F.W0 92 /r","V","V","AVX512DQ","","","","","Move 8 bits mask from r32 to k1."
"KMOVB m8, k1","KMOVB k1, m8","kmovb k1, m8","VEX.L0.66.0F.W0 91 /r","V","V","AVX512DQ","","","","","Move 8 bits mask from k1 and store the result in m8."
"KMOVB r32, k1","KMOVB k1, r32","kmovb k1, r32","VEX.L0.66.0F.W0 93 /r","V","V","AVX512DQ","","","","","Move 8 bits mask from k1 to r32."
"KMOVD k1, k2/m32","KMOVD k2/m32, k1","kmovd k2/m32, k1","VEX.L0.66.0F.W1 90 /r","V","V","AVX512BW","","","","","Move 32 bits mask from k2/m32 and store the result in k1."
"KMOVD k1, r32","KMOVD r32, k1","kmovd r32, k1","VEX.L0.F2.0F.W0 92 /r","V","V","AVX512BW","","","","","Move 32 bits mask from r32 to k1."
"KMOVD m32, k1","KMOVD k1, m32","kmovd k1, m32","VEX.L0.66.0F.W1 91 /r","V","V","AVX512BW","","","","","Move 32 bits mask from k1 and store the result in m32."
"KMOVD r32, k1","KMOVD k1, r32","kmovd k1, r32","VEX.L0.F2.0F.W0 93 /r","V","V","AVX512BW","","","","","Move 32 bits mask from k1 to r32."
"KMOVQ k1, k2/m64","KMOVQ k2/m64, k1","kmovq k2/m64, k1","VEX.L0.0F.W1 90 /r","V","V","AVX512BW","","","","","Move 64 bits mask from k2/m64 and store the result in k1."
"KMOVQ k1, r64","KMOVQ r64, k1","kmovq r64, k1","VEX.L0.F2.0F.W1 92 /r","I","V","AVX512BW","","","","","Move 64 bits mask from r64 to k1."
"KMOVQ m64, k1","KMOVQ k1, m64","kmovq k1, m64","VEX.L0.0F.W1 91 /r","V","V","AVX512BW","","","","","Move 64 bits mask from k1 and store the result in m64."
"KMOVQ r64, k1","KMOVQ k1, r64","kmovq k1, r64","VEX.L0.F2.0F.W1 93 /r","I","V","AVX512BW","","","","","Move 64 bits mask from k1 to r64."
"KMOVW k1, k2/m16","KMOVW k2/m16, k1","kmovw k2/m16, k1","VEX.L0.0F.W0 90 /r","V","V","AVX512F","","","","","Move 16 bits mask from k2/m16 and store the result in k1."
"KMOVW k1, r32","KMOVW r32, k1","kmovw r32, k1","VEX.L0.0F.W0 92 /r","V","V","AVX512F","","","","","Move 16 bits mask from r32 to k1."
"KMOVW m16, k1","KMOVW k1, m16","kmovw k1, m16","VEX.L0.0F.W0 91 /r","V","V","AVX512F","","","","","Move 16 bits mask from k1 and store the result in m16."
"KMOVW r32, k1","KMOVW k1, r32","kmovw k1, r32","VEX.L0.0F.W0 93 /r","V","V","AVX512F","","","","","Move 16 bits mask from k1 to r32."
"KNOTB k1, k2","KNOTB k2, k1","knotb k2, k1","VEX.L0.66.0F.W0 44 /r","V","V","AVX512DQ","","","","","Bitwise NOT of 8 bits mask k2."
"KNOTD k1, k2","KNOTD k2, k1","knotd k2, k1","VEX.L0.66.0F.W1 44 /r","V","V","AVX512BW","","","","","Bitwise NOT of 32 bits mask k2."
"KNOTQ k1, k2","KNOTQ k2, k1","knotq k2, k1","VEX.L0.0F.W1 44 /r","V","V","AVX512BW","","","","","Bitwise NOT of 64 bits mask k2."
"KNOTW k1, k2","KNOTW k2, k1","knotw k2, k1","VEX.L0.0F.W0 44 /r","V","V","AVX512F","","","","","Bitwise NOT of 16 bits mask k2."
"KORB k1, k2, k3","KORB k3, k2, k1","korb k3, k2, k1","VEX.L1.66.0F.W0 45 /r","V","V","AVX512DQ","","","","","Bitwise OR 8 bits masks k2 and k3 and place result in k1."
"KORD k1, k2, k3","KORD k3, k2, k1","kord k3, k2, k1","VEX.L1.66.0F.W1 45 /r","V","V","AVX512BW","","","","","Bitwise OR 32 bits masks k2 and k3 and place result in k1."
"KORQ k1, k2, k3","KORQ k3, k2, k1","korq k3, k2, k1","VEX.L1.0F.W1 45 /r","V","V","AVX512BW","","","","","Bitwise OR 64 bits masks k2 and k3 and place result in k1."
"KORTESTB k1, k2","KORTESTB k2, k1","kortestb k2, k1","VEX.L0.66.0F.W0 98 /r","V","V","AVX512DQ","","","","","Bitwise OR 8 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTD k1, k2","KORTESTD k2, k1","kortestd k2, k1","VEX.L0.66.0F.W1 98 /r","V","V","AVX512BW","","","","","Bitwise OR 32 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTQ k1, k2","KORTESTQ k2, k1","kortestq k2, k1","VEX.L0.0F.W1 98 /r","V","V","AVX512BW","","","","","Bitwise OR 64 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTW k1, k2","KORTESTW k2, k1","kortestw k2, k1","VEX.L0.0F.W0 98 /r","V","V","AVX512F","","","","","Bitwise OR 16 bits masks k1 and k2 and update ZF and CF accordingly."
"KORW k1, k2, k3","KORW k3, k2, k1","korw k3, k2, k1","VEX.L1.0F.W0 45 /r","V","V","AVX512F","","","","","Bitwise OR 16 bits masks k2 and k3 and place result in k1."
"KSHIFTLB k1, k2, imm8","KSHIFTLB imm8, k2, k1","kshiftlb imm8, k2, k1","VEX.L0.66.0F3A.W0 32 /r","V","V","AVX512DQ","","","","","Shift left 8 bits in k2 by immediate and write result in k1."
"KSHIFTLD k1, k2, imm8","KSHIFTLD imm8, k2, k1","kshiftld imm8, k2, k1","VEX.L0.66.0F3A.W0 33 /r","V","V","AVX512BW","","","","","Shift left 32 bits in k2 by immediate and write result in k1."
"KSHIFTLQ k1, k2, imm8","KSHIFTLQ imm8, k2, k1","kshiftlq imm8, k2, k1","VEX.L0.66.0F3A.W1 33 /r","V","V","AVX512BW","","","","","Shift left 64 bits in k2 by immediate and write result in k1."
"KSHIFTLW k1, k2, imm8","KSHIFTLW imm8, k2, k1","kshiftlw imm8, k2, k1","VEX.L0.66.0F3A.W1 32 /r","V","V","AVX512F","","","","","Shift left 16 bits in k2 by immediate and write result in k1."
"KSHIFTRB k1, k2, imm8","KSHIFTRB imm8, k2, k1","kshiftrb imm8, k2, k1","VEX.L0.66.0F3A.W0 30 /r","V","V","AVX512DQ","","","","","Shift right 8 bits in k2 by immediate and write result in k1."
"KSHIFTRD k1, k2, imm8","KSHIFTRD imm8, k2, k1","kshiftrd imm8, k2, k1","VEX.L0.66.0F3A.W0 31 /r","V","V","AVX512BW","","","","","Shift right 32 bits in k2 by immediate and write result in k1."
"KSHIFTRQ k1, k2, imm8","KSHIFTRQ imm8, k2, k1","kshiftrq imm8, k2, k1","VEX.L0.66.0F3A.W1 31 /r","V","V","AVX512BW","","","","","Shift right 64 bits in k2 by immediate and write result in k1."
"KSHIFTRW k1, k2, imm8","KSHIFTRW imm8, k2, k1","kshiftrw imm8, k2, k1","VEX.L0.66.0F3A.W1 30 /r","V","V","AVX512F","","","","","Shift right 16 bits in k2 by immediate and write result in k1."
"KUNPCKBW k1, k2, k3","KUNPCKBW k3, k2, k1","kunpckbw k3, k2, k1","VEX.L1.66.0F.W0 4B /r","V","V","AVX512F","","","","","Unpack 8-bit masks in k2 and k3 and write word result in k1."
"KUNPCKDQ k1, k2, k3","KUNPCKDQ k3, k2, k1","kunpckdq k3, k2, k1","VEX.L1.0F.W1 4B /r","V","V","AVX512BW","","","","","Unpack 32-bit masks in k2 and k3 and write quadword result in k1."
"KUNPCKWD k1, k2, k3","KUNPCKWD k3, k2, k1","kunpckwd k3, k2, k1","VEX.L1.0F.W0 4B /r","V","V","AVX512BW","","","","","Unpack 16-bit masks in k2 and k3 and write doubleword result in k1."
"KXNORB k1, k2, k3","KXNORB k3, k2, k1","kxnorb k3, k2, k1","VEX.L1.66.0F.W0 46 /r","V","V","AVX512DQ","","","","","Bitwise XNOR 8-bit masks k2 and k3 and place result in k1."
"KXNORD k1, k2, k3","KXNORD k3, k2, k1","kxnord k3, k2, k1","VEX.L1.66.0F.W1 46 /r","V","V","AVX512BW","","","","","Bitwise XNOR 32-bit masks k2 and k3 and place result in k1."
"KXNORQ k1, k2, k3","KXNORQ k3, k2, k1","kxnorq k3, k2, k1","VEX.L1.0F.W1 46 /r","V","V","AVX512BW","","","","","Bitwise XNOR 64-bit masks k2 and k3 and place result in k1."
"KXNORW k1, k2, k3","KXNORW k3, k2, k1","kxnorw k3, k2, k1","VEX.L1.0F.W0 46 /r","V","V","AVX512F","","","","","Bitwise XNOR 16-bit masks k2 and k3 and place result in k1."
"KXORB k1, k2, k3","KXORB k3, k2, k1","kxorb k3, k2, k1","VEX.L1.66.0F.W0 47 /r","V","V","AVX512DQ","","","","","Bitwise XOR 8-bit masks k2 and k3 and place result in k1."
"KXORD k1, k2, k3","KXORD k3, k2, k1","kxord k3, k2, k1","VEX.L1.66.0F.W1 47 /r","V","V","AVX512BW","","","","","Bitwise XOR 32-bit masks k2 and k3 and place result in k1."
"KXORQ k1, k2, k3","KXORQ k3, k2, k1","kxorq k3, k2, k1","VEX.L1.0F.W1 47 /r","V","V","AVX512BW","","","","","Bitwise XOR 64-bit masks k2 and k3 and place result in k1."
"KXORW k1, k2, k3","KXORW k3, k2, k1","kxorw k3, k2, k1","VEX.L1.0F.W0 47 /r","V","V","AVX512F","","","","","Bitwise XOR 16-bit masks k2 and k3 and place result in k1."
"LAHF","LAHF","lahf","9F","V","V","","","","","","Load: AH := EFLAGS(SF:ZF:0:AF:0:PF:1:CF)."
"LAR r16, r/m16","LARW r/m16, r16","larw r/m16, r16","0F 02 /r","V","V","","","","Y","16","r16 := access rights referenced by r16/m16"
"LAR r64, r/m16","LARQ r/m16, r64","larq r/m16, r64","REX.W 0F 02 /r","N.E.","V","","","w,r","Y","64",""
"LAR reg, r32/m16","LAR r32/m16, reg","lar r32/m16, reg","0F 02 /r","V","V","","","","Y","","reg := access rights referenced by r32/m16"
"LDDQU xmm1, mem","LDDQU mem, xmm1","lddqu mem, xmm1","F2 0F F0 /r","V","V","SSE3","","","","","Load unaligned data from mem and return double quadword in xmm1."
"LDMXCSR m32","LDMXCSR m32","ldmxcsr m32","NP 0F AE /2","V","V","SSE","","","","","Load MXCSR register from m32."
"LDS r16, m16:16","LDSW m16:16, r16","ldsw m16:16, r16","C5 /r","V","I","","operand16","","Y","16","Load DS:r16 with far pointer from memory."
"LDS r32, m16:32","LDSL m16:32, r32","ldsl m16:32, r32","C5 /r","V","I","","operand32","","Y","32","Load DS:r32 with far pointer from memory."
"LEA r16, m","LEAW m, r16","leaw m, r16","8D /r","V","V","","operand16","","Y","16","Store effective address for m in register r16."
"LEA r32, m","LEAL m, r32","leal m, r32","8D /r","V","V","","operand32","","Y","32","Store effective address for m in register r32."
"LEA r64, m","LEAQ m, r64","leaq m, r64","REX.W 8D /r","N.E.","V","","","","Y","64","Store effective address for m in register r64."
"LEAVE","LEAVEW/LEAVEL/LEAVEQ","leavew/leavel/leaveq","C9","V","N.E.","","operand32","","Y","","Set ESP to EBP, then pop EBP."
"LEAVE","LEAVEW/LEAVEL/LEAVEQ","leavew/leavel/leaveq","C9","V","V","","operand16","","Y","","Set SP to BP, then pop BP."
"LEAVE","LEAVEW/LEAVEL/LEAVEQ","leavew/leavel/leaveq","C9","N.E.","V","","operand32,operand64","","Y","","Set RSP to RBP, then pop RBP."
"LES r16, m16:16","LESW m16:16, r16","lesw m16:16, r16","C4 /r","V","I","","operand16","","Y","16","Load ES:r16 with far pointer from memory."
"LES r32, m16:32","LESL m16:32, r32","lesl m16:32, r32","C4 /r","V","I","","operand32","","Y","32","Load ES:r32 with far pointer from memory."
"LFS r16, m16:16","LFSW m16:16, r16","lfsw m16:16, r16","0F B4 /r","V","V","","operand16","","Y","16","Load FS:r16 with far pointer from memory."
"LFS r32, m16:32","LFSL m16:32, r32","lfsl m16:32, r32","0F B4 /r","V","V","","operand32","","Y","32","Load FS:r32 with far pointer from memory."
"LFS r64, m16:64","LFSQ m16:64, r64","lfsq m16:64, r64","REX.W 0F B4 /r","N.E.","V","","","","Y","64","Load FS:r64 with far pointer from memory."
"LGDT m16&32","LGDTW/LGDTL m16&32","lgdtw/lgdtl m16&32","0F 01 /2","V","N.E.","","","","","","Load m into GDTR."
"LGDT m16&64","LGDT m16&64","lgdt m16&64","0F 01 /2","N.E.","V","","","","","","Load m into GDTR."
"LGS r16, m16:16","LGSW m16:16, r16","lgsw m16:16, r16","0F B5 /r","V","V","","operand16","","Y","16","Load GS:r16 with far pointer from memory."
"LGS r32, m16:32","LGSL m16:32, r32","lgsl m16:32, r32","0F B5 /r","V","V","","operand32","","Y","32","Load GS:r32 with far pointer from memory."
"LGS r64, m16:64","LGSQ m16:64, r64","lgsq m16:64, r64","REX.W 0F B5 /r","N.E.","V","","","","Y","64","Load GS:r64 with far pointer from memory."
"LIDT m16&32","LIDTW/LIDTL m16&32","lidtw/lidtl m16&32","0F 01 /3","V","N.E.","","","","","","Load m into IDTR."
"LIDT m16&64","LIDT m16&64","lidt m16&64","0F 01 /3","N.E.","V","","","","","","Load m into IDTR."
"LLDT r/m16","LLDT r/m16","lldt r/m16","0F 00 /2","V","V","","","","","","Load segment selector r/m16 into LDTR."
"LMSW r/m16","LMSW r/m16","lmsw r/m16","0F 01 /6","V","V","","","","","","Loads r/m16 in machine status word of CR0."
"LOADIWKEY xmm1, xmm2, <EAX>, <XMM0>","LOADIWKEY <XMM0>, <EAX>, xmm2, xmm1","loadiwkey <XMM0>, <EAX>, xmm2, xmm1","F3 0F 38 DC 11:rrr:bbb","V","V","KL","","","","","Load internal wrapping key from xmm1, xmm2, and XMM0."
"LODSB","LODSB","lodsb","AC","V","V","","","","","","For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL."
"LODSD","LODSL","lodsl","AD","V","V","","operand32","","","","For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX."
"LODSQ","LODSQ","lodsq","REX.W AD","N.E.","V","","","","","","Load qword at address (R)SI into RAX."
"LODSW","LODSW","lodsw","AD","V","V","","operand16","","","","For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX."
"LOOP rel8","LOOP rel8","loop rel8","E2 cb","V","V","","","","","","Decrement count; jump short if count ≠ 0."
"LOOPE rel8","LOOPEQ rel8","loope rel8","E1 cb","V","V","","","","","","Decrement count; jump short if count ≠ 0 and ZF = 1."
"LOOPNE rel8","LOOPNE rel8","loopne rel8","E0 cb","V","V","","","","","","Decrement count; jump short if count ≠ 0 and ZF = 0."
"LSL r16, r/m16","LSLW r/m16, r16","lslw r/m16, r16","0F 03 /r","V","V","","operand16","","Y","16","Load: r16 := segment limit, selector r16/m16."
"LSL r32, r32/m16","LSLL r32/m16, r32","lsll r32/m16, r32","0F 03 /r","V","V","","operand32","","Y","32","Load: r32 := segment limit, selector r32/m16."
"LSL r64, r32/m16","LSLQ r32/m16, r64","lslq r32/m16, r64","REX.W 0F 03 /r","V","V","","","","Y","64","Load: r64 := segment limit, selector r32/m16"
"LSS r16, m16:16","LSSW m16:16, r16","lssw m16:16, r16","0F B2 /r","V","V","","operand16","","Y","16","Load SS:r16 with far pointer from memory."
"LSS r32, m16:32","LSSL m16:32, r32","lssl m16:32, r32","0F B2 /r","V","V","","operand32","","Y","32","Load SS:r32 with far pointer from memory."
"LSS r64, m16:64","LSSQ m16:64, r64","lssq m16:64, r64","REX.W 0F B2 /r","N.E.","V","","","","Y","64","Load SS:r64 with far pointer from memory."
"LTR r/m16","LTR r/m16","ltr r/m16","0F 00 /3","V","V","","","","","","Load r/m16 into task register."
"LZCNT r16, r/m16","LZCNTW r/m16, r16","lzcntw r/m16, r16","F3 0F BD /r","V","V","LZCNT","operand16","","Y","16","Count the number of leading zero bits in r/m16, return result in r16."
"LZCNT r32, r/m32","LZCNTL r/m32, r32","lzcntl r/m32, r32","F3 0F BD /r","V","V","LZCNT","operand32","","Y","32","Count the number of leading zero bits in r/m32, return result in r32."
"LZCNT r64, r/m64","LZCNTQ r/m64, r64","lzcntq r/m64, r64","F3 REX.W 0F BD /r","N.E.","V","LZCNT","","","Y","64","Count the number of leading zero bits in r/m64, return result in r64."
"MASKMOVDQU xmm1, xmm2","MASKMOVOU xmm2, xmm1","maskmovdqu xmm2, xmm1","66 0F F7 /r","V","V","SSE2","","","","","Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI."
"MASKMOVQ mm1, mm2","MASKMOVQ mm2, mm1","maskmovq mm2, mm1","NP 0F F7 /r","V","V","","","","","","Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI."
"MAXPD xmm1, xmm2/m128","MAXPD xmm2/m128, xmm1","maxpd xmm2/m128, xmm1","66 0F 5F /r","V","V","SSE2","","","","","Return the maximum double precision floating-point values between xmm1 and xmm2/m128."
"MAXPS xmm1, xmm2/m128","MAXPS xmm2/m128, xmm1","maxps xmm2/m128, xmm1","NP 0F 5F /r","V","V","SSE","","","","","Return the maximum single precision floating-point values between xmm1 and xmm2/mem."
"MAXSD xmm1, xmm2/m64","MAXSD xmm2/m64, xmm1","maxsd xmm2/m64, xmm1","F2 0F 5F /r","V","V","SSE2","","","","","Return the maximum scalar double precision floating-point value between xmm2/m64 and xmm1."
"MAXSS xmm1, xmm2/m32","MAXSS xmm2/m32, xmm1","maxss xmm2/m32, xmm1","F3 0F 5F /r","V","V","SSE","","","","","Return the maximum scalar single precision floating-point value between xmm2/m32 and xmm1."
"MINPD xmm1, xmm2/m128","MINPD xmm2/m128, xmm1","minpd xmm2/m128, xmm1","66 0F 5D /r","V","V","SSE2","","","","","Return the minimum double precision floating-point values between xmm1 and xmm2/mem"
"MINPS xmm1, xmm2/m128","MINPS xmm2/m128, xmm1","minps xmm2/m128, xmm1","NP 0F 5D /r","V","V","SSE","","","","","Return the minimum single precision floating-point values between xmm1 and xmm2/mem."
"MINSD xmm1, xmm2/m64","MINSD xmm2/m64, xmm1","minsd xmm2/m64, xmm1","F2 0F 5D /r","V","V","SSE2","","","","","Return the minimum scalar double precision floating- point value between xmm2/m64 and xmm1."
"MINSS xmm1, xmm2/m32","MINSS xmm2/m32, xmm1","minss xmm2/m32, xmm1","F3 0F 5D /r","V","V","SSE","","","","","Return the minimum scalar single precision floating-point value between xmm2/m32 and xmm1."
"MONITOR","MONITOR","monitor","0F 01 C8","V","V","","","","","","Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write- back memory caching type. The address is DS:RAX/EAX/AX."
"MOV AL, moffs8","MOVB/MOVB/MOVABSB moffs8, AL","movb/movb/movabsb moffs8, AL","A0","V","V","","ignoreREXW","","Y","8","Move byte at (seg:offset) to AL."
"MOV AL, moffs8","MOVB/MOVB/MOVABSB moffs8, AL","movb/movb/movabsb moffs8, AL","REX.W A0","N.E.","V","","pseudo","","Y","8","Move byte at (offset) to AL."
"MOV AX, moffs16","MOVW moffs16, AX","movw moffs16, AX","A1","V","V","","operand16","","Y","16","Move word at (seg:offset) to AX."
"MOV CR0-CR7, r32","MOVL r32, CR0-CR7","movl r32, CR0-CR7","0F 22 /r","V","N.E.","","","","Y","32","Move r32 to control register."
"MOV CR0-CR7, r64","MOVQ r64, CR0-CR7","movq r64, CR0-CR7","0F 22 /r","N.E.","V","","","","Y","64","Move r64 to extended control register."
"MOV CR8, r64","MOVQ r64, CR8","movq r64, CR8","REX.R + 0F 22 /0","N.E.","V","","","","Y","64","Move r64 to extended CR8."
"MOV DR0-DR7, r32","MOVL r32, DR0-DR7","movl r32, DR0-DR7","0F 23 /r","V","N.E.","","","","Y","32","Move r32 to debug register."
"MOV DR0-DR7, r64","MOVQ r64, DR0-DR7","movq r64, DR0-DR7","0F 23 /r","N.E.","V","","","","Y","64","Move r64 to extended debug register."
"MOV EAX, moffs32","MOVL moffs32, EAX","movl moffs32, EAX","A1","V","V","","operand32","","Y","32","Move doubleword at (seg:offset) to EAX."
"MOV RAX, moffs64","MOVQ moffs64, RAX","movabsq moffs64, RAX","REX.W A1","N.E.","V","","","","Y","64","Move quadword at (offset) to RAX."
"MOV Sreg, r/m16","MOVW r/m16, Sreg","movw r/m16, Sreg","8E /r","V","V","","operand16","","Y","16","Move r/m16 to segment register."
"MOV Sreg, r/m16","MOVW r/m16, Sreg","movw r/m16, Sreg","REX.W 8E /r","V","V","","","","Y","16","Move lower 16 bits of r/m64 to segment register."
"MOV Sreg, r32/m16","MOV{L/W} r32/m16, Sreg","mov{l/w} r32/m16, Sreg","8E /r","V","V","","operand32","w,r","Y","",""
"MOV moffs16, AX","MOVW AX, moffs16","movw AX, moffs16","A3","V","V","","operand16","","Y","16","Move AX to (seg:offset)."
"MOV moffs32, EAX","MOVL EAX, moffs32","movl EAX, moffs32","A3","V","V","","operand32","","Y","32","Move EAX to (seg:offset)."
"MOV moffs64, RAX","MOVQ RAX, moffs64","movabsq RAX, moffs64","REX.W A3","N.E.","V","","","","Y","64","Move RAX to (offset)."
"MOV moffs8, AL","MOVB/MOVB/MOVABSB AL, moffs8","movb/movb/movabsb AL, moffs8","A2","V","V","","ignoreREXW","","Y","8","Move AL to (seg:offset)."
"MOV moffs8, AL","MOVB/MOVB/MOVABSB AL, moffs8","movb/movb/movabsb AL, moffs8","REX.W A2","N.E.","V","","pseudo","","Y","8","Move AL to (offset)."
"MOV r/m16, Sreg","MOVW Sreg, r/m16","movw Sreg, r/m16","8C /r","V","V","","","","Y","16","Move segment register to r/m16."
"MOV r/m16, imm16","MOVW imm16, r/m16","movw imm16, r/m16","C7 /0 iw","V","V","","operand16","","Y","16","Move imm16 to r/m16."
"MOV r/m16, r16","MOVW r16, r/m16","movw r16, r/m16","89 /r","V","V","","operand16","","Y","16","Move r16 to r/m16."
"MOV r/m32, Sreg","MOVL Sreg, r/m32","movl Sreg, r/m32","8C /r","V","V","","operand32","w,r","Y","32",""
"MOV r/m32, imm32","MOVL imm32, r/m32","movl imm32, r/m32","C7 /0 id","V","V","","operand32","","Y","32","Move imm32 to r/m32."
"MOV r/m32, r32","MOVL r32, r/m32","movl r32, r/m32","89 /r","V","V","","operand32","","Y","32","Move r32 to r/m32."
"MOV r/m64, imm32","MOVQ imm32, r/m64","movq imm32, r/m64","REX.W C7 /0 id","N.E.","V","","","","Y","64","Move imm32 sign extended to 64-bits to r/m64."
"MOV r/m64, r64","MOVQ r64, r/m64","movq r64, r/m64","REX.W 89 /r","N.E.","V","","","","Y","64","Move r64 to r/m64."
"MOV r/m8, imm8u","MOVB imm8u, r/m8","movb imm8u, r/m8","C6 /0 ib","V","V","","","","Y","8","Move imm8 to r/m8."
"MOV r/m8, imm8u","MOVB imm8u, r/m8","movb imm8u, r/m8","REX C6 /0 ib","N.E.","V","","pseudo64","","Y","8","Move imm8 to r/m8."
"MOV r/m8, r8","MOVB r8, r/m8","movb r8, r/m8","88 /r","V","V","","","","Y","8","Move r8 to r/m8."
"MOV r/m8, r8","MOVB r8, r/m8","movb r8, r/m8","REX 88 /r","N.E.","V","","pseudo64","","Y","8","Move r8 to r/m8."
"MOV r16, imm16","MOVW imm16, r16","movw imm16, r16","B8+rw iw","V","V","","operand16","","Y","16","Move imm16 to r16."
"MOV r16, r/m16","MOVW r/m16, r16","movw r/m16, r16","8B /r","V","V","","operand16","","Y","16","Move r/m16 to r16."
"MOV r16/r32/m16, Sreg","MOV Sreg, r16/r32/m16","mov Sreg, r16/r32/m16","8C /r","V","V","","operand16,operand32","","Y","","Move zero extended 16-bit segment register to r16/r32/m16."
"MOV r32, CR0-CR7","MOVL CR0-CR7, r32","movl CR0-CR7, r32","0F 20 /r","V","N.E.","","","","Y","32","Move control register to r32."
"MOV r32, DR0-DR7","MOVL DR0-DR7, r32","movl DR0-DR7, r32","0F 21 /r","V","N.E.","","","","Y","32","Move debug register to r32."
"MOV r32, imm32","MOVL imm32, r32","movl imm32, r32","B8+rd id","V","V","","operand32","","Y","32","Move imm32 to r32."
"MOV r32, r/m32","MOVL r/m32, r32","movl r/m32, r32","8B /r","V","V","","operand32","","Y","32","Move r/m32 to r32."
"MOV r64, CR0-CR7","MOVQ CR0-CR7, r64","movq CR0-CR7, r64","0F 20 /r","N.E.","V","","","","Y","64","Move extended control register to r64."
"MOV r64, CR8","MOVQ CR8, r64","movq CR8, r64","REX.R + 0F 20 /0","N.E.","V","","","","Y","64","Move extended CR8 to r64."
"MOV r64, DR0-DR7","MOVQ DR0-DR7, r64","movq DR0-DR7, r64","0F 21 /r","N.E.","V","","","","Y","64","Move extended debug register to r64."
"MOV r64, imm64","MOVQ imm64, r64","movq imm64, r64","REX.W B8+rd io","N.E.","V","","","","Y","64","Move imm64 to r64."
"MOV r64, r/m64","MOVQ r/m64, r64","movq r/m64, r64","REX.W 8B /r","N.E.","V","","","","Y","64","Move r/m64 to r64."
"MOV r64/m16, Sreg","MOV{Q/W} Sreg, r64/m16","mov{q/w} Sreg, r64/m16","REX.W 8C /r","V","V","","","","Y","","Move zero extended 16-bit segment register to r64/m16."
"MOV r8, imm8","MOVB imm8, r8","movb imm8, r8","B0+rb ib","V","V","","","","Y","8","Move imm8 to r8."
"MOV r8, imm8","MOVB imm8, r8","movb imm8, r8","REX B0+rb ib","N.E.","V","","pseudo64","","Y","8","Move imm8 to r8."
"MOV r8, r/m8","MOVB r/m8, r8","movb r/m8, r8","8A /r","V","V","","","","Y","8","Move r/m8 to r8."
"MOV r8, r/m8","MOVB r/m8, r8","movb r/m8, r8","REX 8A /r","N.E.","V","","pseudo64","","Y","8","Move r/m8 to r8."
"MOVAPD xmm1, xmm2/m128","MOVAPD xmm2/m128, xmm1","movapd xmm2/m128, xmm1","66 0F 28 /r","V","V","SSE2","","","","","Move aligned packed double precision floating- point values from xmm2/mem to xmm1."
"MOVAPD xmm2/m128, xmm1","MOVAPD xmm1, xmm2/m128","movapd xmm1, xmm2/m128","66 0F 29 /r","V","V","SSE2","","","","","Move aligned packed double precision floating- point values from xmm1 to xmm2/mem."
"MOVAPS xmm1, xmm2/m128","MOVAPS xmm2/m128, xmm1","movaps xmm2/m128, xmm1","NP 0F 28 /r","V","V","SSE","","","","","Move aligned packed single precision floating-point values from xmm2/mem to xmm1."
"MOVAPS xmm2/m128, xmm1","MOVAPS xmm1, xmm2/m128","movaps xmm1, xmm2/m128","NP 0F 29 /r","V","V","SSE","","","","","Move aligned packed single precision floating-point values from xmm1 to xmm2/mem."
"MOVBE m16, r16","MOVBEWW r16, m16","movbeww r16, m16","0F 38 F1 /r","V","V","MOVBE","operand16","","Y","16","Reverse byte order in r16 and move to m16."
"MOVBE m32, r32","MOVBELL r32, m32","movbell r32, m32","0F 38 F1 /r","V","V","MOVBE","operand32","","Y","32","Reverse byte order in r32 and move to m32."
"MOVBE m64, r64","MOVBEQQ r64, m64","movbeqq r64, m64","REX.W 0F 38 F1 /r","N.E.","V","MOVBE","","","Y","64","Reverse byte order in r64 and move to m64."
"MOVBE r16, m16","MOVBEWW m16, r16","movbeww m16, r16","0F 38 F0 /r","V","V","MOVBE","operand16","","Y","16","Reverse byte order in m16 and move to r16."
"MOVBE r32, m32","MOVBELL m32, r32","movbell m32, r32","0F 38 F0 /r","V","V","MOVBE","operand32","","Y","32","Reverse byte order in m32 and move to r32."
"MOVBE r64, m64","MOVBEQQ m64, r64","movbeqq m64, r64","REX.W 0F 38 F0 /r","N.E.","V","MOVBE","","","Y","64","Reverse byte order in m64 and move to r64."
"MOVD mm, r/m32","MOVD r/m32, mm","movd r/m32, mm","NP 0F 6E /r","V","V","MMX","operand16,operand32","","","","Move doubleword from r/m32 to mm."
"MOVD r/m32, mm","MOVD mm, r/m32","movd mm, r/m32","NP 0F 7E /r","V","V","MMX","operand16,operand32","","","","Move doubleword from mm to r/m32."
"MOVD r/m32, xmm","MOVD xmm, r/m32","movd xmm, r/m32","66 0F 7E /r","V","V","SSE2","operand16,operand32","","","","Move doubleword from xmm register to r/m32."
"MOVD xmm, r/m32","MOVD r/m32, xmm","movd r/m32, xmm","66 0F 6E /r","V","V","SSE2","operand16,operand32","","","","Move doubleword from r/m32 to xmm."
"MOVDIR64B r16/r32/r64, m512","MOVDIR64B m512, r16/r32/r64","movdir64b m512, r16/r32/r64","66 0F 38 F8 /r","V","V","MOVDIR64B","","","","","Move 64-bytes as direct-store with guaranteed 64- byte write atomicity from the source memory operand address to destination memory address specified as offset to ES segment in the register operand."
"MOVDIRI m32, r32","MOVDIRIL r32, m32","movdiril r32, m32","NP 0F 38 F9 /r","V","V","MOVDIRI","operand16,operand32","","Y","32","Move doubleword from r32 to m32 using direct store."
"MOVDIRI m64, r64","MOVDIRIQ r64, m64","movdiriq r64, m64","NP REX.W 0F 38 F9 /r","N.E.","V","MOVDIRI","","","Y","64","Move quadword from r64 to m64 using direct store."
"MOVDQ2Q mm, xmm","MOVQ xmm, mm","movdq2q xmm, mm","F2 0F D6 /r","V","V","","","","","","Move low quadword from xmm to mmx register."
"MOVDQA xmm1, xmm2/m128","MOVO xmm2/m128, xmm1","movdqa xmm2/m128, xmm1","66 0F 6F /r","V","V","SSE2","","","","","Move aligned packed integer values from xmm2/mem to xmm1."
"MOVDQA xmm2/m128, xmm1","MOVO xmm1, xmm2/m128","movdqa xmm1, xmm2/m128","66 0F 7F /r","V","V","SSE2","","","","","Move aligned packed integer values from xmm1 to xmm2/mem."
"MOVHLPS xmm1, xmm2","MOVHLPS xmm2, xmm1","movhlps xmm2, xmm1","NP 0F 12 /r","V","V","SSE","","","","","Move two packed single precision floating-point values from high quadword of xmm2 to low quadword of xmm1."
"MOVLHPS xmm1, xmm2","MOVLHPS xmm2, xmm1","movlhps xmm2, xmm1","NP 0F 16 /r","V","V","SSE","","","","","Move two packed single precision floating-point values from low quadword of xmm2 to high quadword of xmm1."
"MOVMSKPD reg, xmm","MOVMSKPD xmm, reg","movmskpd xmm, reg","66 0F 50 /r","V","V","SSE2","","","","","Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros."
"MOVMSKPS reg, xmm","MOVMSKPS xmm, reg","movmskps xmm, reg","NP 0F 50 /r","V","V","SSE","","","","","Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros."
"MOVNTDQ m128, xmm1","MOVNTO xmm1, m128","movntdq xmm1, m128","66 0F E7 /r","V","V","SSE2","","","","","Move packed integer values in xmm1 to m128 using non- temporal hint."
"MOVNTDQA xmm1, m128","MOVNTDQA m128, xmm1","movntdqa m128, xmm1","66 0F 38 2A /r","V","V","SSE4_1","","","","","Move double quadword from m128 to xmm1 using non- temporal hint if WC memory type."
"MOVNTPD m128, xmm1","MOVNTPD xmm1, m128","movntpd xmm1, m128","66 0F 2B /r","V","V","SSE2","","","","","Move packed double precision values in xmm1 to m128 using non-temporal hint."
"MOVNTPS m128, xmm1","MOVNTPS xmm1, m128","movntps xmm1, m128","NP 0F 2B /r","V","V","SSE","","","","","Move packed single precision values xmm1 to mem using non-temporal hint."
"MOVNTQ m64, mm","MOVNTQ mm, m64","movntq mm, m64","NP 0F E7 /r","V","V","","","","","","Move quadword from mm to m64 using non- temporal hint."
"MOVNTSD m64, xmm1","MOVNTSD xmm1, m64","movntsd xmm1, m64","F2 0F 2B /r","V","V","SSE","","w,r","","",""
"MOVNTSS m32, xmm1","MOVNTSS xmm1, m32","movntss xmm1, m32","F3 0F 2B /r","V","V","SSE","","w,r","","",""
"MOVQ mm, mm/m64","MOVQ mm/m64, mm","movq mm/m64, mm","NP 0F 6F /r","V","V","MMX","","","","","Move quadword from mm/m64 to mm."
"MOVQ mm, r/m64","MOVQ r/m64, mm","movq r/m64, mm","NP REX.W 0F 6E /r","N.E.","V","MMX","","","","","Move quadword from r/m64 to mm."
"MOVQ mm/m64, mm","MOVQ mm, mm/m64","movq mm, mm/m64","NP 0F 7F /r","V","V","MMX","","","","","Move quadword from mm to mm/m64."
"MOVQ r/m64, mm","MOVQ mm, r/m64","movq mm, r/m64","NP REX.W 0F 7E /r","N.E.","V","MMX","","","","","Move quadword from mm to r/m64."
"MOVQ r/m64, xmm","MOVQ xmm, r/m64","movq xmm, r/m64","66 REX.W 0F 7E /r","N.E.","V","SSE2","","","","","Move quadword from xmm register to r/m64."
"MOVQ xmm, r/m64","MOVQ r/m64, xmm","movq r/m64, xmm","66 REX.W 0F 6E /r","N.E.","V","SSE2","","","","","Move quadword from r/m64 to xmm."
"MOVQ xmm1, xmm2/m64","MOVQ xmm2/m64, xmm1","movq xmm2/m64, xmm1","F3 0F 7E /r","V","V","SSE2","","","","","Move quadword from xmm2/mem64 to xmm1."
"MOVQ xmm2/m64, xmm1","MOVQ xmm1, xmm2/m64","movq xmm1, xmm2/m64","66 0F D6 /r","V","V","SSE2","","","","","Move quadword from xmm1 to xmm2/mem64."
"MOVSB","MOVSB","movsb","A4","V","V","","","","","","For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI."
"MOVSD","MOVSL","movsl","A5","V","V","","operand32","","","","For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI."
"MOVSHDUP xmm1, xmm2/m128","MOVSHDUP xmm2/m128, xmm1","movshdup xmm2/m128, xmm1","F3 0F 16 /r","V","V","SSE3","","","","","Move odd index single precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"MOVSQ","MOVSQ","movsq","REX.W A5","N.E.","V","","","","","","Move qword from address (R|E)SI to (R|E)DI."
"MOVSW","MOVSW","movsw","A5","V","V","","operand16","","","","For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI."
"MOVSX r16, r/m16","MOVSWW r/m16, r16","movsww r/m16, r16","0F BF /r","V","V","","operand16","w,r","Y","16",""
"MOVSX r16, r/m8","MOVBWSX r/m8, r16","movsbw r/m8, r16","0F BE /r","V","V","","operand16","","Y","16","Move byte to word with sign-extension."
"MOVSX r32, r/m16","MOVWLSX r/m16, r32","movswl r/m16, r32","0F BF /r","V","V","","operand32","","Y","32","Move word to doubleword, with sign- extension."
"MOVSX r32, r/m8","MOVBLSX r/m8, r32","movsbl r/m8, r32","0F BE /r","V","V","","operand32","","Y","32","Move byte to doubleword with sign- extension."
"MOVSX r64, r/m16","MOVWQSX r/m16, r64","movswq r/m16, r64","REX.W 0F BF /r","N.E.","V","","","","Y","64","Move word to quadword with sign-extension."
"MOVSX r64, r/m8","MOVBQSX r/m8, r64","movsbq r/m8, r64","REX.W 0F BE /r","N.E.","V","","","","Y","64","Move byte to quadword with sign-extension."
"MOVSXD r16, r/m16","MOVWQSX r/m16, r16","movsxdw r/m16, r16","63 /r","N.E.","V","","operand16","","Y","16","Move word to word with sign-extension."
"MOVSXD r16, r/m32","MOVWQSX r/m32, r16","movsxdw r/m32, r16","63 /r","N.E.","V","","operand16","w,r","Y","16",""
"MOVSXD r32, r/m32","MOVLQSX r/m32, r32","movsxdl r/m32, r32","63 /r","N.E.","V","","operand32","w,r","Y","32",""
"MOVSXD r32, r/m32","MOVLQSX r/m32, r32","movsxdl r/m32, r32","63 /r","N.E.","V","","operand32","","Y","32","Move doubleword to doubleword with sign- extension."
"MOVSXD r64, r/m32","MOVLQSX r/m32, r64","movslq r/m32, r64","REX.W 63 /r","N.E.","V","","","","Y","64","Move doubleword to quadword with sign- extension."
"MOVZX r16, r/m16","MOVZWW r/m16, r16","movzww r/m16, r16","0F B7 /r","V","V","","operand16","w,r","Y","16",""
"MOVZX r16, r/m8","MOVBWZX r/m8, r16","movzbw r/m8, r16","0F B6 /r","V","V","","operand16","","Y","16","Move byte to word with zero-extension."
"MOVZX r32, r/m16","MOVWLZX r/m16, r32","movzwl r/m16, r32","0F B7 /r","V","V","","operand32","","Y","32","Move word to doubleword, zero-extension."
"MOVZX r32, r/m8","MOVBLZX r/m8, r32","movzbl r/m8, r32","0F B6 /r","V","V","","operand32","","Y","32","Move byte to doubleword, zero-extension."
"MOVZX r64, r/m16","MOVWQZX r/m16, r64","movzwq r/m16, r64","REX.W 0F B7 /r","N.E.","V","","","","Y","64","Move word to quadword, zero-extension."
"MOVZX r64, r/m8","MOVBQZX r/m8, r64","movzbq r/m8, r64","REX.W 0F B6 /r","N.E.","V","","","","Y","64","Move byte to quadword, zero-extension."
"MPSADBW xmm1, xmm2/m128, imm8","MPSADBW imm8, xmm2/m128, xmm1","mpsadbw imm8, xmm2/m128, xmm1","66 0F 3A 42 /r ib","V","V","SSE4_1","","","","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8."
"MUL r/m16","MULW r/m16","mulw r/m16","F7 /4","V","V","","operand16","","Y","16","Unsigned multiply (DX:AX := AX ∗ r/m16)."
"MUL r/m32","MULL r/m32","mull r/m32","F7 /4","V","V","","operand32","","Y","32","Unsigned multiply (EDX:EAX := EAX ∗ r/m32)."
"MUL r/m64","MULQ r/m64","mulq r/m64","REX.W F7 /4","N.E.","V","","","","Y","64","Unsigned multiply (RDX:RAX := RAX ∗ r/m64)."
"MUL r/m8","MULB r/m8","mulb r/m8","F6 /4","V","V","","","","Y","8","Unsigned multiply (AX := AL ∗ r/m8)."
"MUL r/m8","MULB r/m8","mulb r/m8","REX F6 /4","N.E.","V","","pseudo64","","Y","8","Unsigned multiply (AX := AL ∗ r/m8)."
"MULPD xmm1, xmm2/m128","MULPD xmm2/m128, xmm1","mulpd xmm2/m128, xmm1","66 0F 59 /r","V","V","SSE2","","","","","Multiply packed double precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1."
"MULPS xmm1, xmm2/m128","MULPS xmm2/m128, xmm1","mulps xmm2/m128, xmm1","NP 0F 59 /r","V","V","SSE","","","","","Multiply packed single precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1."
"MULSD xmm1, xmm2/m64","MULSD xmm2/m64, xmm1","mulsd xmm2/m64, xmm1","F2 0F 59 /r","V","V","SSE2","","","","","Multiply the low double precision floating-point value in xmm2/m64 by low double precision floating-point value in xmm1."
"MULSS xmm1, xmm2/m32","MULSS xmm2/m32, xmm1","mulss xmm2/m32, xmm1","F3 0F 59 /r","V","V","SSE","","","","","Multiply the low single precision floating-point value in xmm2/m32 by the low single precision floating-point value in xmm1."
"MULX r32a, r32b, r/m32","MULXL r/m32, r32b, r32a","mulxl r/m32, r32b, r32a","VEX.LZ.F2.0F38.W0 F6 /r","V","V","BMI2","","","Y","32","Unsigned multiply of r/m32 with EDX without affecting arithmetic flags."
"MULX r64a, r64b, r/m64","MULXQ r/m64, r64b, r64a","mulxq r/m64, r64b, r64a","VEX.LZ.F2.0F38.W1 F6 /r","N.E.","V","BMI2","","","Y","64","Unsigned multiply of r/m64 with RDX without affecting arithmetic flags."
"MWAIT","MWAIT","mwait","0F 01 C9","V","V","","","","","","A hint that allows the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events."
"NEG r/m16","NEGW r/m16","negw r/m16","F7 /3","V","V","","operand16","","Y","16","Two's complement negate r/m16."
"NEG r/m32","NEGL r/m32","negl r/m32","F7 /3","V","V","","operand32","","Y","32","Two's complement negate r/m32."
"NEG r/m64","NEGQ r/m64","negq r/m64","REX.W F7 /3","N.E.","V","","","","Y","64","Two's complement negate r/m64."
"NEG r/m8","NEGB r/m8","negb r/m8","F6 /3","V","V","","","","Y","8","Two's complement negate r/m8."
"NEG r/m8","NEGB r/m8","negb r/m8","REX F6 /3","N.E.","V","","pseudo64","","Y","8","Two's complement negate r/m8."
"NOP","NOP","nop","NP 90","V","V","","","","Y","","One byte no-operation instruction."
"NOP r/m16","NOPW r/m16","nopw r/m16","NP 0F 1F /0","V","V","","operand16","","Y","16","Multi-byte no-operation instruction."
"NOP r/m32","NOPL r/m32","nopl r/m32","NP 0F 1F /0","V","V","","operand32","","Y","32","Multi-byte no-operation instruction."
"NOT r/m16","NOTW r/m16","notw r/m16","F7 /2","V","V","","operand16","","Y","16","Reverse each bit of r/m16."
"NOT r/m32","NOTL r/m32","notl r/m32","F7 /2","V","V","","operand32","","Y","32","Reverse each bit of r/m32."
"NOT r/m64","NOTQ r/m64","notq r/m64","REX.W F7 /2","N.E.","V","","","","Y","64","Reverse each bit of r/m64."
"NOT r/m8","NOTB r/m8","notb r/m8","F6 /2","V","V","","","","Y","8","Reverse each bit of r/m8."
"NOT r/m8","NOTB r/m8","notb r/m8","REX F6 /2","N.E.","V","","pseudo64","","Y","8","Reverse each bit of r/m8."
"OR AL, imm8","ORB imm8, AL","orb imm8, AL","0C ib","V","V","","","","Y","8","AL OR imm8."
"OR AX, imm16","ORW imm16, AX","orw imm16, AX","0D iw","V","V","","operand16","","Y","16","AX OR imm16."
"OR EAX, imm32","ORL imm32, EAX","orl imm32, EAX","0D id","V","V","","operand32","","Y","32","EAX OR imm32."
"OR RAX, imm32","ORQ imm32, RAX","orq imm32, RAX","REX.W 0D id","N.E.","V","","","","Y","64","RAX OR imm32 (sign-extended)."
"OR r/m16, imm16","ORW imm16, r/m16","orw imm16, r/m16","81 /1 iw","V","V","","operand16","","Y","16","r/m16 OR imm16."
"OR r/m16, imm8","ORW imm8, r/m16","orw imm8, r/m16","83 /1 ib","V","V","","operand16","","Y","16","r/m16 OR imm8 (sign-extended)."
"OR r/m16, r16","ORW r16, r/m16","orw r16, r/m16","09 /r","V","V","","operand16","","Y","16","r/m16 OR r16."
"OR r/m32, imm32","ORL imm32, r/m32","orl imm32, r/m32","81 /1 id","V","V","","operand32","","Y","32","r/m32 OR imm32."
"OR r/m32, imm8","ORL imm8, r/m32","orl imm8, r/m32","83 /1 ib","V","V","","operand32","","Y","32","r/m32 OR imm8 (sign-extended)."
"OR r/m32, r32","ORL r32, r/m32","orl r32, r/m32","09 /r","V","V","","operand32","","Y","32","r/m32 OR r32."
"OR r/m64, imm32","ORQ imm32, r/m64","orq imm32, r/m64","REX.W 81 /1 id","N.E.","V","","","","Y","64","r/m64 OR imm32 (sign-extended)."
"OR r/m64, imm8","ORQ imm8, r/m64","orq imm8, r/m64","REX.W 83 /1 ib","N.E.","V","","","","Y","64","r/m64 OR imm8 (sign-extended)."
"OR r/m64, r64","ORQ r64, r/m64","orq r64, r/m64","REX.W 09 /r","N.E.","V","","","","Y","64","r/m64 OR r64."
"OR r/m8, imm8","ORB imm8, r/m8","orb imm8, r/m8","80 /1 ib","V","V","","","","Y","8","r/m8 OR imm8."
"OR r/m8, imm8","ORB imm8, r/m8","orb imm8, r/m8","REX 80 /1 ib","N.E.","V","","pseudo64","","Y","8","r/m8 OR imm8."
"OR r/m8, r8","ORB r8, r/m8","orb r8, r/m8","08 /r","V","V","","","","Y","8","r/m8 OR r8."
"OR r/m8, r8","ORB r8, r/m8","orb r8, r/m8","REX 08 /r","N.E.","V","","pseudo64","","Y","8","r/m8 OR r8."
"OR r16, r/m16","ORW r/m16, r16","orw r/m16, r16","0B /r","V","V","","operand16","","Y","16","r16 OR r/m16."
"OR r32, r/m32","ORL r/m32, r32","orl r/m32, r32","0B /r","V","V","","operand32","","Y","32","r32 OR r/m32."
"OR r64, r/m64","ORQ r/m64, r64","orq r/m64, r64","REX.W 0B /r","N.E.","V","","","","Y","64","r64 OR r/m64."
"OR r8, r/m8","ORB r/m8, r8","orb r/m8, r8","0A /r","V","V","","","","Y","8","r8 OR r/m8."
"OR r8, r/m8","ORB r/m8, r8","orb r/m8, r8","REX 0A /r","N.E.","V","","pseudo64","","Y","8","r8 OR r/m8."
"ORPD xmm1, xmm2/m128","ORPD xmm2/m128, xmm1","orpd xmm2/m128, xmm1","66 0F 56 /r","V","V","SSE2","","","","","Return the bitwise logical OR of packed double precision floating-point values in xmm1 and xmm2/mem."
"ORPS xmm1, xmm2/m128","ORPS xmm2/m128, xmm1","orps xmm2/m128, xmm1","NP 0F 56 /r","V","V","SSE","","","","","Return the bitwise logical OR of packed single precision floating-point values in xmm1 and xmm2/mem."
"PABSB mm1, mm2/m64","PABSB mm2/m64, mm1","pabsb mm2/m64, mm1","NP 0F 38 1C /r","V","V","SSSE3","","","","","Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1."
"PABSB xmm1, xmm2/m128","PABSB xmm2/m128, xmm1","pabsb xmm2/m128, xmm1","66 0F 38 1C /r","V","V","SSSE3","","","","","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1."
"PABSD mm1, mm2/m64","PABSD mm2/m64, mm1","pabsd mm2/m64, mm1","NP 0F 38 1E /r","V","V","SSSE3","","","","","Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1."
"PABSD xmm1, xmm2/m128","PABSD xmm2/m128, xmm1","pabsd xmm2/m128, xmm1","66 0F 38 1E /r","V","V","SSSE3","","","","","Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"PABSW mm1, mm2/m64","PABSW mm2/m64, mm1","pabsw mm2/m64, mm1","NP 0F 38 1D /r","V","V","SSSE3","","","","","Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1."
"PABSW xmm1, xmm2/m128","PABSW xmm2/m128, xmm1","pabsw xmm2/m128, xmm1","66 0F 38 1D /r","V","V","SSSE3","","","","","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"PACKSSDW mm1, mm2/m64","PACKSSLW mm2/m64, mm1","packssdw mm2/m64, mm1","NP 0F 6B /r","V","V","MMX","","","","","Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation."
"PACKSSDW xmm1, xmm2/m128","PACKSSLW xmm2/m128, xmm1","packssdw xmm2/m128, xmm1","66 0F 6B /r","V","V","SSE2","","","","","Converts 4 packed signed doubleword integers from xmm1 and from xmm2/m128 into 8 packed signed word integers in xmm1 using signed saturation."
"PACKSSWB mm1, mm2/m64","PACKSSWB mm2/m64, mm1","packsswb mm2/m64, mm1","NP 0F 63 /r","V","V","MMX","","","","","Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation."
"PACKSSWB xmm1, xmm2/m128","PACKSSWB xmm2/m128, xmm1","packsswb xmm2/m128, xmm1","66 0F 63 /r","V","V","SSE2","","","","","Converts 8 packed signed word integers from xmm1 and from xmm2/m128 into 16 packed signed byte integers in xmm1 using signed saturation."
"PACKUSDW xmm1, xmm2/m128","PACKUSDW xmm2/m128, xmm1","packusdw xmm2/m128, xmm1","66 0F 38 2B /r","V","V","SSE4_1","","","","","Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation."
"PACKUSWB mm, mm/m64","PACKUSWB mm/m64, mm","packuswb mm/m64, mm","NP 0F 67 /r","V","V","MMX","","","","","Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation."
"PACKUSWB xmm1, xmm2/m128","PACKUSWB xmm2/m128, xmm1","packuswb xmm2/m128, xmm1","66 0F 67 /r","V","V","SSE2","","","","","Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation."
"PADDSB mm, mm/m64","PADDSB mm/m64, mm","paddsb mm/m64, mm","NP 0F EC /r","V","V","MMX","","","","","Add packed signed byte integers from mm/m64 and mm and saturate the results."
"PADDSB xmm1, xmm2/m128","PADDSB xmm2/m128, xmm1","paddsb xmm2/m128, xmm1","66 0F EC /r","V","V","SSE2","","","","","Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results."
"PADDSW mm, mm/m64","PADDSW mm/m64, mm","paddsw mm/m64, mm","NP 0F ED /r","V","V","MMX","","","","","Add packed signed word integers from mm/m64 and mm and saturate the results."
"PADDSW xmm1, xmm2/m128","PADDSW xmm2/m128, xmm1","paddsw xmm2/m128, xmm1","66 0F ED /r","V","V","SSE2","","","","","Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results."
"PADDUSB mm, mm/m64","PADDUSB mm/m64, mm","paddusb mm/m64, mm","NP 0F DC /r","V","V","MMX","","","","","Add packed unsigned byte integers from mm/m64 and mm and saturate the results."
"PADDUSB xmm1, xmm2/m128","PADDUSB xmm2/m128, xmm1","paddusb xmm2/m128, xmm1","66 0F DC /r","V","V","SSE2","","","","","Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results."
"PADDUSW mm, mm/m64","PADDUSW mm/m64, mm","paddusw mm/m64, mm","NP 0F DD /r","V","V","MMX","","","","","Add packed unsigned word integers from mm/m64 and mm and saturate the results."
"PADDUSW xmm1, xmm2/m128","PADDUSW xmm2/m128, xmm1","paddusw xmm2/m128, xmm1","66 0F DD /r","V","V","SSE2","","","","","Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results."
"PALIGNR mm1, mm2/m64, imm8","PALIGNR imm8, mm2/m64, mm1","palignr imm8, mm2/m64, mm1","NP 0F 3A 0F /r ib","V","V","SSSE3","","","","","Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1."
"PALIGNR xmm1, xmm2/m128, imm8","PALIGNR imm8, xmm2/m128, xmm1","palignr imm8, xmm2/m128, xmm1","66 0F 3A 0F /r ib","V","V","SSSE3","","","","","Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1."
"PAND mm, mm/m64","PAND mm/m64, mm","pand mm/m64, mm","NP 0F DB /r","V","V","MMX","","","","","Bitwise AND mm/m64 and mm."
"PAND xmm1, xmm2/m128","PAND xmm2/m128, xmm1","pand xmm2/m128, xmm1","66 0F DB /r","V","V","SSE2","","","","","Bitwise AND of xmm2/m128 and xmm1."
"PANDN mm, mm/m64","PANDN mm/m64, mm","pandn mm/m64, mm","NP 0F DF /r","V","V","MMX","","","","","Bitwise AND NOT of mm/m64 and mm."
"PANDN xmm1, xmm2/m128","PANDN xmm2/m128, xmm1","pandn xmm2/m128, xmm1","66 0F DF /r","V","V","SSE2","","","","","Bitwise AND NOT of xmm2/m128 and xmm1."
"PAUSE","PAUSE","pause","F3 90","V","V","","pseudo","","","","Gives hint to processor that improves performance of spin-wait loops."
"PAVGB mm1, mm2/m64","PAVGB mm2/m64, mm1","pavgb mm2/m64, mm1","NP 0F E0 /r","V","V","SSE","","","","","Average packed unsigned byte integers from mm2/m64 and mm1 with rounding."
"PAVGB xmm1, xmm2/m128","PAVGB xmm2/m128, xmm1","pavgb xmm2/m128, xmm1","66 0F E0 /r","V","V","SSE2","","","","","Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding."
"PAVGW mm1, mm2/m64","PAVGW mm2/m64, mm1","pavgw mm2/m64, mm1","NP 0F E3 /r","V","V","SSE","","","","","Average packed unsigned word integers from mm2/m64 and mm1 with rounding."
"PAVGW xmm1, xmm2/m128","PAVGW xmm2/m128, xmm1","pavgw xmm2/m128, xmm1","66 0F E3 /r","V","V","SSE2","","","","","Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding."
"PBLENDVB xmm1, xmm2/m128, <XMM0>","PBLENDVB <XMM0>, xmm2/m128, xmm1","pblendvb <XMM0>, xmm2/m128, xmm1","66 0F 38 10 /r","V","V","SSE4_1","","","","","Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1."
"PBLENDW xmm1, xmm2/m128, imm8","PBLENDW imm8, xmm2/m128, xmm1","pblendw imm8, xmm2/m128, xmm1","66 0F 3A 0E /r ib","V","V","SSE4_1","","","","","Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"PCLMULQDQ xmm1, xmm2/m128, imm8","PCLMULQDQ imm8, xmm2/m128, xmm1","pclmulqdq imm8, xmm2/m128, xmm1","66 0F 3A 44 /r ib","V","V","PCLMULQDQ","","","","","Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The imme- diate is used to determine which quadwords of xmm1 and xmm2/m128 should be used."
"PCMPEQB mm, mm/m64","PCMPEQB mm/m64, mm","pcmpeqb mm/m64, mm","NP 0F 74 /r","V","V","MMX","","","","","Compare packed bytes in mm/m64 and mm for equality."
"PCMPEQB xmm1, xmm2/m128","PCMPEQB xmm2/m128, xmm1","pcmpeqb xmm2/m128, xmm1","66 0F 74 /r","V","V","SSE2","","","","","Compare packed bytes in xmm2/m128 and xmm1 for equality."
"PCMPEQD mm, mm/m64","PCMPEQL mm/m64, mm","pcmpeqd mm/m64, mm","NP 0F 76 /r","V","V","MMX","","","","","Compare packed doublewords in mm/m64 and mm for equality."
"PCMPEQD xmm1, xmm2/m128","PCMPEQL xmm2/m128, xmm1","pcmpeqd xmm2/m128, xmm1","66 0F 76 /r","V","V","SSE2","","","","","Compare packed doublewords in xmm2/m128 and xmm1 for equality."
"PCMPEQQ xmm1, xmm2/m128","PCMPEQQ xmm2/m128, xmm1","pcmpeqq xmm2/m128, xmm1","66 0F 38 29 /r","V","V","SSE4_1","","","","","Compare packed qwords in xmm2/m128 and xmm1 for equality."
"PCMPEQW mm, mm/m64","PCMPEQW mm/m64, mm","pcmpeqw mm/m64, mm","NP 0F 75 /r","V","V","MMX","","","","","Compare packed words in mm/m64 and mm for equality."
"PCMPEQW xmm1, xmm2/m128","PCMPEQW xmm2/m128, xmm1","pcmpeqw xmm2/m128, xmm1","66 0F 75 /r","V","V","SSE2","","","","","Compare packed words in xmm2/m128 and xmm1 for equality."
"PCMPESTRI xmm1, xmm2/m128, imm8","PCMPESTRI imm8, xmm2/m128, xmm1","pcmpestri imm8, xmm2/m128, xmm1","66 0F 3A 61 /r ib","V","V","SSE4_2","","","","","Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX."
"PCMPESTRM xmm1, xmm2/m128, imm8","PCMPESTRM imm8, xmm2/m128, xmm1","pcmpestrm imm8, xmm2/m128, xmm1","66 0F 3A 60 /r ib","V","V","SSE4_2","","","","","Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0."
"PCMPGTB mm, mm/m64","PCMPGTB mm/m64, mm","pcmpgtb mm/m64, mm","NP 0F 64 /r","V","V","MMX","","","","","Compare packed signed byte integers in mm and mm/m64 for greater than."
"PCMPGTB xmm1, xmm2/m128","PCMPGTB xmm2/m128, xmm1","pcmpgtb xmm2/m128, xmm1","66 0F 64 /r","V","V","SSE2","","","","","Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than."
"PCMPGTD mm, mm/m64","PCMPGTL mm/m64, mm","pcmpgtd mm/m64, mm","NP 0F 66 /r","V","V","MMX","","","","","Compare packed signed doubleword integers in mm and mm/m64 for greater than."
"PCMPGTD xmm1, xmm2/m128","PCMPGTL xmm2/m128, xmm1","pcmpgtd xmm2/m128, xmm1","66 0F 66 /r","V","V","SSE2","","","","","Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than."
"PCMPGTQ xmm1, xmm2/m128","PCMPGTQ xmm2/m128, xmm1","pcmpgtq xmm2/m128, xmm1","66 0F 38 37 /r","V","V","SSE4_2","","","","","Compare packed signed qwords in xmm2/m128 and xmm1 for greater than."
"PCMPGTW mm, mm/m64","PCMPGTW mm/m64, mm","pcmpgtw mm/m64, mm","NP 0F 65 /r","V","V","MMX","","","","","Compare packed signed word integers in mm and mm/m64 for greater than."
"PCMPGTW xmm1, xmm2/m128","PCMPGTW xmm2/m128, xmm1","pcmpgtw xmm2/m128, xmm1","66 0F 65 /r","V","V","SSE2","","","","","Compare packed signed word integers in xmm1 and xmm2/m128 for greater than."
"PCMPISTRI xmm1, xmm2/m128, imm8","PCMPISTRI imm8, xmm2/m128, xmm1","pcmpistri imm8, xmm2/m128, xmm1","66 0F 3A 63 /r ib","V","V","SSE4_2","","","","","Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX."
"PCMPISTRM xmm1, xmm2/m128, imm8","PCMPISTRM imm8, xmm2/m128, xmm1","pcmpistrm imm8, xmm2/m128, xmm1","66 0F 3A 62 /r ib","V","V","SSE4_2","","","","","Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0."
"PDEP r32a, r32b, r/m32","PDEPL r/m32, r32b, r32a","pdepl r/m32, r32b, r32a","VEX.LZ.F2.0F38.W0 F5 /r","V","V","BMI2","","","Y","32","Parallel deposit of bits from r32b using mask in r/m32, result is writ- ten to r32a."
"PDEP r64a, r64b, r/m64","PDEPQ r/m64, r64b, r64a","pdepq r/m64, r64b, r64a","VEX.LZ.F2.0F38.W1 F5 /r","N.E.","V","BMI2","","","Y","64","Parallel deposit of bits from r64b using mask in r/m64, result is writ- ten to r64a."
"PEXT r32a, r32b, r/m32","PEXTL r/m32, r32b, r32a","pextl r/m32, r32b, r32a","VEX.LZ.F3.0F38.W0 F5 /r","V","V","BMI2","","","Y","32","Parallel extract of bits from r32b using mask in r/m32, result is writ- ten to r32a."
"PEXT r64a, r64b, r/m64","PEXTQ r/m64, r64b, r64a","pextq r/m64, r64b, r64a","VEX.LZ.F3.0F38.W1 F5 /r","N.E.","V","BMI2","","","Y","64","Parallel extract of bits from r64b using mask in r/m64, result is writ- ten to r64a."
"PEXTRB reg/m8, xmm2, imm8","PEXTRB imm8, xmm2, reg/m8","pextrb imm8, xmm2, reg/m8","66 0F 3A 14 /r ib","V","V","SSE4_1","","","","","Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r32 or r64 are zeroed."
"PEXTRD r/m32, xmm2, imm8","PEXTRD imm8, xmm2, r/m32","pextrd imm8, xmm2, r/m32","66 0F 3A 16 /r ib","V","V","SSE4_1","operand16,operand32","","","","Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32."
"PEXTRQ r/m64, xmm2, imm8","PEXTRQ imm8, xmm2, r/m64","pextrq imm8, xmm2, r/m64","66 REX.W 0F 3A 16 /r ib","N.E.","V","SSE4_1","","","","","Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64."
"PEXTRW reg, mm, imm8","PEXTRW imm8, mm, reg","pextrw imm8, mm, reg","NP 0F C5 /r ib","V","V","SSE","","","","","Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed."
"PEXTRW reg, xmm, imm8","PEXTRW imm8, xmm, reg","pextrw imm8, xmm, reg","66 0F C5 /r ib","V","V","SSE2","","","","","Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed."
"PEXTRW reg/m16, xmm, imm8","PEXTRW imm8, xmm, reg/m16","pextrw imm8, xmm, reg/m16","66 0F 3A 15 /r ib","V","V","SSE4_1","","","","","Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero- extend the result in the destination, r32 or r64."
"PHADDD mm1, mm2/m64","PHADDD mm2/m64, mm1","phaddd mm2/m64, mm1","NP 0F 38 02 /r","V","V","SSSE3","","","","","Add 32-bit integers horizontally, pack to mm1."
"PHADDD xmm1, xmm2/m128","PHADDD xmm2/m128, xmm1","phaddd xmm2/m128, xmm1","66 0F 38 02 /r","V","V","SSSE3","","","","","Add 32-bit integers horizontally, pack to xmm1."
"PHADDSW mm1, mm2/m64","PHADDSW mm2/m64, mm1","phaddsw mm2/m64, mm1","NP 0F 38 03 /r","V","V","SSSE3","","","","","Add 16-bit signed integers horizontally, pack saturated integers to mm1."
"PHADDSW xmm1, xmm2/m128","PHADDSW xmm2/m128, xmm1","phaddsw xmm2/m128, xmm1","66 0F 38 03 /r","V","V","SSSE3","","","","","Add 16-bit signed integers horizontally, pack saturated integers to xmm1."
"PHADDW mm1, mm2/m64","PHADDW mm2/m64, mm1","phaddw mm2/m64, mm1","NP 0F 38 01 /r","V","V","SSSE3","","","","","Add 16-bit integers horizontally, pack to mm1."
"PHADDW xmm1, xmm2/m128","PHADDW xmm2/m128, xmm1","phaddw xmm2/m128, xmm1","66 0F 38 01 /r","V","V","SSSE3","","","","","Add 16-bit integers horizontally, pack to xmm1."
"PHMINPOSUW xmm1, xmm2/m128","PHMINPOSUW xmm2/m128, xmm1","phminposuw xmm2/m128, xmm1","66 0F 38 41 /r","V","V","SSE4_1","","","","","Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second- lowest word of xmm1."
"PHSUBD mm1, mm2/m64","PHSUBD mm2/m64, mm1","phsubd mm2/m64, mm1","NP 0F 38 06 /r","V","V","SSSE3","","","","","Subtract 32-bit signed integers horizontally, pack to mm1."
"PHSUBD xmm1, xmm2/m128","PHSUBD xmm2/m128, xmm1","phsubd xmm2/m128, xmm1","66 0F 38 06 /r","V","V","SSSE3","","","","","Subtract 32-bit signed integers horizontally, pack to xmm1."
"PHSUBSW mm1, mm2/m64","PHSUBSW mm2/m64, mm1","phsubsw mm2/m64, mm1","NP 0F 38 07 /r","V","V","SSSE3","","","","","Subtract 16-bit signed integer horizontally, pack saturated integers to mm1."
"PHSUBSW xmm1, xmm2/m128","PHSUBSW xmm2/m128, xmm1","phsubsw xmm2/m128, xmm1","66 0F 38 07 /r","V","V","SSSE3","","","","","Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1."
"PHSUBW mm1, mm2/m64","PHSUBW mm2/m64, mm1","phsubw mm2/m64, mm1","NP 0F 38 05 /r","V","V","SSSE3","","","","","Subtract 16-bit signed integers horizontally, pack to mm1."
"PHSUBW xmm1, xmm2/m128","PHSUBW xmm2/m128, xmm1","phsubw xmm2/m128, xmm1","66 0F 38 05 /r","V","V","SSSE3","","","","","Subtract 16-bit signed integers horizontally, pack to xmm1."
"PINSRB xmm1, r32/m8, imm8","PINSRB imm8, r32/m8, xmm1","pinsrb imm8, r32/m8, xmm1","66 0F 3A 20 /r ib","V","V","SSE4_1","","","","","Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8."
"PINSRD xmm1, r/m32, imm8","PINSRD imm8, r/m32, xmm1","pinsrd imm8, r/m32, xmm1","66 0F 3A 22 /r ib","V","V","SSE4_1","operand16,operand32","","","","Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8."
"PINSRQ xmm1, r/m64, imm8","PINSRQ imm8, r/m64, xmm1","pinsrq imm8, r/m64, xmm1","66 REX.W 0F 3A 22 /r ib","N.E.","V","SSE4_1","","","","","Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8."
"PINSRW mm, r32/m16, imm8","PINSRW imm8, r32/m16, mm","pinsrw imm8, r32/m16, mm","NP 0F C4 /r ib","V","V","SSE","","","","","Insert the low word from r32 or from m16 into mm at the word position specified by imm8."
"PINSRW xmm, r32/m16, imm8","PINSRW imm8, r32/m16, xmm","pinsrw imm8, r32/m16, xmm","66 0F C4 /r ib","V","V","SSE2","","","","","Move the low word of r32 or from m16 into xmm at the word position specified by imm8."
"PMADDUBSW mm1, mm2/m64","PMADDUBSW mm2/m64, mm1","pmaddubsw mm2/m64, mm1","NP 0F 38 04 /r","V","V","SSSE3","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to mm1."
"PMADDUBSW xmm1, xmm2/m128","PMADDUBSW xmm2/m128, xmm1","pmaddubsw xmm2/m128, xmm1","66 0F 38 04 /r","V","V","SSSE3","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1."
"PMADDWD mm, mm/m64","PMADDWL mm/m64, mm","pmaddwd mm/m64, mm","NP 0F F5 /r","V","V","MMX","","","","","Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm."
"PMADDWD xmm1, xmm2/m128","PMADDWL xmm2/m128, xmm1","pmaddwd xmm2/m128, xmm1","66 0F F5 /r","V","V","SSE2","","","","","Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1."
"PMAXSB xmm1, xmm2/m128","PMAXSB xmm2/m128, xmm1","pmaxsb xmm2/m128, xmm1","66 0F 38 3C /r","V","V","SSE4_1","","","","","Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"PMAXSD xmm1, xmm2/m128","PMAXSD xmm2/m128, xmm1","pmaxsd xmm2/m128, xmm1","66 0F 38 3D /r","V","V","SSE4_1","","","","","Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"PMAXSW mm1, mm2/m64","PMAXSW mm2/m64, mm1","pmaxsw mm2/m64, mm1","NP 0F EE /r","V","V","SSE","","","","","Compare signed word integers in mm2/m64 and mm1 and return maximum values."
"PMAXSW xmm1, xmm2/m128","PMAXSW xmm2/m128, xmm1","pmaxsw xmm2/m128, xmm1","66 0F EE /r","V","V","SSE2","","","","","Compare packed signed word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1."
"PMAXUB mm1, mm2/m64","PMAXUB mm2/m64, mm1","pmaxub mm2/m64, mm1","NP 0F DE /r","V","V","SSE","","","","","Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values."
"PMAXUB xmm1, xmm2/m128","PMAXUB xmm2/m128, xmm1","pmaxub xmm2/m128, xmm1","66 0F DE /r","V","V","SSE2","","","","","Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"PMAXUW xmm1, xmm2/m128","PMAXUW xmm2/m128, xmm1","pmaxuw xmm2/m128, xmm1","66 0F 38 3E /r","V","V","SSE4_1","","","","","Compare packed unsigned word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1."
"PMINSB xmm1, xmm2/m128","PMINSB xmm2/m128, xmm1","pminsb xmm2/m128, xmm1","66 0F 38 38 /r","V","V","SSE4_1","","","","","Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"PMINSW mm1, mm2/m64","PMINSW mm2/m64, mm1","pminsw mm2/m64, mm1","NP 0F EA /r","V","V","SSE","","","","","Compare signed word integers in mm2/m64 and mm1 and return minimum values."
"PMINSW xmm1, xmm2/m128","PMINSW xmm2/m128, xmm1","pminsw xmm2/m128, xmm1","66 0F EA /r","V","V","SSE2","","","","","Compare packed signed word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1."
"PMINUB mm1, mm2/m64","PMINUB mm2/m64, mm1","pminub mm2/m64, mm1","NP 0F DA /r","V","V","SSE","","","","","Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values."
"PMINUB xmm1, xmm2/m128","PMINUB xmm2/m128, xmm1","pminub xmm2/m128, xmm1","66 0F DA /r","V","V","SSE2","","","","","Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"PMINUD xmm1, xmm2/m128","PMINUD xmm2/m128, xmm1","pminud xmm2/m128, xmm1","66 0F 38 3B /r","V","V","SSE4_1","","","","","Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"PMINUW xmm1, xmm2/m128","PMINUW xmm2/m128, xmm1","pminuw xmm2/m128, xmm1","66 0F 38 3A /r","V","V","SSE4_1","","","","","Compare packed unsigned word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1."
"PMOVMSKB reg, mm","PMOVMSKB mm, reg","pmovmskb mm, reg","NP 0F D7 /r","V","V","SSE","","","","","Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed"
"PMOVMSKB reg, xmm","PMOVMSKB xmm, reg","pmovmskb xmm, reg","66 0F D7 /r","V","V","SSE2","","","","","Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed"
"PMOVSXBD xmm1, xmm2/m32","PMOVSXBD xmm2/m32, xmm1","pmovsxbd xmm2/m32, xmm1","66 0F 38 21 /r","V","V","SSE4_1","","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"PMOVSXBQ xmm1, xmm2/m16","PMOVSXBQ xmm2/m16, xmm1","pmovsxbq xmm2/m16, xmm1","66 0F 38 22 /r","V","V","SSE4_1","","","","","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"PMOVSXBW xmm1, xmm2/m64","PMOVSXBW xmm2/m64, xmm1","pmovsxbw xmm2/m64, xmm1","66 0F 38 20 /r","V","V","SSE4_1","","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"PMOVSXDQ xmm1, xmm2/m64","PMOVSXDQ xmm2/m64, xmm1","pmovsxdq xmm2/m64, xmm1","66 0F 38 25 /r","V","V","SSE4_1","","","","","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"PMOVSXWD xmm1, xmm2/m64","PMOVSXWD xmm2/m64, xmm1","pmovsxwd xmm2/m64, xmm1","66 0F 38 23 /r","V","V","SSE4_1","","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"PMOVSXWQ xmm1, xmm2/m32","PMOVSXWQ xmm2/m32, xmm1","pmovsxwq xmm2/m32, xmm1","66 0F 38 24 /r","V","V","SSE4_1","","","","","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"PMOVZXBD xmm1, xmm2/m32","PMOVZXBD xmm2/m32, xmm1","pmovzxbd xmm2/m32, xmm1","66 0F 38 31 /r","V","V","SSE4_1","","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"PMOVZXBQ xmm1, xmm2/m16","PMOVZXBQ xmm2/m16, xmm1","pmovzxbq xmm2/m16, xmm1","66 0F 38 32 /r","V","V","SSE4_1","","","","","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"PMOVZXBW xmm1, xmm2/m64","PMOVZXBW xmm2/m64, xmm1","pmovzxbw xmm2/m64, xmm1","66 0F 38 30 /r","V","V","SSE4_1","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"PMOVZXDQ xmm1, xmm2/m64","PMOVZXDQ xmm2/m64, xmm1","pmovzxdq xmm2/m64, xmm1","66 0F 38 35 /r","V","V","SSE4_1","","","","","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"PMOVZXWD xmm1, xmm2/m64","PMOVZXWD xmm2/m64, xmm1","pmovzxwd xmm2/m64, xmm1","66 0F 38 33 /r","V","V","SSE4_1","","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"PMOVZXWQ xmm1, xmm2/m32","PMOVZXWQ xmm2/m32, xmm1","pmovzxwq xmm2/m32, xmm1","66 0F 38 34 /r","V","V","SSE4_1","","","","","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"PMULDQ xmm1, xmm2/m128","PMULDQ xmm2/m128, xmm1","pmuldq xmm2/m128, xmm1","66 0F 38 28 /r","V","V","SSE4_1","","","","","Multiply packed signed doubleword integers in xmm1 by packed signed doubleword integers in xmm2/m128, and store the quadword results in xmm1."
"PMULHRSW mm1, mm2/m64","PMULHRSW mm2/m64, mm1","pmulhrsw mm2/m64, mm1","NP 0F 38 0B /r","V","V","SSSE3","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to mm1."
"PMULHRSW xmm1, xmm2/m128","PMULHRSW xmm2/m128, xmm1","pmulhrsw xmm2/m128, xmm1","66 0F 38 0B /r","V","V","SSSE3","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1."
"PMULHUW mm1, mm2/m64","PMULHUW mm2/m64, mm1","pmulhuw mm2/m64, mm1","NP 0F E4 /r","V","V","SSE","","","","","Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1."
"PMULHUW xmm1, xmm2/m128","PMULHUW xmm2/m128, xmm1","pmulhuw xmm2/m128, xmm1","66 0F E4 /r","V","V","SSE2","","","","","Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1."
"PMULHW mm, mm/m64","PMULHW mm/m64, mm","pmulhw mm/m64, mm","NP 0F E5 /r","V","V","MMX","","","","","Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1."
"PMULHW xmm1, xmm2/m128","PMULHW xmm2/m128, xmm1","pmulhw xmm2/m128, xmm1","66 0F E5 /r","V","V","SSE2","","","","","Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1."
"PMULLW mm, mm/m64","PMULLW mm/m64, mm","pmullw mm/m64, mm","NP 0F D5 /r","V","V","MMX","","","","","Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1."
"PMULLW xmm1, xmm2/m128","PMULLW xmm2/m128, xmm1","pmullw xmm2/m128, xmm1","66 0F D5 /r","V","V","SSE2","","","","","Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1."
"PMULUDQ mm1, mm2/m64","PMULULQ mm2/m64, mm1","pmuludq mm2/m64, mm1","NP 0F F4 /r","V","V","SSE2","","","","","Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1."
"PMULUDQ xmm1, xmm2/m128","PMULULQ xmm2/m128, xmm1","pmuludq xmm2/m128, xmm1","66 0F F4 /r","V","V","SSE2","","","","","Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1."
"POP DS","POPW/POPL/POPQ DS","popw/popl/popq DS","1F","V","I","","","w","Y","","Pop top of stack into DS; increment stack pointer."
"POP ES","POPW/POPL/POPQ ES","popw/popl/popq ES","07","V","I","","","w","Y","","Pop top of stack into ES; increment stack pointer."
"POP FS","POPW/POPL/POPQ FS","popw/popl/popq FS","0F A1","N.E.","V","","operand32,operand64","w","Y","","Pop top of stack into FS; increment stack pointer by 64 bits."
"POP FS","POPW/POPL/POPQ FS","popw/popl/popq FS","0F A1","V","N.E.","","operand32","w","Y","","Pop top of stack into FS; increment stack pointer by 32 bits."
"POP FS","POPW/POPL/POPQ FS","popw/popl/popq FS","0F A1","V","V","","operand16","w","Y","","Pop top of stack into FS; increment stack pointer by 16 bits."
"POP GS","POPW/POPL/POPQ GS","popw/popl/popq GS","0F A9","V","V","","operand16","w","Y","","Pop top of stack into GS; increment stack pointer by 16 bits."
"POP GS","POPW/POPL/POPQ GS","popw/popl/popq GS","0F A9","V","N.E.","","operand32","w","Y","","Pop top of stack into GS; increment stack pointer by 32 bits."
"POP GS","POPW/POPL/POPQ GS","popw/popl/popq GS","0F A9","N.E.","V","","operand32,operand64","w","Y","","Pop top of stack into GS; increment stack pointer by 64 bits."
"POP SS","POPW/POPL/POPQ SS","popw/popl/popq SS","17","V","I","","","w","Y","","Pop top of stack into SS; increment stack pointer."
"POP r/m16","POPW r/m16","popw r/m16","8F /0","V","V","","operand16","","Y","16","Pop top of stack into m16; increment stack pointer."
"POP r/m32","POPL r/m32","popl r/m32","8F /0","V","N.E.","","operand32","","Y","32","Pop top of stack into m32; increment stack pointer."
"POP r/m64","POPQ r/m64","popq r/m64","8F /0","N.E.","V","","operand32,operand64","","Y","64","Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size."
"POP r16","POPW r16","popw r16","58+rw","V","V","","operand16","","Y","16","Pop top of stack into r16; increment stack pointer."
"POP r32","POPL r32","popl r32","58+rd","V","N.E.","","operand32","","Y","32","Pop top of stack into r32; increment stack pointer."
"POP r64","POPQ r64","popq r64","58+rd","N.E.","V","","","","Y","64","Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size."
"POPA","POPAW","popaw","61","V","I","","operand16","","","","Pop DI, SI, BP, BX, DX, CX, and AX."
"POPAD","POPAL","popal","61","V","I","","operand32","","","","Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX."
"POPCNT r16, r/m16","POPCNTW r/m16, r16","popcntw r/m16, r16","F3 0F B8 /r","V","V","","operand16","","Y","16","POPCNT on r/m16"
"POPCNT r32, r/m32","POPCNTL r/m32, r32","popcntl r/m32, r32","F3 0F B8 /r","V","V","","operand32","","Y","32","POPCNT on r/m32"
"POPCNT r64, r/m64","POPCNTQ r/m64, r64","popcntq r/m64, r64","F3 REX.W 0F B8 /r","N.E.","V","","","","Y","64","POPCNT on r/m64"
"POPF","POPFW","popfw","9D","V","V","","operand16","","","","Pop top of stack into lower 16 bits of EFLAGS."
"POPFD","POPFL","popfl","9D","V","N.E.","","operand32","","","","Pop top of stack into EFLAGS."
"POPFQ","POPFQ","popfq","9D","N.E.","V","","operand32,operand64","","","","Pop top of stack and zero-extend into RFLAGS."
"POR mm, mm/m64","POR mm/m64, mm","por mm/m64, mm","NP 0F EB /r","V","V","MMX","","","","","Bitwise OR of mm/m64 and mm."
"POR xmm1, xmm2/m128","POR xmm2/m128, xmm1","por xmm2/m128, xmm1","66 0F EB /r","V","V","SSE2","","","","","Bitwise OR of xmm2/m128 and xmm1."
"PREFETCHNTA m8","PREFETCHNTA m8","prefetchnta m8","0F 18 /0","V","V","","","","","","Move data from m8 closer to the processor using NTA hint."
"PREFETCHT0 m8","PREFETCHT0 m8","prefetcht0 m8","0F 18 /1","V","V","","","","","","Move data from m8 closer to the processor using T0 hint."
"PREFETCHT1 m8","PREFETCHT1 m8","prefetcht1 m8","0F 18 /2","V","V","","","","","","Move data from m8 closer to the processor using T1 hint."
"PREFETCHT2 m8","PREFETCHT2 m8","prefetcht2 m8","0F 18 /3","V","V","","","","","","Move data from m8 closer to the processor using T2 hint."
"PREFETCHW m8","PREFETCHW m8","prefetchw m8","0F 0D /1","V","V","PREFETCHW","","","","","Move data from m8 closer to the processor in anticipation of a write."
"PSHUFB mm1, mm2/m64","PSHUFB mm2/m64, mm1","pshufb mm2/m64, mm1","NP 0F 38 00 /r","V","V","SSSE3","","","","","Shuffle bytes in mm1 according to contents of mm2/m64."
"PSHUFB xmm1, xmm2/m128","PSHUFB xmm2/m128, xmm1","pshufb xmm2/m128, xmm1","66 0F 38 00 /r","V","V","SSSE3","","","","","Shuffle bytes in xmm1 according to contents of xmm2/m128."
"PSHUFD xmm1, xmm2/m128, imm8","PSHUFD imm8, xmm2/m128, xmm1","pshufd imm8, xmm2/m128, xmm1","66 0F 70 /r ib","V","V","SSE2","","","","","Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"PSHUFHW xmm1, xmm2/m128, imm8","PSHUFHW imm8, xmm2/m128, xmm1","pshufhw imm8, xmm2/m128, xmm1","F3 0F 70 /r ib","V","V","SSE2","","","","","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"PSHUFLW xmm1, xmm2/m128, imm8","PSHUFLW imm8, xmm2/m128, xmm1","pshuflw imm8, xmm2/m128, xmm1","F2 0F 70 /r ib","V","V","SSE2","","","","","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"PSHUFW mm1, mm2/m64, imm8","PSHUFW imm8, mm2/m64, mm1","pshufw imm8, mm2/m64, mm1","NP 0F 70 /r ib","V","V","","","","","","Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1."
"PSIGNB mm1, mm2/m64","PSIGNB mm2/m64, mm1","psignb mm2/m64, mm1","NP 0F 38 08 /r","V","V","SSSE3","","","","","Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64."
"PSIGNB xmm1, xmm2/m128","PSIGNB xmm2/m128, xmm1","psignb xmm2/m128, xmm1","66 0F 38 08 /r","V","V","SSSE3","","","","","Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128."
"PSIGND mm1, mm2/m64","PSIGND mm2/m64, mm1","psignd mm2/m64, mm1","NP 0F 38 0A /r","V","V","SSSE3","","","","","Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128."
"PSIGND xmm1, xmm2/m128","PSIGND xmm2/m128, xmm1","psignd xmm2/m128, xmm1","66 0F 38 0A /r","V","V","SSSE3","","","","","Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128."
"PSIGNW mm1, mm2/m64","PSIGNW mm2/m64, mm1","psignw mm2/m64, mm1","NP 0F 38 09 /r","V","V","SSSE3","","","","","Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128."
"PSIGNW xmm1, xmm2/m128","PSIGNW xmm2/m128, xmm1","psignw xmm2/m128, xmm1","66 0F 38 09 /r","V","V","SSSE3","","","","","Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128."
"PSLLD mm, imm8","PSLLL imm8, mm","pslld imm8, mm","NP 0F 72 /6 ib","V","V","MMX","","","","","Shift doublewords in mm left by imm8 while shifting in 0s."
"PSLLD mm, mm/m64","PSLLL mm/m64, mm","pslld mm/m64, mm","NP 0F F2 /r","V","V","MMX","","","","","Shift doublewords in mm left by mm/m64 while shifting in 0s."
"PSLLD xmm1, imm8","PSLLL imm8, xmm1","pslld imm8, xmm1","66 0F 72 /6 ib","V","V","SSE2","","","","","Shift doublewords in xmm1 left by imm8 while shifting in 0s."
"PSLLD xmm1, xmm2/m128","PSLLL xmm2/m128, xmm1","pslld xmm2/m128, xmm1","66 0F F2 /r","V","V","SSE2","","","","","Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s."
"PSLLDQ xmm1, imm8","PSLLO imm8, xmm1","pslldq imm8, xmm1","66 0F 73 /7 ib","V","V","SSE2","","","","","Shift xmm1 left by imm8 bytes while shifting in 0s."
"PSLLQ mm, imm8","PSLLQ imm8, mm","psllq imm8, mm","NP 0F 73 /6 ib","V","V","MMX","","","","","Shift quadword in mm left by imm8 while shifting in 0s."
"PSLLQ mm, mm/m64","PSLLQ mm/m64, mm","psllq mm/m64, mm","NP 0F F3 /r","V","V","MMX","","","","","Shift quadword in mm left by mm/m64 while shifting in 0s."
"PSLLQ xmm1, imm8","PSLLQ imm8, xmm1","psllq imm8, xmm1","66 0F 73 /6 ib","V","V","SSE2","","","","","Shift quadwords in xmm1 left by imm8 while shifting in 0s."
"PSLLQ xmm1, xmm2/m128","PSLLQ xmm2/m128, xmm1","psllq xmm2/m128, xmm1","66 0F F3 /r","V","V","SSE2","","","","","Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s."
"PSLLW mm, mm/m64","PSLLW mm/m64, mm","psllw mm/m64, mm","NP 0F F1 /r","V","V","MMX","","","","","Shift words in mm left mm/m64 while shifting in 0s."
"PSLLW mm1, imm8","PSLLW imm8, mm1","psllw imm8, mm1","NP 0F 71 /6 ib","V","V","MMX","","","","","Shift words in mm left by imm8 while shifting in 0s."
"PSLLW xmm1, imm8","PSLLW imm8, xmm1","psllw imm8, xmm1","66 0F 71 /6 ib","V","V","SSE2","","","","","Shift words in xmm1 left by imm8 while shifting in 0s."
"PSLLW xmm1, xmm2/m128","PSLLW xmm2/m128, xmm1","psllw xmm2/m128, xmm1","66 0F F1 /r","V","V","SSE2","","","","","Shift words in xmm1 left by xmm2/m128 while shifting in 0s."
"PSRAD mm, imm8","PSRAL imm8, mm","psrad imm8, mm","NP 0F 72 /4 ib","V","V","MMX","","","","","Shift doublewords in mm right by imm8 while shifting in sign bits."
"PSRAD mm, mm/m64","PSRAL mm/m64, mm","psrad mm/m64, mm","NP 0F E2 /r","V","V","MMX","","","","","Shift doublewords in mm right by mm/m64 while shifting in sign bits."
"PSRAD xmm1, imm8","PSRAL imm8, xmm1","psrad imm8, xmm1","66 0F 72 /4 ib","V","V","SSE2","","","","","Shift doublewords in xmm1 right by imm8 while shifting in sign bits."
"PSRAD xmm1, xmm2/m128","PSRAL xmm2/m128, xmm1","psrad xmm2/m128, xmm1","66 0F E2 /r","V","V","SSE2","","","","","Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits."
"PSRAW mm, imm8","PSRAW imm8, mm","psraw imm8, mm","NP 0F 71 /4 ib","V","V","MMX","","","","","Shift words in mm right by imm8 while shifting in sign bits"
"PSRAW mm, mm/m64","PSRAW mm/m64, mm","psraw mm/m64, mm","NP 0F E1 /r","V","V","MMX","","","","","Shift words in mm right by mm/m64 while shifting in sign bits."
"PSRAW xmm1, imm8","PSRAW imm8, xmm1","psraw imm8, xmm1","66 0F 71 /4 ib","V","V","SSE2","","","","","Shift words in xmm1 right by imm8 while shifting in sign bits"
"PSRAW xmm1, xmm2/m128","PSRAW xmm2/m128, xmm1","psraw xmm2/m128, xmm1","66 0F E1 /r","V","V","SSE2","","","","","Shift words in xmm1 right by xmm2/m128 while shifting in sign bits."
"PSRLD mm, imm8","PSRLL imm8, mm","psrld imm8, mm","NP 0F 72 /2 ib","V","V","MMX","","","","","Shift doublewords in mm right by imm8 while shifting in 0s."
"PSRLD mm, mm/m64","PSRLL mm/m64, mm","psrld mm/m64, mm","NP 0F D2 /r","V","V","MMX","","","","","Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLD xmm1, imm8","PSRLL imm8, xmm1","psrld imm8, xmm1","66 0F 72 /2 ib","V","V","SSE2","","","","","Shift doublewords in xmm1 right by imm8 while shifting in 0s."
"PSRLD xmm1, xmm2/m128","PSRLL xmm2/m128, xmm1","psrld xmm2/m128, xmm1","66 0F D2 /r","V","V","SSE2","","","","","Shift doublewords in xmm1 right by amount specified in xmm2 /m128 while shifting in 0s."
"PSRLDQ xmm1, imm8","PSRLO imm8, xmm1","psrldq imm8, xmm1","66 0F 73 /3 ib","V","V","SSE2","","","","","Shift xmm1 right by imm8 while shifting in 0s."
"PSRLQ mm, imm8","PSRLQ imm8, mm","psrlq imm8, mm","NP 0F 73 /2 ib","V","V","MMX","","","","","Shift mm right by imm8 while shifting in 0s."
"PSRLQ mm, mm/m64","PSRLQ mm/m64, mm","psrlq mm/m64, mm","NP 0F D3 /r","V","V","MMX","","","","","Shift mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLQ xmm1, imm8","PSRLQ imm8, xmm1","psrlq imm8, xmm1","66 0F 73 /2 ib","V","V","SSE2","","","","","Shift quadwords in xmm1 right by imm8 while shifting in 0s."
"PSRLQ xmm1, xmm2/m128","PSRLQ xmm2/m128, xmm1","psrlq xmm2/m128, xmm1","66 0F D3 /r","V","V","SSE2","","","","","Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s."
"PSRLW mm, imm8","PSRLW imm8, mm","psrlw imm8, mm","NP 0F 71 /2 ib","V","V","MMX","","","","","Shift words in mm right by imm8 while shifting in 0s."
"PSRLW mm, mm/m64","PSRLW mm/m64, mm","psrlw mm/m64, mm","NP 0F D1 /r","V","V","MMX","","","","","Shift words in mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLW xmm1, imm8","PSRLW imm8, xmm1","psrlw imm8, xmm1","66 0F 71 /2 ib","V","V","SSE2","","","","","Shift words in xmm1 right by imm8 while shifting in 0s."
"PSRLW xmm1, xmm2/m128","PSRLW xmm2/m128, xmm1","psrlw xmm2/m128, xmm1","66 0F D1 /r","V","V","SSE2","","","","","Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s."
"PSUBB mm, mm/m64","PSUBB mm/m64, mm","psubb mm/m64, mm","NP 0F F8 /r","V","V","MMX","","","","","Subtract packed byte integers in mm/m64 from packed byte integers in mm."
"PSUBB xmm1, xmm2/m128","PSUBB xmm2/m128, xmm1","psubb xmm2/m128, xmm1","66 0F F8 /r","V","V","SSE2","","","","","Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1."
"PSUBD mm, mm/m64","PSUBL mm/m64, mm","psubd mm/m64, mm","NP 0F FA /r","V","V","MMX","","","","","Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm."
"PSUBD xmm1, xmm2/m128","PSUBL xmm2/m128, xmm1","psubd xmm2/m128, xmm1","66 0F FA /r","V","V","SSE2","","","","","Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1."
"PSUBQ mm1, mm2/m64","PSUBQ mm2/m64, mm1","psubq mm2/m64, mm1","NP 0F FB /r","V","V","SSE2","","","","","Subtract quadword integer in mm1 from mm2 /m64."
"PSUBQ xmm1, xmm2/m128","PSUBQ xmm2/m128, xmm1","psubq xmm2/m128, xmm1","66 0F FB /r","V","V","SSE2","","","","","Subtract packed quadword integers in xmm1 from xmm2 /m128."
"PSUBSB mm, mm/m64","PSUBSB mm/m64, mm","psubsb mm/m64, mm","NP 0F E8 /r","V","V","MMX","","","","","Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results."
"PSUBSB xmm1, xmm2/m128","PSUBSB xmm2/m128, xmm1","psubsb xmm2/m128, xmm1","66 0F E8 /r","V","V","SSE2","","","","","Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results."
"PSUBSW mm, mm/m64","PSUBSW mm/m64, mm","psubsw mm/m64, mm","NP 0F E9 /r","V","V","MMX","","","","","Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results."
"PSUBSW xmm1, xmm2/m128","PSUBSW xmm2/m128, xmm1","psubsw xmm2/m128, xmm1","66 0F E9 /r","V","V","SSE2","","","","","Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results."
"PSUBUSB mm, mm/m64","PSUBUSB mm/m64, mm","psubusb mm/m64, mm","NP 0F D8 /r","V","V","MMX","","","","","Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result."
"PSUBUSB xmm1, xmm2/m128","PSUBUSB xmm2/m128, xmm1","psubusb xmm2/m128, xmm1","66 0F D8 /r","V","V","SSE2","","","","","Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result."
"PSUBUSW mm, mm/m64","PSUBUSW mm/m64, mm","psubusw mm/m64, mm","NP 0F D9 /r","V","V","MMX","","","","","Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result."
"PSUBUSW xmm1, xmm2/m128","PSUBUSW xmm2/m128, xmm1","psubusw xmm2/m128, xmm1","66 0F D9 /r","V","V","SSE2","","","","","Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result."
"PSUBW mm, mm/m64","PSUBW mm/m64, mm","psubw mm/m64, mm","NP 0F F9 /r","V","V","MMX","","","","","Subtract packed word integers in mm/m64 from packed word integers in mm."
"PSUBW xmm1, xmm2/m128","PSUBW xmm2/m128, xmm1","psubw xmm2/m128, xmm1","66 0F F9 /r","V","V","SSE2","","","","","Subtract packed word integers in xmm2/m128 from packed word integers in xmm1."
"PTEST xmm1, xmm2/m128","PTEST xmm2/m128, xmm1","ptest xmm2/m128, xmm1","66 0F 38 17 /r","V","V","SSE4_1","","","","","Set ZF if xmm2/m128 AND xmm1 result is all 0s. Set CF if xmm2/m128 AND NOT xmm1 result is all 0s."
"PUNPCKHBW mm, mm/m64","PUNPCKHBW mm/m64, mm","punpckhbw mm/m64, mm","NP 0F 68 /r","V","V","MMX","","","","","Unpack and interleave high-order bytes from mm and mm/m64 into mm."
"PUNPCKHBW xmm1, xmm2/m128","PUNPCKHBW xmm2/m128, xmm1","punpckhbw xmm2/m128, xmm1","66 0F 68 /r","V","V","SSE2","","","","","Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHDQ mm, mm/m64","PUNPCKHLQ mm/m64, mm","punpckhdq mm/m64, mm","NP 0F 6A /r","V","V","MMX","","","","","Unpack and interleave high-order doublewords from mm and mm/m64 into mm."
"PUNPCKHDQ xmm1, xmm2/m128","PUNPCKHLQ xmm2/m128, xmm1","punpckhdq xmm2/m128, xmm1","66 0F 6A /r","V","V","SSE2","","","","","Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHQDQ xmm1, xmm2/m128","PUNPCKHQDQ xmm2/m128, xmm1","punpckhqdq xmm2/m128, xmm1","66 0F 6D /r","V","V","SSE2","","","","","Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHWD mm, mm/m64","PUNPCKHWL mm/m64, mm","punpckhwd mm/m64, mm","NP 0F 69 /r","V","V","MMX","","","","","Unpack and interleave high-order words from mm and mm/m64 into mm."
"PUNPCKHWD xmm1, xmm2/m128","PUNPCKHWL xmm2/m128, xmm1","punpckhwd xmm2/m128, xmm1","66 0F 69 /r","V","V","SSE2","","","","","Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLBW mm, mm/m32","PUNPCKLBW mm/m32, mm","punpcklbw mm/m32, mm","NP 0F 60 /r","V","V","MMX","","","","","Interleave low-order bytes from mm and mm/m32 into mm."
"PUNPCKLBW xmm1, xmm2/m128","PUNPCKLBW xmm2/m128, xmm1","punpcklbw xmm2/m128, xmm1","66 0F 60 /r","V","V","SSE2","","","","","Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLDQ mm, mm/m32","PUNPCKLLQ mm/m32, mm","punpckldq mm/m32, mm","NP 0F 62 /r","V","V","MMX","","","","","Interleave low-order doublewords from mm and mm/m32 into mm."
"PUNPCKLDQ xmm1, xmm2/m128","PUNPCKLLQ xmm2/m128, xmm1","punpckldq xmm2/m128, xmm1","66 0F 62 /r","V","V","SSE2","","","","","Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLQDQ xmm1, xmm2/m128","PUNPCKLQDQ xmm2/m128, xmm1","punpcklqdq xmm2/m128, xmm1","66 0F 6C /r","V","V","SSE2","","","","","Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register."
"PUNPCKLWD mm, mm/m32","PUNPCKLWL mm/m32, mm","punpcklwd mm/m32, mm","NP 0F 61 /r","V","V","MMX","","","","","Interleave low-order words from mm and mm/m32 into mm."
"PUNPCKLWD xmm1, xmm2/m128","PUNPCKLWL xmm2/m128, xmm1","punpcklwd xmm2/m128, xmm1","66 0F 61 /r","V","V","SSE2","","","","","Interleave low-order words from xmm1 and xmm2/m128 into xmm1."
"PUSHA","PUSHAW","pushaw","60","V","I","","operand16","","","","Push AX, CX, DX, BX, original SP, BP, SI, and DI."
"PUSHAD","PUSHAL","pushal","60","V","I","","operand32","","","","Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI."
"PUSHF","PUSHFW","pushfw","9C","V","V","","operand16","","","","Push lower 16 bits of EFLAGS."
"PUSHFD","PUSHFL","pushfl","9C","V","N.E.","","operand32","","","","Push EFLAGS."
"PUSHFQ","PUSHFQ","pushfq","9C","N.E.","V","","operand32,operand64","","","","Push RFLAGS."
"PXOR mm, mm/m64","PXOR mm/m64, mm","pxor mm/m64, mm","NP 0F EF /r","V","V","MMX","","","","","Bitwise XOR of mm/m64 and mm."
"PXOR xmm1, xmm2/m128","PXOR xmm2/m128, xmm1","pxor xmm2/m128, xmm1","66 0F EF /r","V","V","SSE2","","","","","Bitwise XOR of xmm2/m128 and xmm1."
"RCPPS xmm1, xmm2/m128","RCPPS xmm2/m128, xmm1","rcpps xmm2/m128, xmm1","NP 0F 53 /r","V","V","SSE","","","","","Computes the approximate reciprocals of the packed single precision floating-point values in xmm2/m128 and stores the results in xmm1."
"RCPSS xmm1, xmm2/m32","RCPSS xmm2/m32, xmm1","rcpss xmm2/m32, xmm1","F3 0F 53 /r","V","V","SSE","","","","","Computes the approximate reciprocal of the scalar single precision floating-point value in xmm2/m32 and stores the result in xmm1."
"RDFSBASE r32","RDFSBASE r32","rdfsbase r32","F3 0F AE /0","I","V","FSGSBASE","operand16,operand32","","Y","32","Load the 32-bit destination register with the FS base address."
"RDFSBASE r64","RDFSBASE r64","rdfsbase r64","F3 REX.W 0F AE /0","I","V","FSGSBASE","","","Y","64","Load the 64-bit destination register with the FS base address."
"RDGSBASE r32","RDGSBASE r32","rdgsbase r32","F3 0F AE /1","I","V","FSGSBASE","operand16,operand32","","Y","32","Load the 32-bit destination register with the GS base address."
"RDGSBASE r64","RDGSBASE r64","rdgsbase r64","F3 REX.W 0F AE /1","I","V","FSGSBASE","","","Y","64","Load the 64-bit destination register with the GS base address."
"RDPID r32","RDPID r32","rdpid r32","F3 0F C7 /7","V","N.E.","RDPID","","","","","Read IA32_TSC_AUX into r32."
"RDPID r64","RDPID r64","rdpid r64","F3 0F C7 /7","N.E.","V","RDPID","","","","","Read IA32_TSC_AUX into r64."
"RDPKRU","RDPKRU","rdpkru","NP 0F 01 EE","V","V","OSPKE","","","","","Reads PKRU into EAX."
"RDPMC","RDPMC","rdpmc","0F 33","V","V","","","","","","Read performance-monitoring counter specified by ECX into EDX:EAX."
"RDRAND r16","RDRAND r16","rdrand r16","NFx 0F C7 /6","V","V","RDRAND","operand16","","Y","16","Read a 16-bit random number and store in the destination register."
"RDRAND r32","RDRAND r32","rdrand r32","NFx 0F C7 /6","V","V","RDRAND","operand32","","Y","32","Read a 32-bit random number and store in the destination register."
"RDRAND r64","RDRAND r64","rdrand r64","NFx REX.W 0F C7 /6","I","V","RDRAND","","","Y","64","Read a 64-bit random number and store in the destination register."
"RDSEED r16","RDSEED r16","rdseed r16","NFx 0F C7 /7","V","V","RDSEED","operand16","","Y","16","Read a 16-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDSEED r32","RDSEED r32","rdseed r32","NFx 0F C7 /7","V","V","RDSEED","operand32","","Y","32","Read a 32-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDSEED r64","RDSEED r64","rdseed r64","NFx REX.W 0F C7 /7","I","V","RDSEED","","","Y","64","Read a 64-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDSSPD r32","RDSSPD r32","rdsspd r32","F3 0F 1E /1 (mod=11)","V","V","CET_SS","operand16,operand32","","","","Copy low 32 bits of shadow stack pointer (SSP) to r32."
"RDSSPQ r64","RDSSPQ r64","rdsspq r64","F3 REX.W 0F 1E /1 (mod=11)","N.E.","V","CET_SS","","","","","Copies shadow stack pointer (SSP) to r64."
"RDTSC","RDTSC","rdtsc","0F 31","V","V","","","","","","Read time-stamp counter into EDX:EAX."
"RDTSCP","RDTSCP","rdtscp","0F 01 F9","V","V","","","","","","Read 64-bit time-stamp counter and IA32_TSC_AUX value into EDX:EAX and ECX."
"RET","RETW/RETL/RETQ","retw/retl/retq","C3","V","V","","","","","","Near return to calling procedure."
"RET imm16u","RETW/RETL/RETQ imm16u","retw/retl/retq imm16u","C2 iw","V","V","","","","","","Near return to calling procedure and pop imm16 bytes from stack."
"RET_FAR","RETFW/RETFL/RETFQ","lretw/lretl/lretl","CB","V","V","","","","","","Far return to calling procedure."
"RET_FAR imm16u","RETFW/RETFL/RETFQ imm16u","lretw/lretl/lretl imm16u","CA iw","V","V","","","","","","Far return to calling procedure and pop imm16 bytes from stack."
"RORX r32, r/m32, imm8","RORXL imm8, r/m32, r32","rorxl imm8, r/m32, r32","VEX.LZ.F2.0F3A.W0 F0 /r ib","V","V","BMI2","","","Y","32","Rotate 32-bit r/m32 right imm8 times without affecting arithmetic flags."
"RORX r64, r/m64, imm8","RORXQ imm8, r/m64, r64","rorxq imm8, r/m64, r64","VEX.LZ.F2.0F3A.W1 F0 /r ib","N.E.","V","BMI2","","","Y","64","Rotate 64-bit r/m64 right imm8 times without affecting arithmetic flags."
"ROUNDPD xmm1, xmm2/m128, imm8","ROUNDPD imm8, xmm2/m128, xmm1","roundpd imm8, xmm2/m128, xmm1","66 0F 3A 09 /r ib","V","V","SSE4_1","","","","","Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"ROUNDPS xmm1, xmm2/m128, imm8","ROUNDPS imm8, xmm2/m128, xmm1","roundps imm8, xmm2/m128, xmm1","66 0F 3A 08 /r ib","V","V","SSE4_1","","","","","Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"ROUNDSD xmm1, xmm2/m64, imm8","ROUNDSD imm8, xmm2/m64, xmm1","roundsd imm8, xmm2/m64, xmm1","66 0F 3A 0B /r ib","V","V","SSE4_1","","","","","Round the low packed double precision floating-point value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8."
"ROUNDSS xmm1, xmm2/m32, imm8","ROUNDSS imm8, xmm2/m32, xmm1","roundss imm8, xmm2/m32, xmm1","66 0F 3A 0A /r ib","V","V","SSE4_1","","","","","Round the low packed single precision floating-point value in xmm2/m32 and place the result in xmm1. The rounding mode is determined by imm8."
"RSM","RSM","rsm","0F AA","V","V","","","","","","Resume operation of interrupted program."
"RSQRTPS xmm1, xmm2/m128","RSQRTPS xmm2/m128, xmm1","rsqrtps xmm2/m128, xmm1","NP 0F 52 /r","V","V","SSE","","","","","Computes the approximate reciprocals of the square roots of the packed single precision floating-point values in xmm2/m128 and stores the results in xmm1."
"RSQRTSS xmm1, xmm2/m32","RSQRTSS xmm2/m32, xmm1","rsqrtss xmm2/m32, xmm1","F3 0F 52 /r","V","V","SSE","","","","","Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm2/m32 and stores the results in xmm1."
"RSTORSSP m64","RSTORSSP m64","rstorssp m64","F3 0F 01 /5 (mod!=11 /5 memory only)","V","V","CET_SS","","","","","Restore SSP."
"SARX r32a, r/m32, r32b","SARXL r32b, r/m32, r32a","sarxl r32b, r/m32, r32a","VEX.LZ.F3.0F38.W0 F7 /r","V","V","BMI2","","","Y","32","Shift r/m32 arithmetically right with count specified in r32b."
"SARX r64a, r/m64, r64b","SARXQ r64b, r/m64, r64a","sarxq r64b, r/m64, r64a","VEX.LZ.F3.0F38.W1 F7 /r","N.E.","V","BMI2","","","Y","64","Shift r/m64 arithmetically right with count specified in r64b."
"SAVEPREVSSP","SAVEPREVSSP","saveprevssp","F3 0F 01 EA (mod!=11 /5 RM=010)","V","V","CET_SS","","","","","Save a restore-shadow-stack token on previous shadow stack."
"SBB AL, imm8","SBBB imm8, AL","sbbb imm8, AL","1C ib","V","V","","","","Y","8","Subtract with borrow imm8 from AL."
"SBB AX, imm16","SBBW imm16, AX","sbbw imm16, AX","1D iw","V","V","","operand16","","Y","16","Subtract with borrow imm16 from AX."
"SBB EAX, imm32","SBBL imm32, EAX","sbbl imm32, EAX","1D id","V","V","","operand32","","Y","32","Subtract with borrow imm32 from EAX."
"SBB RAX, imm32","SBBQ imm32, RAX","sbbq imm32, RAX","REX.W 1D id","N.E.","V","","","","Y","64","Subtract with borrow sign-extended imm.32 to 64-bits from RAX."
"SBB r/m16, imm16","SBBW imm16, r/m16","sbbw imm16, r/m16","81 /3 iw","V","V","","operand16","","Y","16","Subtract with borrow imm16 from r/m16."
"SBB r/m16, imm8","SBBW imm8, r/m16","sbbw imm8, r/m16","83 /3 ib","V","V","","operand16","","Y","16","Subtract with borrow sign-extended imm8 from r/m16."
"SBB r/m16, r16","SBBW r16, r/m16","sbbw r16, r/m16","19 /r","V","V","","operand16","","Y","16","Subtract with borrow r16 from r/m16."
"SBB r/m32, imm32","SBBL imm32, r/m32","sbbl imm32, r/m32","81 /3 id","V","V","","operand32","","Y","32","Subtract with borrow imm32 from r/m32."
"SBB r/m32, imm8","SBBL imm8, r/m32","sbbl imm8, r/m32","83 /3 ib","V","V","","operand32","","Y","32","Subtract with borrow sign-extended imm8 from r/m32."
"SBB r/m32, r32","SBBL r32, r/m32","sbbl r32, r/m32","19 /r","V","V","","operand32","","Y","32","Subtract with borrow r32 from r/m32."
"SBB r/m64, imm32","SBBQ imm32, r/m64","sbbq imm32, r/m64","REX.W 81 /3 id","N.E.","V","","","","Y","64","Subtract with borrow sign-extended imm32 to 64-bits from r/m64."
"SBB r/m64, imm8","SBBQ imm8, r/m64","sbbq imm8, r/m64","REX.W 83 /3 ib","N.E.","V","","","","Y","64","Subtract with borrow sign-extended imm8 from r/m64."
"SBB r/m64, r64","SBBQ r64, r/m64","sbbq r64, r/m64","REX.W 19 /r","N.E.","V","","","","Y","64","Subtract with borrow r64 from r/m64."
"SBB r/m8, imm8","SBBB imm8, r/m8","sbbb imm8, r/m8","80 /3 ib","V","V","","","","Y","8","Subtract with borrow imm8 from r/m8."
"SBB r/m8, imm8","SBBB imm8, r/m8","sbbb imm8, r/m8","REX 80 /3 ib","N.E.","V","","pseudo64","","Y","8","Subtract with borrow imm8 from r/m8."
"SBB r/m8, r8","SBBB r8, r/m8","sbbb r8, r/m8","18 /r","V","V","","","","Y","8","Subtract with borrow r8 from r/m8."
"SBB r/m8, r8","SBBB r8, r/m8","sbbb r8, r/m8","REX 18 /r","N.E.","V","","pseudo64","","Y","8","Subtract with borrow r8 from r/m8."
"SBB r16, r/m16","SBBW r/m16, r16","sbbw r/m16, r16","1B /r","V","V","","operand16","","Y","16","Subtract with borrow r/m16 from r16."
"SBB r32, r/m32","SBBL r/m32, r32","sbbl r/m32, r32","1B /r","V","V","","operand32","","Y","32","Subtract with borrow r/m32 from r32."
"SBB r64, r/m64","SBBQ r/m64, r64","sbbq r/m64, r64","REX.W 1B /r","N.E.","V","","","","Y","64","Subtract with borrow r/m64 from r64."
"SBB r8, r/m8","SBBB r/m8, r8","sbbb r/m8, r8","1A /r","V","V","","","","Y","8","Subtract with borrow r/m8 from r8."
"SBB r8, r/m8","SBBB r/m8, r8","sbbb r/m8, r8","REX 1A /r","N.E.","V","","pseudo64","","Y","8","Subtract with borrow r/m8 from r8."
"SCASB","SCASB","scasb","AE","V","V","","","","","","Compare AL with byte at ES:(E)DI or RDI then set status flags."
"SCASD","SCASL","scasl","AF","V","V","","operand32","","","","Compare EAX with doubleword at ES:(E)DI or RDI then set status flags."
"SCASQ","SCASQ","scasq","REX.W AF","N.E.","V","","","","","","Compare RAX with quadword at RDI or EDI then set status flags."
"SCASW","SCASW","scasw","AF","V","V","","operand16","","","","Compare AX with word at ES:(E)DI or RDI then set status flags."
"SETA r/m8","SETHI r/m8","seta r/m8","0F 97","V","V","","","","","","Set byte if above (CF=0 and ZF=0)."
"SETA r/m8","SETHI r/m8","seta r/m8","REX 0F 97","N.E.","V","","pseudo64","","","","Set byte if above (CF=0 and ZF=0)."
"SETAE r/m8","SETCC r/m8","setae r/m8","0F 93","V","V","","","","","","Set byte if above or equal (CF=0)."
"SETAE r/m8","SETCC r/m8","setae r/m8","REX 0F 93","N.E.","V","","pseudo64","","","","Set byte if above or equal (CF=0)."
"SETB r/m8","SETCS r/m8","setb r/m8","0F 92","V","V","","","","","","Set byte if below (CF=1)."
"SETB r/m8","SETCS r/m8","setb r/m8","REX 0F 92","N.E.","V","","pseudo64","","","","Set byte if below (CF=1)."
"SETBE r/m8","SETLS r/m8","setbe r/m8","0F 96","V","V","","","","","","Set byte if below or equal (CF=1 or ZF=1)."
"SETBE r/m8","SETLS r/m8","setbe r/m8","REX 0F 96","N.E.","V","","pseudo64","","","","Set byte if below or equal (CF=1 or ZF=1)."
"SETC r/m8","SETCS r/m8","setc r/m8","0F 92","V","V","","pseudo","","","","Set byte if carry (CF=1)."
"SETC r/m8","SETCS r/m8","setc r/m8","REX 0F 92","N.E.","V","","pseudo","","","","Set byte if carry (CF=1)."
"SETE r/m8","SETEQ r/m8","sete r/m8","0F 94","V","V","","","","","","Set byte if equal (ZF=1)."
"SETE r/m8","SETEQ r/m8","sete r/m8","REX 0F 94","N.E.","V","","pseudo64","","","","Set byte if equal (ZF=1)."
"SETG r/m8","SETGT r/m8","setg r/m8","0F 9F","V","V","","","","","","Set byte if greater (ZF=0 and SF=OF)."
"SETG r/m8","SETGT r/m8","setg r/m8","REX 0F 9F","N.E.","V","","pseudo64","","","","Set byte if greater (ZF=0 and SF=OF)."
"SETGE r/m8","SETGE r/m8","setge r/m8","0F 9D","V","V","","","","","","Set byte if greater or equal (SF=OF)."
"SETGE r/m8","SETGE r/m8","setge r/m8","REX 0F 9D","N.E.","V","","pseudo64","","","","Set byte if greater or equal (SF=OF)."
"SETL r/m8","SETLT r/m8","setl r/m8","0F 9C","V","V","","","","","","Set byte if less (SF≠ OF)."
"SETL r/m8","SETLT r/m8","setl r/m8","REX 0F 9C","N.E.","V","","pseudo64","","","","Set byte if less (SF≠ OF)."
"SETLE r/m8","SETLE r/m8","setle r/m8","0F 9E","V","V","","","","","","Set byte if less or equal (ZF=1 or SF≠ OF)."
"SETLE r/m8","SETLE r/m8","setle r/m8","REX 0F 9E","N.E.","V","","pseudo64","","","","Set byte if less or equal (ZF=1 or SF≠ OF)."
"SETNA r/m8","SETLS r/m8","setna r/m8","0F 96","V","V","","pseudo","","","","Set byte if not above (CF=1 or ZF=1)."
"SETNA r/m8","SETLS r/m8","setna r/m8","REX 0F 96","N.E.","V","","pseudo","","","","Set byte if not above (CF=1 or ZF=1)."
"SETNAE r/m8","SETCS r/m8","setnae r/m8","0F 92","V","V","","pseudo","","","","Set byte if not above or equal (CF=1)."
"SETNAE r/m8","SETCS r/m8","setnae r/m8","REX 0F 92","N.E.","V","","pseudo","","","","Set byte if not above or equal (CF=1)."
"SETNB r/m8","SETCC r/m8","setnb r/m8","0F 93","V","V","","pseudo","","","","Set byte if not below (CF=0)."
"SETNB r/m8","SETCC r/m8","setnb r/m8","REX 0F 93","N.E.","V","","pseudo","","","","Set byte if not below (CF=0)."
"SETNBE r/m8","SETHI r/m8","setnbe r/m8","0F 97","V","V","","pseudo","","","","Set byte if not below or equal (CF=0 and ZF=0)."
"SETNBE r/m8","SETHI r/m8","setnbe r/m8","REX 0F 97","N.E.","V","","pseudo","","","","Set byte if not below or equal (CF=0 and ZF=0)."
"SETNC r/m8","SETCC r/m8","setnc r/m8","0F 93","V","V","","pseudo","","","","Set byte if not carry (CF=0)."
"SETNC r/m8","SETCC r/m8","setnc r/m8","REX 0F 93","N.E.","V","","pseudo","","","","Set byte if not carry (CF=0)."
"SETNE r/m8","SETNE r/m8","setne r/m8","0F 95","V","V","","","","","","Set byte if not equal (ZF=0)."
"SETNE r/m8","SETNE r/m8","setne r/m8","REX 0F 95","N.E.","V","","pseudo64","","","","Set byte if not equal (ZF=0)."
"SETNG r/m8","SETLE r/m8","setng r/m8","0F 9E","V","V","","pseudo","","","","Set byte if not greater (ZF=1 or SF≠ OF)"
"SETNG r/m8","SETLE r/m8","setng r/m8","REX 0F 9E","N.E.","V","","pseudo","","","","Set byte if not greater (ZF=1 or SF≠ OF)."
"SETNGE r/m8","SETLT r/m8","setnge r/m8","0F 9C","V","V","","pseudo","","","","Set byte if not greater or equal (SF≠ OF)."
"SETNGE r/m8","SETLT r/m8","setnge r/m8","REX 0F 9C","N.E.","V","","pseudo","","","","Set byte if not greater or equal (SF≠ OF)."
"SETNL r/m8","SETGE r/m8","setnl r/m8","0F 9D","V","V","","pseudo","","","","Set byte if not less (SF=OF)."
"SETNL r/m8","SETGE r/m8","setnl r/m8","REX 0F 9D","N.E.","V","","pseudo","","","","Set byte if not less (SF=OF)."
"SETNLE r/m8","SETGT r/m8","setnle r/m8","0F 9F","V","V","","pseudo","","","","Set byte if not less or equal (ZF=0 and SF=OF)."
"SETNLE r/m8","SETGT r/m8","setnle r/m8","REX 0F 9F","N.E.","V","","pseudo","","","","Set byte if not less or equal (ZF=0 and SF=OF)."
"SETNO r/m8","SETOC r/m8","setno r/m8","0F 91","V","V","","","","","","Set byte if not overflow (OF=0)."
"SETNO r/m8","SETOC r/m8","setno r/m8","REX 0F 91","N.E.","V","","pseudo64","","","","Set byte if not overflow (OF=0)."
"SETNP r/m8","SETPC r/m8","setnp r/m8","0F 9B","V","V","","","","","","Set byte if not parity (PF=0)."
"SETNP r/m8","SETPC r/m8","setnp r/m8","REX 0F 9B","N.E.","V","","pseudo64","","","","Set byte if not parity (PF=0)."
"SETNS r/m8","SETPL r/m8","setns r/m8","0F 99","V","V","","","","","","Set byte if not sign (SF=0)."
"SETNS r/m8","SETPL r/m8","setns r/m8","REX 0F 99","N.E.","V","","pseudo64","","","","Set byte if not sign (SF=0)."
"SETNZ r/m8","SETNE r/m8","setnz r/m8","0F 95","V","V","","pseudo","","","","Set byte if not zero (ZF=0)."
"SETNZ r/m8","SETNE r/m8","setnz r/m8","REX 0F 95","N.E.","V","","pseudo","","","","Set byte if not zero (ZF=0)."
"SETO r/m8","SETOS r/m8","seto r/m8","0F 90","V","V","","","","","","Set byte if overflow (OF=1)"
"SETO r/m8","SETOS r/m8","seto r/m8","REX 0F 90","N.E.","V","","pseudo64","","","","Set byte if overflow (OF=1)."
"SETP r/m8","SETPS r/m8","setp r/m8","0F 9A","V","V","","","","","","Set byte if parity (PF=1)."
"SETP r/m8","SETPS r/m8","setp r/m8","REX 0F 9A","N.E.","V","","pseudo64","","","","Set byte if parity (PF=1)."
"SETPE r/m8","SETPS r/m8","setpe r/m8","0F 9A","V","V","","pseudo","","","","Set byte if parity even (PF=1)."
"SETPE r/m8","SETPS r/m8","setpe r/m8","REX 0F 9A","N.E.","V","","pseudo","","","","Set byte if parity even (PF=1)."
"SETPO r/m8","SETPC r/m8","setpo r/m8","0F 9B","V","V","","pseudo","","","","Set byte if parity odd (PF=0)."
"SETPO r/m8","SETPC r/m8","setpo r/m8","REX 0F 9B","N.E.","V","","pseudo","","","","Set byte if parity odd (PF=0)."
"SETS r/m8","SETMI r/m8","sets r/m8","0F 98","V","V","","","","","","Set byte if sign (SF=1)."
"SETS r/m8","SETMI r/m8","sets r/m8","REX 0F 98","N.E.","V","","pseudo64","","","","Set byte if sign (SF=1)."
"SETSSBSY","SETSSBSY","setssbsy","F3 0F 01 E8","V","V","CET_SS","","","","","Set busy flag in supervisor shadow stack token reference by IA32_PL0_SSP."
"SETZ r/m8","SETEQ r/m8","setz r/m8","0F 94","V","V","","pseudo","","","","Set byte if zero (ZF=1)."
"SETZ r/m8","SETEQ r/m8","setz r/m8","REX 0F 94","N.E.","V","","pseudo","","","","Set byte if zero (ZF=1)."
"SFENCE","SFENCE","sfence","NP 0F AE F8","V","V","","","","","","Serializes store operations."
"SHA1MSG1 xmm1, xmm2/m128","SHA1MSG1 xmm2/m128, xmm1","sha1msg1 xmm2/m128, xmm1","NP 0F 38 C9 /r","V","V","SHA","","","","","Performs an intermediate calculation for the next four SHA1 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA1MSG2 xmm1, xmm2/m128","SHA1MSG2 xmm2/m128, xmm1","sha1msg2 xmm2/m128, xmm1","NP 0F 38 CA /r","V","V","SHA","","","","","Performs the final calculation for the next four SHA1 message dwords using intermediate results from xmm1 and the previous message dwords from xmm2/m128, storing the result in xmm1."
"SHA1NEXTE xmm1, xmm2/m128","SHA1NEXTE xmm2/m128, xmm1","sha1nexte xmm2/m128, xmm1","NP 0F 38 C8 /r","V","V","SHA","","","","","Calculates SHA1 state variable E after four rounds of operation from the current SHA1 state variable A in xmm1. The calculated value of the SHA1 state variable E is added to the scheduled dwords in xmm2/m128, and stored with some of the scheduled dwords in xmm1."
"SHA1RNDS4 xmm1, xmm2/m128, imm8","SHA1RNDS4 imm8, xmm2/m128, xmm1","sha1rnds4 imm8, xmm2/m128, xmm1","NP 0F 3A CC /r ib","V","V","SHA","","","","","Performs four rounds of SHA1 operation operating on SHA1 state (A,B,C,D) from xmm1, with a pre-computed sum of the next 4 round message dwords and state variable E from xmm2/m128. The immediate byte controls logic functions and round constants."
"SHA256MSG1 xmm1, xmm2/m128","SHA256MSG1 xmm2/m128, xmm1","sha256msg1 xmm2/m128, xmm1","NP 0F 38 CC /r","V","V","SHA","","","","","Performs an intermediate calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA256MSG2 xmm1, xmm2/m128","SHA256MSG2 xmm2/m128, xmm1","sha256msg2 xmm2/m128, xmm1","NP 0F 38 CD /r","V","V","SHA","","","","","Performs the final calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA256RNDS2 xmm1, xmm2/m128, <XMM0>","SHA256RNDS2 <XMM0>, xmm2/m128, xmm1","sha256rnds2 <XMM0>, xmm2/m128, xmm1","NP 0F 38 CB /r","V","V","SHA","","","","","Perform 2 rounds of SHA256 operation using an initial SHA256 state (C,D,G,H) from xmm1, an initial SHA256 state (A,B,E,F) from xmm2/m128, and a pre-computed sum of the next 2 round mes- sage dwords and the corresponding round constants from the implicit operand XMM0, storing the updated SHA256 state (A,B,E,F) result in xmm1."
"SHLD r/m16, r16, CL","SHLW CL, r16, r/m16","shldw CL, r16, r/m16","0F A5 /r","V","V","","operand16","","Y","16","Shift r/m16 to left CL places while shifting bits from r16 in from the right."
"SHLD r/m16, r16, imm8","SHLW imm8, r16, r/m16","shldw imm8, r16, r/m16","0F A4 /r ib","V","V","","operand16","","Y","16","Shift r/m16 to left imm8 places while shifting bits from r16 in from the right."
"SHLD r/m32, r32, CL","SHLL CL, r32, r/m32","shldl CL, r32, r/m32","0F A5 /r","V","V","","operand32","","Y","32","Shift r/m32 to left CL places while shifting bits from r32 in from the right."
"SHLD r/m32, r32, imm8","SHLL imm8, r32, r/m32","shldl imm8, r32, r/m32","0F A4 /r ib","V","V","","operand32","","Y","32","Shift r/m32 to left imm8 places while shifting bits from r32 in from the right."
"SHLD r/m64, r64, CL","SHLQ CL, r64, r/m64","shldq CL, r64, r/m64","REX.W 0F A5 /r","N.E.","V","","","","Y","64","Shift r/m64 to left CL places while shifting bits from r64 in from the right."
"SHLD r/m64, r64, imm8","SHLQ imm8, r64, r/m64","shldq imm8, r64, r/m64","REX.W 0F A4 /r ib","N.E.","V","","","","Y","64","Shift r/m64 to left imm8 places while shifting bits from r64 in from the right."
"SHLX r32a, r/m32, r32b","SHLXL r32b, r/m32, r32a","shlxl r32b, r/m32, r32a","VEX.LZ.66.0F38.W0 F7 /r","V","V","BMI2","","","Y","32","Shift r/m32 logically left with count specified in r32b."
"SHLX r64a, r/m64, r64b","SHLXQ r64b, r/m64, r64a","shlxq r64b, r/m64, r64a","VEX.LZ.66.0F38.W1 F7 /r","N.E.","V","BMI2","","","Y","64","Shift r/m64 logically left with count specified in r64b."
"SHRD r/m16, r16, CL","SHRW CL, r16, r/m16","shrdw CL, r16, r/m16","0F AD /r","V","V","","operand16","","Y","16","Shift r/m16 to right CL places while shifting bits from r16 in from the left."
"SHRD r/m16, r16, imm8","SHRW imm8, r16, r/m16","shrdw imm8, r16, r/m16","0F AC /r ib","V","V","","operand16","","Y","16","Shift r/m16 to right imm8 places while shifting bits from r16 in from the left."
"SHRD r/m32, r32, CL","SHRL CL, r32, r/m32","shrdl CL, r32, r/m32","0F AD /r","V","V","","operand32","","Y","32","Shift r/m32 to right CL places while shifting bits from r32 in from the left."
"SHRD r/m32, r32, imm8","SHRL imm8, r32, r/m32","shrdl imm8, r32, r/m32","0F AC /r ib","V","V","","operand32","","Y","32","Shift r/m32 to right imm8 places while shifting bits from r32 in from the left."
"SHRD r/m64, r64, CL","SHRQ CL, r64, r/m64","shrdq CL, r64, r/m64","REX.W 0F AD /r","N.E.","V","","","","Y","64","Shift r/m64 to right CL places while shifting bits from r64 in from the left."
"SHRD r/m64, r64, imm8","SHRQ imm8, r64, r/m64","shrdq imm8, r64, r/m64","REX.W 0F AC /r ib","N.E.","V","","","","Y","64","Shift r/m64 to right imm8 places while shifting bits from r64 in from the left."
"SHRX r32a, r/m32, r32b","SHRXL r32b, r/m32, r32a","shrxl r32b, r/m32, r32a","VEX.LZ.F2.0F38.W0 F7 /r","V","V","BMI2","","","Y","32","Shift r/m32 logically right with count specified in r32b."
"SHRX r64a, r/m64, r64b","SHRXQ r64b, r/m64, r64a","shrxq r64b, r/m64, r64a","VEX.LZ.F2.0F38.W1 F7 /r","N.E.","V","BMI2","","","Y","64","Shift r/m64 logically right with count specified in r64b."
"SHUFPD xmm1, xmm2/m128, imm8","SHUFPD imm8, xmm2/m128, xmm1","shufpd imm8, xmm2/m128, xmm1","66 0F C6 /r ib","V","V","SSE2","","","","","Shuffle two pairs of double precision floating-point values from xmm1 and xmm2/m128 using imm8 to select from each pair, interleaved result is stored in xmm1."
"SHUFPS xmm1, xmm3/m128, imm8","SHUFPS imm8, xmm3/m128, xmm1","shufps imm8, xmm3/m128, xmm1","NP 0F C6 /r ib","V","V","SSE","","","","","Select from quadruplet of single precision floating- point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1."
"SLDT r/m16","SLDT r/m16","sldt r/m16","0F 00 /0","V","V","","","","","","Stores segment selector from LDTR in r/m16."
"SLDT r32/m16","SLDT{L/W} r32/m16","sldt{l/w} r32/m16","0F 00 /0","V","V","","operand32","w","","",""
"SMSW r/m16","SMSWW r/m16","smsww r/m16","0F 01 /4","V","V","","operand16","","Y","16","Store machine status word to r/m16."
"SMSW r32/m16","SMSW{L/W} r32/m16","smsw{l/w} r32/m16","0F 01 /4","V","V","","operand32","","Y","","Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined."
"SMSW r64/m16","SMSW{Q/W} r64/m16","smsw{q/w} r64/m16","REX.W 0F 01 /4","V","V","","","","Y","","Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined."
"SQRTPD xmm1, xmm2/m128","SQRTPD xmm2/m128, xmm1","sqrtpd xmm2/m128, xmm1","66 0F 51 /r","V","V","SSE2","","","","","Computes Square Roots of the packed double precision floating-point values in xmm2/m128 and stores the result in xmm1."
"SQRTPS xmm1, xmm2/m128","SQRTPS xmm2/m128, xmm1","sqrtps xmm2/m128, xmm1","NP 0F 51 /r","V","V","SSE","","","","","Computes Square Roots of the packed single precision floating-point values in xmm2/m128 and stores the result in xmm1."
"SQRTSD xmm1, xmm2/m64","SQRTSD xmm2/m64, xmm1","sqrtsd xmm2/m64, xmm1","F2 0F 51 /r","V","V","SSE2","","","","","Computes square root of the low double precision floating- point value in xmm2/m64 and stores the results in xmm1."
"SQRTSS xmm1, xmm2/m32","SQRTSS xmm2/m32, xmm1","sqrtss xmm2/m32, xmm1","F3 0F 51 /r","V","V","SSE","","","","","Computes square root of the low single precision floating-point value in xmm2/m32 and stores the results in xmm1."
"STAC","STAC","stac","NP 0F 01 CB","V","V","SMAP","","","","","Set the AC flag in the EFLAGS register."
"STC","STC","stc","F9","V","V","","","","","","Set CF flag."
"STD","STD","std","FD","V","V","","","","","","Set DF flag."
"STI","STI","sti","FB","V","V","","","","","","Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction."
"STMXCSR m32","STMXCSR m32","stmxcsr m32","NP 0F AE /3","V","V","SSE","","","","","Store contents of MXCSR register to m32."
"STOSB","STOSB","stosb","AA","V","V","","","","","","For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI."
"STOSD","STOSL","stosl","AB","V","V","","operand32","","","","For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI."
"STOSQ","STOSQ","stosq","REX.W AB","N.E.","V","","","","","","Store RAX at address RDI or EDI."
"STOSW","STOSW","stosw","AB","V","V","","operand16","","","","For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI."
"STR r/m16","STRW r/m16","strw r/m16","0F 00 /1","V","V","","operand16","","Y","16","Stores segment selector from TR in r/m16."
"STR r32/m16","STR{L/W} r32/m16","str{l/w} r32/m16","0F 00 /1","V","V","","operand32","w","Y","",""
"STR r64/m16","STR{Q/W} r64/m16","str{q/w} r64/m16","REX.W 0F 00 /1","N.E.","V","","","w","Y","",""
"SUB AL, imm8","SUBB imm8, AL","subb imm8, AL","2C ib","V","V","","","","Y","8","Subtract imm8 from AL."
"SUB AX, imm16","SUBW imm16, AX","subw imm16, AX","2D iw","V","V","","operand16","","Y","16","Subtract imm16 from AX."
"SUB EAX, imm32","SUBL imm32, EAX","subl imm32, EAX","2D id","V","V","","operand32","","Y","32","Subtract imm32 from EAX."
"SUB RAX, imm32","SUBQ imm32, RAX","subq imm32, RAX","REX.W 2D id","N.E.","V","","","","Y","64","Subtract imm32 sign-extended to 64-bits from RAX."
"SUB r/m16, imm16","SUBW imm16, r/m16","subw imm16, r/m16","81 /5 iw","V","V","","operand16","","Y","16","Subtract imm16 from r/m16."
"SUB r/m16, imm8","SUBW imm8, r/m16","subw imm8, r/m16","83 /5 ib","V","V","","operand16","","Y","16","Subtract sign-extended imm8 from r/m16."
"SUB r/m16, r16","SUBW r16, r/m16","subw r16, r/m16","29 /r","V","V","","operand16","","Y","16","Subtract r16 from r/m16."
"SUB r/m32, imm32","SUBL imm32, r/m32","subl imm32, r/m32","81 /5 id","V","V","","operand32","","Y","32","Subtract imm32 from r/m32."
"SUB r/m32, imm8","SUBL imm8, r/m32","subl imm8, r/m32","83 /5 ib","V","V","","operand32","","Y","32","Subtract sign-extended imm8 from r/m32."
"SUB r/m32, r32","SUBL r32, r/m32","subl r32, r/m32","29 /r","V","V","","operand32","","Y","32","Subtract r32 from r/m32."
"SUB r/m64, imm32","SUBQ imm32, r/m64","subq imm32, r/m64","REX.W 81 /5 id","N.E.","V","","","","Y","64","Subtract imm32 sign-extended to 64-bits from r/m64."
"SUB r/m64, imm8","SUBQ imm8, r/m64","subq imm8, r/m64","REX.W 83 /5 ib","N.E.","V","","","","Y","64","Subtract sign-extended imm8 from r/m64."
"SUB r/m64, r64","SUBQ r64, r/m64","subq r64, r/m64","REX.W 29 /r","N.E.","V","","","","Y","64","Subtract r64 from r/m64."
"SUB r/m8, imm8","SUBB imm8, r/m8","subb imm8, r/m8","80 /5 ib","V","V","","","","Y","8","Subtract imm8 from r/m8."
"SUB r/m8, imm8","SUBB imm8, r/m8","subb imm8, r/m8","REX 80 /5 ib","N.E.","V","","pseudo64","","Y","8","Subtract imm8 from r/m8."
"SUB r/m8, r8","SUBB r8, r/m8","subb r8, r/m8","28 /r","V","V","","","","Y","8","Subtract r8 from r/m8."
"SUB r/m8, r8","SUBB r8, r/m8","subb r8, r/m8","REX 28 /r","N.E.","V","","pseudo64","","Y","8","Subtract r8 from r/m8."
"SUB r16, r/m16","SUBW r/m16, r16","subw r/m16, r16","2B /r","V","V","","operand16","","Y","16","Subtract r/m16 from r16."
"SUB r32, r/m32","SUBL r/m32, r32","subl r/m32, r32","2B /r","V","V","","operand32","","Y","32","Subtract r/m32 from r32."
"SUB r64, r/m64","SUBQ r/m64, r64","subq r/m64, r64","REX.W 2B /r","N.E.","V","","","","Y","64","Subtract r/m64 from r64."
"SUB r8, r/m8","SUBB r/m8, r8","subb r/m8, r8","2A /r","V","V","","","","Y","8","Subtract r/m8 from r8."
"SUB r8, r/m8","SUBB r/m8, r8","subb r/m8, r8","REX 2A /r","N.E.","V","","pseudo64","","Y","8","Subtract r/m8 from r8."
"SUBPD xmm1, xmm2/m128","SUBPD xmm2/m128, xmm1","subpd xmm2/m128, xmm1","66 0F 5C /r","V","V","SSE2","","","","","Subtract packed double precision floating-point values in xmm2/mem from xmm1 and store result in xmm1."
"SUBPS xmm1, xmm2/m128","SUBPS xmm2/m128, xmm1","subps xmm2/m128, xmm1","NP 0F 5C /r","V","V","SSE","","","","","Subtract packed single precision floating-point values in xmm2/mem from xmm1 and store result in xmm1."
"SUBSD xmm1, xmm2/m64","SUBSD xmm2/m64, xmm1","subsd xmm2/m64, xmm1","F2 0F 5C /r","V","V","SSE2","","","","","Subtract the low double precision floating-point value in xmm2/m64 from xmm1 and store the result in xmm1."
"SUBSS xmm1, xmm2/m32","SUBSS xmm2/m32, xmm1","subss xmm2/m32, xmm1","F3 0F 5C /r","V","V","SSE","","","","","Subtract the low single precision floating-point value in xmm2/m32 from xmm1 and store the result in xmm1."
"SWAPGS","SWAPGS","swapgs","0F 01 F8","I","V","","","","","","Exchanges the current GS base register value with the value contained in MSR address C0000102H."
"SYSCALL","SYSCALL","syscall","0F 05","I","V","","","","","","Fast call to privilege level 0 system procedures."
"SYSENTER","SYSENTER","sysenter","0F 34","V","V","PentiumII","","","","","Fast call to privilege level 0 system procedures."
"SYSEXIT","SYSEXIT","sysexit","0F 35","V","V","PentiumII","ignoreREXW","","","","Fast return to privilege level 3 user code."
"SYSEXIT","SYSEXIT","sysexit","REX.W 0F 35","N.E.","V","","pseudo","","","","Fast return to 64-bit mode privilege level 3 user code."
"SYSRET","SYSRET","sysretw/sysretl/sysretl","0F 07","I","V","","ignoreREXW","","","","Return to compatibility mode from fast system call."
"SYSRET","SYSRET","sysretw/sysretl/sysretl","REX.W 0F 07","I","V","","pseudo","","","","Return to 64-bit mode from fast system call."
"TEST AL, imm8","TESTB imm8, AL","testb imm8, AL","A8 ib","V","V","","","","Y","8","AND imm8 with AL; set SF, ZF, PF according to result."
"TEST AX, imm16","TESTW imm16, AX","testw imm16, AX","A9 iw","V","V","","operand16","","Y","16","AND imm16 with AX; set SF, ZF, PF according to result."
"TEST EAX, imm32","TESTL imm32, EAX","testl imm32, EAX","A9 id","V","V","","operand32","","Y","32","AND imm32 with EAX; set SF, ZF, PF according to result."
"TEST RAX, imm32","TESTQ imm32, RAX","testq imm32, RAX","REX.W A9 id","N.E.","V","","","","Y","64","AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result."
"TEST r/m16, imm16","TESTW imm16, r/m16","testw imm16, r/m16","F7 /0 iw","V","V","","operand16","","Y","16","AND imm16 with r/m16; set SF, ZF, PF according to result."
"TEST r/m16, r16","TESTW r16, r/m16","testw r16, r/m16","85 /r","V","V","","operand16","","Y","16","AND r16 with r/m16; set SF, ZF, PF according to result."
"TEST r/m32, imm32","TESTL imm32, r/m32","testl imm32, r/m32","F7 /0 id","V","V","","operand32","","Y","32","AND imm32 with r/m32; set SF, ZF, PF according to result."
"TEST r/m32, r32","TESTL r32, r/m32","testl r32, r/m32","85 /r","V","V","","operand32","","Y","32","AND r32 with r/m32; set SF, ZF, PF according to result."
"TEST r/m64, imm32","TESTQ imm32, r/m64","testq imm32, r/m64","REX.W F7 /0 id","N.E.","V","","","","Y","64","AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result."
"TEST r/m64, r64","TESTQ r64, r/m64","testq r64, r/m64","REX.W 85 /r","N.E.","V","","","","Y","64","AND r64 with r/m64; set SF, ZF, PF according to result."
"TEST r/m8, imm8","TESTB imm8, r/m8","testb imm8, r/m8","F6 /0 ib","V","V","","","","Y","8","AND imm8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m8, imm8","TESTB imm8, r/m8","testb imm8, r/m8","REX F6 /0 ib","N.E.","V","","pseudo64","","Y","8","AND imm8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m8, r8","TESTB r8, r/m8","testb r8, r/m8","84 /r","V","V","","","","Y","8","AND r8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m8, r8","TESTB r8, r/m8","testb r8, r/m8","REX 84 /r","N.E.","V","","pseudo64","","Y","8","AND r8 with r/m8; set SF, ZF, PF according to result."
"TZCNT r16, r/m16","TZCNTW r/m16, r16","tzcntw r/m16, r16","F3 0F BC /r","V","V","BMI1","operand16","","Y","16","Count the number of trailing zero bits in r/m16, return result in r16."
"TZCNT r32, r/m32","TZCNTL r/m32, r32","tzcntl r/m32, r32","F3 0F BC /r","V","V","BMI1","operand32","","Y","32","Count the number of trailing zero bits in r/m32, return result in r32."
"TZCNT r64, r/m64","TZCNTQ r/m64, r64","tzcntq r/m64, r64","F3 REX.W 0F BC /r","N.E.","V","BMI1","","","Y","64","Count the number of trailing zero bits in r/m64, return result in r64."
"UCOMISD xmm1, xmm2/m64","UCOMISD xmm2/m64, xmm1","ucomisd xmm2/m64, xmm1","66 0F 2E /r","V","V","SSE2","","","","","Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"UCOMISS xmm1, xmm2/m32","UCOMISS xmm2/m32, xmm1","ucomiss xmm2/m32, xmm1","NP 0F 2E /r","V","V","SSE","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"UD0 r32, r/m32","UD0 r/m32, r32","ud0 r/m32, r32","0F FF /r","V","V","","","","","","Raise invalid opcode exception."
"UD1","UD1","ud1","0F B9","V","V","","","","","",""
"UD1 r32, r/m32","UD1 r/m32, r32","ud1 r/m32, r32","0F B9 /r","V","V","","","","","","Raise invalid opcode exception."
"UD2","UD2","ud2","0F 0B","V","V","","","","","","Raise invalid opcode exception."
"UNPCKHPD xmm1, xmm2/m128","UNPCKHPD xmm2/m128, xmm1","unpckhpd xmm2/m128, xmm1","66 0F 15 /r","V","V","SSE2","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of xmm1 and xmm2/m128."
"UNPCKHPS xmm1, xmm2/m128","UNPCKHPS xmm2/m128, xmm1","unpckhps xmm2/m128, xmm1","NP 0F 15 /r","V","V","SSE","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of xmm1 and xmm2/m128."
"UNPCKLPD xmm1, xmm2/m128","UNPCKLPD xmm2/m128, xmm1","unpcklpd xmm2/m128, xmm1","66 0F 14 /r","V","V","SSE2","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of xmm1 and xmm2/m128."
"UNPCKLPS xmm1, xmm2/m128","UNPCKLPS xmm2/m128, xmm1","unpcklps xmm2/m128, xmm1","NP 0F 14 /r","V","V","SSE","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of xmm1 and xmm2/m128."
"VADDPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VADDPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vaddpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 58 /r","V","V","AVX512VL AVX512F","","","","","Add packed double precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPD xmm1, xmm2, xmm3/m128","VADDPD xmm3/m128, xmm2, xmm1","vaddpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 58 /r","V","V","AVX","","","","","Add packed double precision floating-point values from xmm3/mem to xmm2 and store result in xmm1."
"VADDPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VADDPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vaddpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 58 /r","V","V","AVX512VL AVX512F","","","","","Add packed double precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPD ymm1, ymm2, ymm3/m256","VADDPD ymm3/m256, ymm2, ymm1","vaddpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 58 /r","V","V","AVX","","","","","Add packed double precision floating-point values from ymm3/mem to ymm2 and store result in ymm1."
"VADDPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","VADDPD zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","vaddpd zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 58 /r","V","V","AVX512F","","","","","Add packed double precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"VADDPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VADDPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vaddps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 58 /r","V","V","AVX512VL AVX512F","","","","","Add packed single precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPS xmm1, xmm2, xmm3/m128","VADDPS xmm3/m128, xmm2, xmm1","vaddps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 58 /r","V","V","AVX","","","","","Add packed single precision floating-point values from xmm3/m128 to xmm2 and store result in xmm1."
"VADDPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VADDPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vaddps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 58 /r","V","V","AVX512VL AVX512F","","","","","Add packed single precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPS ymm1, ymm2, ymm3/m256","VADDPS ymm3/m256, ymm2, ymm1","vaddps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 58 /r","V","V","AVX","","","","","Add packed single precision floating-point values from ymm3/m256 to ymm2 and store result in ymm1."
"VADDPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","VADDPS zmm3/m512/m32bcst {er}, zmm2, zmm1 {k1}{z}","vaddps zmm3/m512/m32bcst {er}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 58 /r","V","V","AVX512F","","","","","Add packed single precision floating-point values from zmm3/m512/m32bcst to zmm2 and store result in zmm1 with writemask k1."
"VADDSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","VADDSD xmm3/m64{er}, xmm2, xmm1 {k1}{z}","vaddsd xmm3/m64{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 58 /r","V","V","AVX512F","","","","","Add the low double precision floating-point value from xmm3/m64 to xmm2 and store the result in xmm1 with writemask k1."
"VADDSD xmm1, xmm2, xmm3/m64","VADDSD xmm3/m64, xmm2, xmm1","vaddsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 58 /r","V","V","AVX","","","","","Add the low double precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSS xmm1, xmm2, xmm3/m32","VADDSS xmm3/m32, xmm2, xmm1","vaddss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 58 /r","V","V","AVX","","","","","Add the low single precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSS xmm1{k1}{z}, xmm2, xmm3/m32{er}","VADDSS xmm3/m32{er}, xmm2, xmm1{k1}{z}","vaddss xmm3/m32{er}, xmm2, xmm1{k1}{z}","EVEX.LLIG.F3.0F.W0 58 /r","V","V","AVX512F","","","","","Add the low single precision floating-point value from xmm3/m32 to xmm2 and store the result in xmm1with writemask k1."
"VADDSUBPD xmm1, xmm2, xmm3/m128","VADDSUBPD xmm3/m128, xmm2, xmm1","vaddsubpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D0 /r","V","V","AVX","","","","","Add/subtract packed double precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPD ymm1, ymm2, ymm3/m256","VADDSUBPD ymm3/m256, ymm2, ymm1","vaddsubpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG D0 /r","V","V","AVX","","","","","Add / subtract packed double precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"VADDSUBPS xmm1, xmm2, xmm3/m128","VADDSUBPS xmm3/m128, xmm2, xmm1","vaddsubps xmm3/m128, xmm2, xmm1","VEX.128.F2.0F.WIG D0 /r","V","V","AVX","","","","","Add/subtract single precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPS ymm1, ymm2, ymm3/m256","VADDSUBPS ymm3/m256, ymm2, ymm1","vaddsubps ymm3/m256, ymm2, ymm1","VEX.256.F2.0F.WIG D0 /r","V","V","AVX","","","","","Add / subtract single precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"VAESDEC xmm1, xmm2, xmm3/m128","VAESDEC xmm3/m128, xmm2, xmm1","vaesdec xmm3/m128, xmm2, xmm1","EVEX.128.66.0F38.WIG DE /r","V","V","VAES AVX512VL","","","Y","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESDEC xmm1, xmm2, xmm3/m128","VAESDEC xmm3/m128, xmm2, xmm1","vaesdec xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG DE /r","V","V","AES AVX","","","Y","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESDEC ymm1, ymm2, ymm3/m256","VAESDEC ymm3/m256, ymm2, ymm1","vaesdec ymm3/m256, ymm2, ymm1","EVEX.256.66.0F38.WIG DE /r","V","V","VAES AVX512VL","","","Y","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using two 128-bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESDEC ymm1, ymm2, ymm3/m256","VAESDEC ymm3/m256, ymm2, ymm1","vaesdec ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG DE /r","V","V","VAES","","","Y","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using two 128-bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESDEC zmm1, zmm2, zmm3/m512","VAESDEC zmm3/m512, zmm2, zmm1","vaesdec zmm3/m512, zmm2, zmm1","EVEX.512.66.0F38.WIG DE /r","V","V","VAES AVX512F","","","Y","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, using four 128-bit data (state) from zmm2 with four 128-bit round keys from zmm3/m512; store the result in zmm1."
"VAESDECLAST xmm1, xmm2, xmm3/m128","VAESDECLAST xmm3/m128, xmm2, xmm1","vaesdeclast xmm3/m128, xmm2, xmm1","EVEX.128.66.0F38.WIG DF /r","V","V","VAES AVX512VL","","","Y","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESDECLAST xmm1, xmm2, xmm3/m128","VAESDECLAST xmm3/m128, xmm2, xmm1","vaesdeclast xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG DF /r","V","V","AES AVX","","","Y","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESDECLAST ymm1, ymm2, ymm3/m256","VAESDECLAST ymm3/m256, ymm2, ymm1","vaesdeclast ymm3/m256, ymm2, ymm1","EVEX.256.66.0F38.WIG DF /r","V","V","VAES AVX512VL","","","Y","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using two 128- bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESDECLAST ymm1, ymm2, ymm3/m256","VAESDECLAST ymm3/m256, ymm2, ymm1","vaesdeclast ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG DF /r","V","V","VAES","","","Y","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using two 128- bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESDECLAST zmm1, zmm2, zmm3/m512","VAESDECLAST zmm3/m512, zmm2, zmm1","vaesdeclast zmm3/m512, zmm2, zmm1","EVEX.512.66.0F38.WIG DF /r","V","V","VAES AVX512F","","","Y","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, using four128-bit data (state) from zmm2 with four 128-bit round keys from zmm3/m512; store the result in zmm1."
"VAESENC xmm1, xmm2, xmm3/m128","VAESENC xmm3/m128, xmm2, xmm1","vaesenc xmm3/m128, xmm2, xmm1","EVEX.128.66.0F38.WIG DC /r","V","V","VAES AVX512VL","","","Y","","Perform one round of an AES encryption flow, using one 128-bit data (state) from xmm2 with one 128-bit round key from the xmm3/m128; store the result in xmm1."
"VAESENC xmm1, xmm2, xmm3/m128","VAESENC xmm3/m128, xmm2, xmm1","vaesenc xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG DC /r","V","V","AES AVX","","","Y","","Perform one round of an AES encryption flow, using one 128-bit data (state) from xmm2 with one 128-bit round key from the xmm3/m128; store the result in xmm1."
"VAESENC ymm1, ymm2, ymm3/m256","VAESENC ymm3/m256, ymm2, ymm1","vaesenc ymm3/m256, ymm2, ymm1","EVEX.256.66.0F38.WIG DC /r","V","V","VAES AVX512VL","","","Y","","Perform one round of an AES encryption flow, using two 128-bit data (state) from ymm2 with two 128-bit round keys from the ymm3/m256; store the result in ymm1."
"VAESENC ymm1, ymm2, ymm3/m256","VAESENC ymm3/m256, ymm2, ymm1","vaesenc ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG DC /r","V","V","VAES","","","Y","","Perform one round of an AES encryption flow, using two 128-bit data (state) from ymm2 with two 128-bit round keys from the ymm3/m256; store the result in ymm1."
"VAESENC zmm1, zmm2, zmm3/m512","VAESENC zmm3/m512, zmm2, zmm1","vaesenc zmm3/m512, zmm2, zmm1","EVEX.512.66.0F38.WIG DC /r","V","V","VAES AVX512F","","","Y","","Perform one round of an AES encryption flow, using four 128-bit data (state) from zmm2 with four 128-bit round keys from the zmm3/m512; store the result in zmm1."
"VAESENCLAST xmm1, xmm2, xmm3/m128","VAESENCLAST xmm3/m128, xmm2, xmm1","vaesenclast xmm3/m128, xmm2, xmm1","EVEX.128.66.0F38.WIG DD /r","V","V","VAES AVX512VL","","","Y","","Perform the last round of an AES encryption flow, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESENCLAST xmm1, xmm2, xmm3/m128","VAESENCLAST xmm3/m128, xmm2, xmm1","vaesenclast xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG DD /r","V","V","AES AVX","","","Y","","Perform the last round of an AES encryption flow, using one 128-bit data (state) from xmm2 with one 128-bit round key from xmm3/m128; store the result in xmm1."
"VAESENCLAST ymm1, ymm2, ymm3/m256","VAESENCLAST ymm3/m256, ymm2, ymm1","vaesenclast ymm3/m256, ymm2, ymm1","EVEX.256.66.0F38.WIG DD /r","V","V","VAES AVX512VL","","","Y","","Perform the last round of an AES encryption flow, using two 128-bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESENCLAST ymm1, ymm2, ymm3/m256","VAESENCLAST ymm3/m256, ymm2, ymm1","vaesenclast ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG DD /r","V","V","VAES","","","Y","","Perform the last round of an AES encryption flow, using two 128-bit data (state) from ymm2 with two 128-bit round keys from ymm3/m256; store the result in ymm1."
"VAESENCLAST zmm1, zmm2, zmm3/m512","VAESENCLAST zmm3/m512, zmm2, zmm1","vaesenclast zmm3/m512, zmm2, zmm1","EVEX.512.66.0F38.WIG DD /r","V","V","VAES AVX512F","","","Y","","Perform the last round of an AES encryption flow, using four 128-bit data (state) from zmm2 with four 128-bit round keys from zmm3/m512; store the result in zmm1."
"VAESIMC xmm1, xmm2/m128","VAESIMC xmm2/m128, xmm1","vaesimc xmm2/m128, xmm1","VEX.128.66.0F38.WIG DB /r","V","V","Both AES and AVX flags","","","","","Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1."
"VAESKEYGENASSIST xmm1, xmm2/m128, imm8","VAESKEYGENASSIST imm8, xmm2/m128, xmm1","vaeskeygenassist imm8, xmm2/m128, xmm1","VEX.128.66.0F3A.WIG DF /r ib","V","V","Both AES and AVX flags","","","","","Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1."
"VANDNPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VANDNPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vandnpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 55 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VANDNPD xmm1, xmm2, xmm3/m128","VANDNPD xmm3/m128, xmm2, xmm1","vandnpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F 55 /r","V","V","AVX","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in xmm2 and xmm3/mem."
"VANDNPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VANDNPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vandnpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 55 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VANDNPD ymm1, ymm2, ymm3/m256","VANDNPD ymm3/m256, ymm2, ymm1","vandnpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F 55 /r","V","V","AVX","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in ymm2 and ymm3/mem."
"VANDNPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VANDNPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vandnpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 55 /r","V","V","AVX512DQ","","","","","Return the bitwise logical AND NOT of packed double precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VANDNPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VANDNPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vandnps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 55 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VANDNPS xmm1, xmm2, xmm3/m128","VANDNPS xmm3/m128, xmm2, xmm1","vandnps xmm3/m128, xmm2, xmm1","VEX.128.0F 55 /r","V","V","AVX","","","","","Return the bitwise logical AND NOT of packed single precision floating-point values in xmm2 and xmm3/mem."
"VANDNPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VANDNPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vandnps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 55 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VANDNPS ymm1, ymm2, ymm3/m256","VANDNPS ymm3/m256, ymm2, ymm1","vandnps ymm3/m256, ymm2, ymm1","VEX.256.0F 55 /r","V","V","AVX","","","","","Return the bitwise logical AND NOT of packed single precision floating-point values in ymm2 and ymm3/mem."
"VANDNPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VANDNPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vandnps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 55 /r","V","V","AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"VANDPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VANDPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vandpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 54 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed double precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VANDPD xmm1, xmm2, xmm3/m128","VANDPD xmm3/m128, xmm2, xmm1","vandpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F 54 /r","V","V","AVX","","","","","Return the bitwise logical AND of packed double precision floating-point values in xmm2 and xmm3/mem."
"VANDPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VANDPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vandpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 54 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed double precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VANDPD ymm1, ymm2, ymm3/m256","VANDPD ymm3/m256, ymm2, ymm1","vandpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F 54 /r","V","V","AVX","","","","","Return the bitwise logical AND of packed double precision floating-point values in ymm2 and ymm3/mem."
"VANDPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VANDPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vandpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 54 /r","V","V","AVX512DQ","","","","","Return the bitwise logical AND of packed double precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VANDPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VANDPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vandps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 54 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VANDPS xmm1, xmm2, xmm3/m128","VANDPS xmm3/m128, xmm2, xmm1","vandps xmm3/m128, xmm2, xmm1","VEX.128.0F 54 /r","V","V","AVX","","","","","Return the bitwise logical AND of packed single precision floating-point values in xmm2 and xmm3/mem."
"VANDPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VANDPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vandps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 54 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VANDPS ymm1, ymm2, ymm3/m256","VANDPS ymm3/m256, ymm2, ymm1","vandps ymm3/m256, ymm2, ymm1","VEX.256.0F 54 /r","V","V","AVX","","","","","Return the bitwise logical AND of packed single precision floating-point values in ymm2 and ymm3/mem."
"VANDPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VANDPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vandps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 54 /r","V","V","AVX512DQ","","","","","Return the bitwise logical AND of packed single precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"VBLENDPS xmm1, xmm2, xmm3/m128, imm8","VBLENDPS imm8, xmm3/m128, xmm2, xmm1","vblendps imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 0C /r ib","V","V","AVX","","","","","Select packed single precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1."
"VBLENDPS ymm1, ymm2, ymm3/m256, imm8","VBLENDPS imm8, ymm3/m256, ymm2, ymm1","vblendps imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 0C /r ib","V","V","AVX","","","","","Select packed single precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1."
"VBLENDVPD xmm1, xmm2, xmm3/m128, xmm4","VBLENDVPD xmm4, xmm3/m128, xmm2, xmm1","vblendvpd xmm4, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.W0 4B /r /is4","V","V","AVX","","","","","Conditionally copy double precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4."
"VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4","VBLENDVPD ymm4, ymm3/m256, ymm2, ymm1","vblendvpd ymm4, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.W0 4B /r /is4","V","V","AVX","","","","","Conditionally copy double precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4."
"VBLENDVPS xmm1, xmm2, xmm3/m128, xmm4","VBLENDVPS xmm4, xmm3/m128, xmm2, xmm1","vblendvps xmm4, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.W0 4A /r /is4","V","V","AVX","","","","","Conditionally copy single precision floating- point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4."
"VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4","VBLENDVPS ymm4, ymm3/m256, ymm2, ymm1","vblendvps ymm4, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.W0 4A /r /is4","V","V","AVX","","","","","Conditionally copy single precision floating- point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4."
"VCMPPD k1 {k2}, xmm2, xmm3/m128/m64bcst, imm8","VCMPPD imm8, xmm3/m128/m64bcst, xmm2, k1 {k2}","vcmppd imm8, xmm3/m128/m64bcst, xmm2, k1 {k2}","EVEX.128.66.0F.W1 C2 /r ib","V","V","AVX512VL AVX512F","","","","","Compare packed double precision floating-point values in xmm3/m128/m64bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, ymm2, ymm3/m256/m64bcst, imm8","VCMPPD imm8, ymm3/m256/m64bcst, ymm2, k1 {k2}","vcmppd imm8, ymm3/m256/m64bcst, ymm2, k1 {k2}","EVEX.256.66.0F.W1 C2 /r ib","V","V","AVX512VL AVX512F","","","","","Compare packed double precision floating-point values in ymm3/m256/m64bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, zmm2, zmm3/m512/m64bcst{sae}, imm8","VCMPPD imm8, zmm3/m512/m64bcst{sae}, zmm2, k1 {k2}","vcmppd imm8, zmm3/m512/m64bcst{sae}, zmm2, k1 {k2}","EVEX.512.66.0F.W1 C2 /r ib","V","V","AVX512F","","","","","Compare packed double precision floating-point values in zmm3/m512/m64bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD xmm1, xmm2, xmm3/m128, imm8","VCMPPD imm8, xmm3/m128, xmm2, xmm1","vcmppd imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare packed double precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPD ymm1, ymm2, ymm3/m256, imm8","VCMPPD imm8, ymm3/m256, ymm2, ymm1","vcmppd imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare packed double precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPS k1 {k2}, xmm2, xmm3/m128/m32bcst, imm8","VCMPPS imm8, xmm3/m128/m32bcst, xmm2, k1 {k2}","vcmpps imm8, xmm3/m128/m32bcst, xmm2, k1 {k2}","EVEX.128.0F.W0 C2 /r ib","V","V","AVX512VL AVX512F","","","","","Compare packed single precision floating-point values in xmm3/m128/m32bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, ymm2, ymm3/m256/m32bcst, imm8","VCMPPS imm8, ymm3/m256/m32bcst, ymm2, k1 {k2}","vcmpps imm8, ymm3/m256/m32bcst, ymm2, k1 {k2}","EVEX.256.0F.W0 C2 /r ib","V","V","AVX512VL AVX512F","","","","","Compare packed single precision floating-point values in ymm3/m256/m32bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, zmm2, zmm3/m512/m32bcst{sae}, imm8","VCMPPS imm8, zmm3/m512/m32bcst{sae}, zmm2, k1 {k2}","vcmpps imm8, zmm3/m512/m32bcst{sae}, zmm2, k1 {k2}","EVEX.512.0F.W0 C2 /r ib","V","V","AVX512F","","","","","Compare packed single precision floating-point values in zmm3/m512/m32bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS xmm1, xmm2, xmm3/m128, imm8","VCMPPS imm8, xmm3/m128, xmm2, xmm1","vcmpps imm8, xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare packed single precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPS ymm1, ymm2, ymm3/m256, imm8","VCMPPS imm8, ymm3/m256, ymm2, ymm1","vcmpps imm8, ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare packed single precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPSD k1 {k2}, xmm2, xmm3/m64{sae}, imm8","VCMPSD imm8, xmm3/m64{sae}, xmm2, k1 {k2}","vcmpsd imm8, xmm3/m64{sae}, xmm2, k1 {k2}","EVEX.LLIG.F2.0F.W1 C2 /r ib","V","V","AVX512F","","","","","Compare low double precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPSD xmm1, xmm2, xmm3/m64, imm8","VCMPSD imm8, xmm3/m64, xmm2, xmm1","vcmpsd imm8, xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare low double precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate."
"VCMPSS k1 {k2}, xmm2, xmm3/m32{sae}, imm8","VCMPSS imm8, xmm3/m32{sae}, xmm2, k1 {k2}","vcmpss imm8, xmm3/m32{sae}, xmm2, k1 {k2}","EVEX.LLIG.F3.0F.W0 C2 /r ib","V","V","AVX512F","","","","","Compare low single precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPSS xmm1, xmm2, xmm3/m32, imm8","VCMPSS imm8, xmm3/m32, xmm2, xmm1","vcmpss imm8, xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG C2 /r ib","V","V","AVX","","","","","Compare low single precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate."
"VCOMISD xmm1, xmm2/m64","VCOMISD xmm2/m64, xmm1","vcomisd xmm2/m64, xmm1","VEX.LIG.66.0F.WIG 2F /r","V","V","AVX","","","","","Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISD xmm1, xmm2/m64{sae}","VCOMISD xmm2/m64{sae}, xmm1","vcomisd xmm2/m64{sae}, xmm1","EVEX.LLIG.66.0F.W1 2F /r","V","V","AVX512F","","","","","Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32","VCOMISS xmm2/m32, xmm1","vcomiss xmm2/m32, xmm1","VEX.LIG.0F.WIG 2F /r","V","V","AVX","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32{sae}","VCOMISS xmm2/m32{sae}, xmm1","vcomiss xmm2/m32{sae}, xmm1","EVEX.LLIG.0F.W0 2F /r","V","V","AVX512F","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCVTDQ2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","VCVTDQ2PD xmm2/m64/m32bcst, xmm1 {k1}{z}","vcvtdq2pd xmm2/m64/m32bcst, xmm1 {k1}{z}","EVEX.128.F3.0F.W0 E6 /r","V","V","AVX512VL AVX512F","","","","","Convert 2 packed signed doubleword integers from xmm2/m64/m32bcst to eight packed double precision floating-point values in xmm1 with writemask k1."
"VCVTDQ2PD xmm1, xmm2/m64","VCVTDQ2PD xmm2/m64, xmm1","vcvtdq2pd xmm2/m64, xmm1","VEX.128.F3.0F.WIG E6 /r","V","V","AVX","","","","","Convert two packed signed doubleword integers from xmm2/mem to two packed double precision floating- point values in xmm1."
"VCVTDQ2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","VCVTDQ2PD xmm2/m128/m32bcst, ymm1 {k1}{z}","vcvtdq2pd xmm2/m128/m32bcst, ymm1 {k1}{z}","EVEX.256.F3.0F.W0 E6 /r","V","V","AVX512VL AVX512F","","","","","Convert 4 packed signed doubleword integers from xmm2/m128/m32bcst to 4 packed double precision floating-point values in ymm1 with writemask k1."
"VCVTDQ2PD ymm1, xmm2/m128","VCVTDQ2PD xmm2/m128, ymm1","vcvtdq2pd xmm2/m128, ymm1","VEX.256.F3.0F.WIG E6 /r","V","V","AVX","","","","","Convert four packed signed doubleword integers from xmm2/mem to four packed double precision floating- point values in ymm1."
"VCVTDQ2PD zmm1 {k1}{z}, ymm2/m256/m32bcst","VCVTDQ2PD ymm2/m256/m32bcst, zmm1 {k1}{z}","vcvtdq2pd ymm2/m256/m32bcst, zmm1 {k1}{z}","EVEX.512.F3.0F.W0 E6 /r","V","V","AVX512F","","","","","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed double precision floating-point values in zmm1 with writemask k1."
"VCVTPD2PS xmm1 {k1}{z}, xmm2/m128/m64bcst","VCVTPD2PS xmm2/m128/m64bcst, xmm1 {k1}{z}","vcvtpd2ps xmm2/m128/m64bcst, xmm1 {k1}{z}","EVEX.128.66.0F.W1 5A /r","V","V","AVX512VL AVX512F","","","Y","","Convert two packed double precision floating-point values in xmm2/m128/m64bcst to two single precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS xmm1 {k1}{z}, ymm2/m256/m64bcst","VCVTPD2PS ymm2/m256/m64bcst, xmm1 {k1}{z}","vcvtpd2ps ymm2/m256/m64bcst, xmm1 {k1}{z}","EVEX.256.66.0F.W1 5A /r","V","V","AVX512VL AVX512F","","","Y","","Convert four packed double precision floating-point values in ymm2/m256/m64bcst to four single precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS xmm1, xmm2/m128","VCVTPD2PSX xmm2/m128, xmm1","vcvtpd2psx xmm2/m128, xmm1","VEX.128.66.0F.WIG 5A /r","V","V","AVX","","","Y","128","Convert two packed double precision floating-point values in xmm2/mem to two single precision floating-point values in xmm1."
"VCVTPD2PS xmm1, ymm2/m256","VCVTPD2PSY ymm2/m256, xmm1","vcvtpd2psy ymm2/m256, xmm1","VEX.256.66.0F.WIG 5A /r","V","V","AVX","","","Y","256","Convert four packed double precision floating-point values in ymm2/mem to four single precision floating-point values in xmm1."
"VCVTPD2PS ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","VCVTPD2PS zmm2/m512/m64bcst{er}, ymm1 {k1}{z}","vcvtpd2ps zmm2/m512/m64bcst{er}, ymm1 {k1}{z}","EVEX.512.66.0F.W1 5A /r","V","V","AVX512F","","","Y","","Convert eight packed double precision floating-point values in zmm2/m512/m64bcst to eight single precision floating-point values in ymm1with writemask k1."
"VCVTPS2DQ xmm1 {k1}{z}, xmm2/m128/m32bcst","VCVTPS2DQ xmm2/m128/m32bcst, xmm1 {k1}{z}","vcvtps2dq xmm2/m128/m32bcst, xmm1 {k1}{z}","EVEX.128.66.0F.W0 5B /r","V","V","AVX512VL AVX512F","","","","","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 subject to writemask k1."
"VCVTPS2DQ xmm1, xmm2/m128","VCVTPS2DQ xmm2/m128, xmm1","vcvtps2dq xmm2/m128, xmm1","VEX.128.66.0F.WIG 5B /r","V","V","AVX","","","","","Convert four packed single precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1."
"VCVTPS2DQ ymm1 {k1}{z}, ymm2/m256/m32bcst","VCVTPS2DQ ymm2/m256/m32bcst, ymm1 {k1}{z}","vcvtps2dq ymm2/m256/m32bcst, ymm1 {k1}{z}","EVEX.256.66.0F.W0 5B /r","V","V","AVX512VL AVX512F","","","","","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 subject to writemask k1."
"VCVTPS2DQ ymm1, ymm2/m256","VCVTPS2DQ ymm2/m256, ymm1","vcvtps2dq ymm2/m256, ymm1","VEX.256.66.0F.WIG 5B /r","V","V","AVX","","","","","Convert eight packed single precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1."
"VCVTPS2DQ zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","VCVTPS2DQ zmm2/m512/m32bcst{er}, zmm1 {k1}{z}","vcvtps2dq zmm2/m512/m32bcst{er}, zmm1 {k1}{z}","EVEX.512.66.0F.W0 5B /r","V","V","AVX512F","","","","","Convert sixteen packed single precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 subject to writemask k1."
"VCVTPS2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","VCVTPS2PD xmm2/m64/m32bcst, xmm1 {k1}{z}","vcvtps2pd xmm2/m64/m32bcst, xmm1 {k1}{z}","EVEX.128.0F.W0 5A /r","V","V","AVX512VL AVX512F","","","","","Convert two packed single precision floating-point values in xmm2/m64/m32bcst to packed double precision floating- point values in xmm1 with writemask k1."
"VCVTPS2PD xmm1, xmm2/m64","VCVTPS2PD xmm2/m64, xmm1","vcvtps2pd xmm2/m64, xmm1","VEX.128.0F.WIG 5A /r","V","V","AVX","","","","","Convert two packed single precision floating-point values in xmm2/m64 to two packed double precision floating-point values in xmm1."
"VCVTPS2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","VCVTPS2PD xmm2/m128/m32bcst, ymm1 {k1}{z}","vcvtps2pd xmm2/m128/m32bcst, ymm1 {k1}{z}","EVEX.256.0F.W0 5A /r","V","V","AVX512VL","","","","","Convert four packed single precision floating-point values in xmm2/m128/m32bcst to packed double precision floating-point values in ymm1 with writemask k1."
"VCVTPS2PD ymm1, xmm2/m128","VCVTPS2PD xmm2/m128, ymm1","vcvtps2pd xmm2/m128, ymm1","VEX.256.0F.WIG 5A /r","V","V","AVX","","","","","Convert four packed single precision floating-point values in xmm2/m128 to four packed double precision floating- point values in ymm1."
"VCVTPS2PD zmm1 {k1}{z}, ymm2/m256/m32bcst{sae}","VCVTPS2PD ymm2/m256/m32bcst{sae}, zmm1 {k1}{z}","vcvtps2pd ymm2/m256/m32bcst{sae}, zmm1 {k1}{z}","EVEX.512.0F.W0 5A /r","V","V","AVX512F","","","","","Convert eight packed single precision floating-point values in ymm2/m256/b32bcst to eight packed double precision floating-point values in zmm1 with writemask k1."
"VCVTSD2SI r32, xmm1/m64","VCVTSD2SI xmm1/m64, r32","vcvtsd2si xmm1/m64, r32","VEX.LIG.F2.0F.W0 2D /r","V","V","AVX","","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer r32."
"VCVTSD2SI r32, xmm1/m64{er}","VCVTSD2SI xmm1/m64{er}, r32","vcvtsd2si xmm1/m64{er}, r32","EVEX.LLIG.F2.0F.W0 2D /r","V","V","AVX512F","","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer r32."
"VCVTSD2SI r64, xmm1/m64","VCVTSD2SIQ xmm1/m64, r64","vcvtsd2siq xmm1/m64, r64","VEX.LIG.F2.0F.W1 2D /r","N.E.","V","AVX","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer sign- extended into r64."
"VCVTSD2SI r64, xmm1/m64{er}","VCVTSD2SIQ xmm1/m64{er}, r64","vcvtsd2siq xmm1/m64{er}, r64","EVEX.LLIG.F2.0F.W1 2D /r","N.E.","V","AVX512F","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer sign- extended into r64."
"VCVTSD2SS xmm1 {k1}{z}, xmm2, xmm3/m64{er}","VCVTSD2SS xmm3/m64{er}, xmm2, xmm1 {k1}{z}","vcvtsd2ss xmm3/m64{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 5A /r","V","V","AVX512F","","","","","Convert one double precision floating-point value in xmm3/m64 to one single precision floating-point value and merge with high bits in xmm2 under writemask k1."
"VCVTSD2SS xmm1, xmm2, xmm3/m64","VCVTSD2SS xmm3/m64, xmm2, xmm1","vcvtsd2ss xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 5A /r","V","V","AVX","","","","","Convert one double precision floating-point value in xmm3/m64 to one single precision floating-point value and merge with high bits in xmm2."
"VCVTSI2SD xmm1, xmm2, r/m32","VCVTSI2SDL r/m32, xmm2, xmm1","vcvtsi2sdl r/m32, xmm2, xmm1","EVEX.LLIG.F2.0F.W0 2A /r","V","V","AVX512F","","","Y","32","Convert one signed doubleword integer from r/m32 to one double precision floating-point value in xmm1."
"VCVTSI2SD xmm1, xmm2, r/m32","VCVTSI2SDL r/m32, xmm2, xmm1","vcvtsi2sdl r/m32, xmm2, xmm1","VEX.LIG.F2.0F.W0 2A /r","V","V","AVX","","","Y","32","Convert one signed doubleword integer from r/m32 to one double precision floating-point value in xmm1."
"VCVTSI2SD xmm1, xmm2, r/m64","VCVTSI2SDQ r/m64, xmm2, xmm1","vcvtsi2sdq r/m64, xmm2, xmm1","VEX.LIG.F2.0F.W1 2A /r","N.E.","V","AVX","","","Y","64","Convert one signed quadword integer from r/m64 to one double precision floating-point value in xmm1."
"VCVTSI2SD xmm1, xmm2, r/m64{er}","VCVTSI2SD r/m64{er}, xmm2, xmm1","vcvtsi2sd r/m64{er}, xmm2, xmm1","EVEX.LLIG.F2.0F.W1 2A /r","N.E.","V","AVX512F","","","Y","","Convert one signed quadword integer from r/m64 to one double precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, r/m32","VCVTSI2SSL r/m32, xmm2, xmm1","vcvtsi2ssl r/m32, xmm2, xmm1","VEX.LIG.F3.0F.W0 2A /r","V","V","AVX","","","Y","32","Convert one signed doubleword integer from r/m32 to one single precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, r/m32{er}","VCVTSI2SS r/m32{er}, xmm2, xmm1","vcvtsi2ss r/m32{er}, xmm2, xmm1","EVEX.LLIG.F3.0F.W0 2A /r","V","V","AVX512F","","","Y","","Convert one signed doubleword integer from r/m32 to one single precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, r/m64","VCVTSI2SSQ r/m64, xmm2, xmm1","vcvtsi2ssq r/m64, xmm2, xmm1","VEX.LIG.F3.0F.W1 2A /r","N.E.","V","AVX","","","Y","64","Convert one signed quadword integer from r/m64 to one single precision floating-point value in xmm1."
"VCVTSI2SS xmm1, xmm2, r/m64{er}","VCVTSI2SS r/m64{er}, xmm2, xmm1","vcvtsi2ss r/m64{er}, xmm2, xmm1","EVEX.LLIG.F3.0F.W1 2A /r","N.E.","V","AVX512F","","","Y","","Convert one signed quadword integer from r/m64 to one single precision floating-point value in xmm1."
"VCVTSS2SD xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","VCVTSS2SD xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","vcvtss2sd xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 5A /r","V","V","AVX512F","","","","","Convert one single precision floating-point value in xmm3/m32 to one double precision floating-point value and merge with high bits of xmm2 under writemask k1."
"VCVTSS2SD xmm1, xmm2, xmm3/m32","VCVTSS2SD xmm3/m32, xmm2, xmm1","vcvtss2sd xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 5A /r","V","V","AVX","","","","","Convert one single precision floating-point value in xmm3/m32 to one double precision floating-point value and merge with high bits of xmm2."
"VCVTSS2SI r32, xmm1/m32","VCVTSS2SI xmm1/m32, r32","vcvtss2si xmm1/m32, r32","VEX.LIG.F3.0F.W0 2D /r","V","V","AVX","","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32."
"VCVTSS2SI r32, xmm1/m32{er}","VCVTSS2SI xmm1/m32{er}, r32","vcvtss2si xmm1/m32{er}, r32","EVEX.LLIG.F3.0F.W0 2D /r","V","V","AVX512F","","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32."
"VCVTSS2SI r64, xmm1/m32","VCVTSS2SIQ xmm1/m32, r64","vcvtss2siq xmm1/m32, r64","VEX.LIG.F3.0F.W1 2D /r","N.E.","V","AVX","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64."
"VCVTSS2SI r64, xmm1/m32{er}","VCVTSS2SIQ xmm1/m32{er}, r64","vcvtss2siq xmm1/m32{er}, r64","EVEX.LLIG.F3.0F.W1 2D /r","N.E.","V","AVX512F","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64."
"VCVTTPD2DQ xmm1 {k1}{z}, xmm2/m128/m64bcst","VCVTTPD2DQ xmm2/m128/m64bcst, xmm1 {k1}{z}","vcvttpd2dq xmm2/m128/m64bcst, xmm1 {k1}{z}","EVEX.128.66.0F.W1 E6 /r","V","V","AVX512VL AVX512F","","","Y","","Convert two packed double precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ xmm1 {k1}{z}, ymm2/m256/m64bcst","VCVTTPD2DQ ymm2/m256/m64bcst, xmm1 {k1}{z}","vcvttpd2dq ymm2/m256/m64bcst, xmm1 {k1}{z}","EVEX.256.66.0F.W1 E6 /r","V","V","AVX512VL AVX512F","","","Y","","Convert four packed double precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ xmm1, xmm2/m128","VCVTTPD2DQX xmm2/m128, xmm1","vcvttpd2dqx xmm2/m128, xmm1","VEX.128.66.0F.WIG E6 /r","V","V","AVX","","","Y","128","Convert two packed double precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ xmm1, ymm2/m256","VCVTTPD2DQY ymm2/m256, xmm1","vcvttpd2dqy ymm2/m256, xmm1","VEX.256.66.0F.WIG E6 /r","V","V","AVX","","","Y","256","Convert four packed double precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ ymm1 {k1}{z}, zmm2/m512/m64bcst{sae}","VCVTTPD2DQ zmm2/m512/m64bcst{sae}, ymm1 {k1}{z}","vcvttpd2dq zmm2/m512/m64bcst{sae}, ymm1 {k1}{z}","EVEX.512.66.0F.W1 E6 /r","V","V","AVX512F","","","Y","","Convert eight packed double precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 using truncation subject to writemask k1."
"VCVTTPS2DQ xmm1 {k1}{z}, xmm2/m128/m32bcst","VCVTTPS2DQ xmm2/m128/m32bcst, xmm1 {k1}{z}","vcvttps2dq xmm2/m128/m32bcst, xmm1 {k1}{z}","EVEX.128.F3.0F.W0 5B /r","V","V","AVX512VL AVX512F","","","","","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2DQ xmm1, xmm2/m128","VCVTTPS2DQ xmm2/m128, xmm1","vcvttps2dq xmm2/m128, xmm1","VEX.128.F3.0F.WIG 5B /r","V","V","AVX","","","","","Convert four packed single precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation."
"VCVTTPS2DQ ymm1 {k1}{z}, ymm2/m256/m32bcst","VCVTTPS2DQ ymm2/m256/m32bcst, ymm1 {k1}{z}","vcvttps2dq ymm2/m256/m32bcst, ymm1 {k1}{z}","EVEX.256.F3.0F.W0 5B /r","V","V","AVX512VL AVX512F","","","","","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2DQ ymm1, ymm2/m256","VCVTTPS2DQ ymm2/m256, ymm1","vcvttps2dq ymm2/m256, ymm1","VEX.256.F3.0F.WIG 5B /r","V","V","AVX","","","","","Convert eight packed single precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation."
"VCVTTPS2DQ zmm1 {k1}{z}, zmm2/m512/m32bcst {sae}","VCVTTPS2DQ zmm2/m512/m32bcst {sae}, zmm1 {k1}{z}","vcvttps2dq zmm2/m512/m32bcst {sae}, zmm1 {k1}{z}","EVEX.512.F3.0F.W0 5B /r","V","V","AVX512F","","","","","Convert sixteen packed single precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 using truncation subject to writemask k1."
"VCVTTSD2SI r32, xmm1/m64","VCVTTSD2SI xmm1/m64, r32","vcvttsd2si xmm1/m64, r32","VEX.LIG.F2.0F.W0 2C /r","V","V","AVX","","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation."
"VCVTTSD2SI r32, xmm1/m64{sae}","VCVTTSD2SI xmm1/m64{sae}, r32","vcvttsd2si xmm1/m64{sae}, r32","EVEX.LLIG.F2.0F.W0 2C /r","V","V","AVX512F","","","Y","32","Convert one double precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation."
"VCVTTSD2SI r64, xmm1/m64","VCVTTSD2SIQ xmm1/m64, r64","vcvttsd2siq xmm1/m64, r64","VEX.LIG.F2.0F.W1 2C /r","N.E.","V","AVX","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation."
"VCVTTSD2SI r64, xmm1/m64{sae}","VCVTTSD2SIQ xmm1/m64{sae}, r64","vcvttsd2siq xmm1/m64{sae}, r64","EVEX.LLIG.F2.0F.W1 2C /r","N.E.","V","AVX512F","","","Y","64","Convert one double precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation."
"VCVTTSS2SI r32, xmm1/m32","VCVTTSS2SI xmm1/m32, r32","vcvttss2si xmm1/m32, r32","VEX.LIG.F3.0F.W0 2C /r","V","V","AVX","","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation."
"VCVTTSS2SI r32, xmm1/m32{sae}","VCVTTSS2SI xmm1/m32{sae}, r32","vcvttss2si xmm1/m32{sae}, r32","EVEX.LLIG.F3.0F.W0 2C /r","V","V","AVX512F","","","Y","32","Convert one single precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation."
"VCVTTSS2SI r64, xmm1/m32","VCVTTSS2SIQ xmm1/m32, r64","vcvttss2siq xmm1/m32, r64","VEX.LIG.F3.0F.W1 2C /r","N.E.","V","AVX","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation."
"VCVTTSS2SI r64, xmm1/m32{sae}","VCVTTSS2SIQ xmm1/m32{sae}, r64","vcvttss2siq xmm1/m32{sae}, r64","EVEX.LLIG.F3.0F.W1 2C /r","N.E.","V","AVX512F","","","Y","64","Convert one single precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation."
"VDIVPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VDIVPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vdivpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 5E /r","V","V","AVX512VL AVX512F","","","","","Divide packed double precision floating-point values in xmm2 by packed double precision floating-point values in xmm3/m128/m64bcst and write results to xmm1 subject to writemask k1."
"VDIVPD xmm1, xmm2, xmm3/m128","VDIVPD xmm3/m128, xmm2, xmm1","vdivpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 5E /r","V","V","AVX","","","","","Divide packed double precision floating-point values in xmm2 by packed double precision floating-point values in xmm3/mem."
"VDIVPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VDIVPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vdivpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 5E /r","V","V","AVX512VL AVX512F","","","","","Divide packed double precision floating-point values in ymm2 by packed double precision floating-point values in ymm3/m256/m64bcst and write results to ymm1 subject to writemask k1."
"VDIVPD ymm1, ymm2, ymm3/m256","VDIVPD ymm3/m256, ymm2, ymm1","vdivpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 5E /r","V","V","AVX","","","","","Divide packed double precision floating-point values in ymm2 by packed double precision floating-point values in ymm3/mem."
"VDIVPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","VDIVPD zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","vdivpd zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 5E /r","V","V","AVX512F","","","","","Divide packed double precision floating-point values in zmm2 by packed double precision floating-point values in zmm3/m512/m64bcst and write results to zmm1 subject to writemask k1."
"VDIVPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VDIVPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vdivps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 5E /r","V","V","AVX512VL AVX512F","","","","","Divide packed single precision floating-point values in xmm2 by packed single precision floating-point values in xmm3/m128/m32bcst and write results to xmm1 subject to writemask k1."
"VDIVPS xmm1, xmm2, xmm3/m128","VDIVPS xmm3/m128, xmm2, xmm1","vdivps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 5E /r","V","V","AVX","","","","","Divide packed single precision floating-point values in xmm2 by packed single precision floating-point values in xmm3/mem."
"VDIVPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VDIVPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vdivps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 5E /r","V","V","AVX512VL AVX512F","","","","","Divide packed single precision floating-point values in ymm2 by packed single precision floating-point values in ymm3/m256/m32bcst and write results to ymm1 subject to writemask k1."
"VDIVPS ymm1, ymm2, ymm3/m256","VDIVPS ymm3/m256, ymm2, ymm1","vdivps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 5E /r","V","V","AVX","","","","","Divide packed single precision floating-point values in ymm2 by packed single precision floating-point values in ymm3/mem."
"VDIVPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","VDIVPS zmm3/m512/m32bcst{er}, zmm2, zmm1 {k1}{z}","vdivps zmm3/m512/m32bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 5E /r","V","V","AVX512F","","","","","Divide packed single precision floating-point values in zmm2 by packed single precision floating-point values in zmm3/m512/m32bcst and write results to zmm1 subject to writemask k1."
"VDIVSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","VDIVSD xmm3/m64{er}, xmm2, xmm1 {k1}{z}","vdivsd xmm3/m64{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 5E /r","V","V","AVX512F","","","","","Divide low double precision floating-point value in xmm2 by low double precision floating-point value in xmm3/m64."
"VDIVSD xmm1, xmm2, xmm3/m64","VDIVSD xmm3/m64, xmm2, xmm1","vdivsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 5E /r","V","V","AVX","","","","","Divide low double precision floating-point value in xmm2 by low double precision floating-point value in xmm3/m64."
"VDIVSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","VDIVSS xmm3/m32{er}, xmm2, xmm1 {k1}{z}","vdivss xmm3/m32{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 5E /r","V","V","AVX512F","","","","","Divide low single precision floating-point value in xmm2 by low single precision floating-point value in xmm3/m32."
"VDIVSS xmm1, xmm2, xmm3/m32","VDIVSS xmm3/m32, xmm2, xmm1","vdivss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 5E /r","V","V","AVX","","","","","Divide low single precision floating-point value in xmm2 by low single precision floating-point value in xmm3/m32."
"VDPPD xmm1, xmm2, xmm3/m128, imm8","VDPPD imm8, xmm3/m128, xmm2, xmm1","vdppd imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 41 /r ib","V","V","AVX","","","","","Selectively multiply packed double precision floating-point values from xmm2 with packed double precision floating-point values from xmm3, add and selectively store the packed double precision floating-point values to xmm1."
"VDPPS xmm1, xmm2, xmm3/m128, imm8","VDPPS imm8, xmm3/m128, xmm2, xmm1","vdpps imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 40 /r ib","V","V","AVX","","","","","Multiply packed single precision floating-point values from xmm1 with packed single precision floating-point values from xmm2/mem selectively add and store to xmm1."
"VDPPS ymm1, ymm2, ymm3/m256, imm8","VDPPS imm8, ymm3/m256, ymm2, ymm1","vdpps imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 40 /r ib","V","V","AVX","","","","","Multiply packed single precision floating-point values from ymm2 with packed single precision floating-point values from ymm3/mem, selectively add pairs of elements and store to ymm1."
"VHADDPD xmm1, xmm2, xmm3/m128","VHADDPD xmm3/m128, xmm2, xmm1","vhaddpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 7C /r","V","V","AVX","","","","","Horizontal add packed double precision floating-point values from xmm2 and xmm3/mem."
"VHADDPD ymm1, ymm2, ymm3/m256","VHADDPD ymm3/m256, ymm2, ymm1","vhaddpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 7C /r","V","V","AVX","","","","","Horizontal add packed double precision floating-point values from ymm2 and ymm3/mem."
"VHADDPS xmm1, xmm2, xmm3/m128","VHADDPS xmm3/m128, xmm2, xmm1","vhaddps xmm3/m128, xmm2, xmm1","VEX.128.F2.0F.WIG 7C /r","V","V","AVX","","","","","Horizontal add packed single precision floating- point values from xmm2 and xmm3/mem."
"VHADDPS ymm1, ymm2, ymm3/m256","VHADDPS ymm3/m256, ymm2, ymm1","vhaddps ymm3/m256, ymm2, ymm1","VEX.256.F2.0F.WIG 7C /r","V","V","AVX","","","","","Horizontal add packed single precision floating- point values from ymm2 and ymm3/mem."
"VHSUBPD xmm1, xmm2, xmm3/m128","VHSUBPD xmm3/m128, xmm2, xmm1","vhsubpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 7D /r","V","V","AVX","","","","","Horizontal subtract packed double precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPD ymm1, ymm2, ymm3/m256","VHSUBPD ymm3/m256, ymm2, ymm1","vhsubpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 7D /r","V","V","AVX","","","","","Horizontal subtract packed double precision floating-point values from ymm2 and ymm3/mem."
"VHSUBPS xmm1, xmm2, xmm3/m128","VHSUBPS xmm3/m128, xmm2, xmm1","vhsubps xmm3/m128, xmm2, xmm1","VEX.128.F2.0F.WIG 7D /r","V","V","AVX","","","","","Horizontal subtract packed single precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPS ymm1, ymm2, ymm3/m256","VHSUBPS ymm3/m256, ymm2, ymm1","vhsubps ymm3/m256, ymm2, ymm1","VEX.256.F2.0F.WIG 7D /r","V","V","AVX","","","","","Horizontal subtract packed single precision floating-point values from ymm2 and ymm3/mem."
"VINSERTPS xmm1, xmm2, xmm3/m32, imm8","VINSERTPS imm8, xmm3/m32, xmm2, xmm1","vinsertps imm8, xmm3/m32, xmm2, xmm1","EVEX.128.66.0F3A.W0 21 /r ib","V","V","AVX512F","","","Y","","Insert a single precision floating-point value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8."
"VINSERTPS xmm1, xmm2, xmm3/m32, imm8","VINSERTPS imm8, xmm3/m32, xmm2, xmm1","vinsertps imm8, xmm3/m32, xmm2, xmm1","VEX.128.66.0F3A.WIG 21 /r ib","V","V","AVX","","","Y","","Insert a single precision floating-point value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8."
"VLDDQU xmm1, m128","VLDDQU m128, xmm1","vlddqu m128, xmm1","VEX.128.F2.0F.WIG F0 /r","V","V","AVX","","","","","Load unaligned packed integer values from mem to xmm1."
"VLDDQU ymm1, m256","VLDDQU m256, ymm1","vlddqu m256, ymm1","VEX.256.F2.0F.WIG F0 /r","V","V","AVX","","","","","Load unaligned packed integer values from mem to ymm1."
"VLDMXCSR m32","VLDMXCSR m32","vldmxcsr m32","VEX.LZ.0F.WIG AE /2","V","V","AVX","","","","","Load MXCSR register from m32."
"VMASKMOVDQU xmm1, xmm2","VMASKMOVDQU xmm2, xmm1","vmaskmovdqu xmm2, xmm1","VEX.128.66.0F.WIG F7 /r","V","V","AVX","","","","","Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI."
"VMAXPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VMAXPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vmaxpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 5F /r","V","V","AVX512VL AVX512F","","","","","Return the maximum packed double precision floating- point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMAXPD xmm1, xmm2, xmm3/m128","VMAXPD xmm3/m128, xmm2, xmm1","vmaxpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum double precision floating-point values between xmm2 and xmm3/m128."
"VMAXPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VMAXPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vmaxpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 5F /r","V","V","AVX512VL AVX512F","","","","","Return the maximum packed double precision floating- point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMAXPD ymm1, ymm2, ymm3/m256","VMAXPD ymm3/m256, ymm2, ymm1","vmaxpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum packed double precision floating- point values between ymm2 and ymm3/m256."
"VMAXPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","VMAXPD zmm3/m512/m64bcst{sae}, zmm2, zmm1 {k1}{z}","vmaxpd zmm3/m512/m64bcst{sae}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 5F /r","V","V","AVX512F","","","","","Return the maximum packed double precision floating- point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"VMAXPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VMAXPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vmaxps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 5F /r","V","V","AVX512VL AVX512F","","","","","Return the maximum packed single precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMAXPS xmm1, xmm2, xmm3/m128","VMAXPS xmm3/m128, xmm2, xmm1","vmaxps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum single precision floating-point values between xmm2 and xmm3/mem."
"VMAXPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VMAXPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vmaxps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 5F /r","V","V","AVX512VL AVX512F","","","","","Return the maximum packed single precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMAXPS ymm1, ymm2, ymm3/m256","VMAXPS ymm3/m256, ymm2, ymm1","vmaxps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum single precision floating-point values between ymm2 and ymm3/mem."
"VMAXPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","VMAXPS zmm3/m512/m32bcst{sae}, zmm2, zmm1 {k1}{z}","vmaxps zmm3/m512/m32bcst{sae}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 5F /r","V","V","AVX512F","","","","","Return the maximum packed single precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"VMAXSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","VMAXSD xmm3/m64{sae}, xmm2, xmm1 {k1}{z}","vmaxsd xmm3/m64{sae}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 5F /r","V","V","AVX512F","","","","","Return the maximum scalar double precision floating-point value between xmm3/m64 and xmm2."
"VMAXSD xmm1, xmm2, xmm3/m64","VMAXSD xmm3/m64, xmm2, xmm1","vmaxsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum scalar double precision floating-point value between xmm3/m64 and xmm2."
"VMAXSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","VMAXSS xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","vmaxss xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 5F /r","V","V","AVX512F","","","","","Return the maximum scalar single precision floating-point value between xmm3/m32 and xmm2."
"VMAXSS xmm1, xmm2, xmm3/m32","VMAXSS xmm3/m32, xmm2, xmm1","vmaxss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 5F /r","V","V","AVX","","","","","Return the maximum scalar single precision floating-point value between xmm3/m32 and xmm2."
"VMINPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VMINPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vminpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 5D /r","V","V","AVX512VL AVX512F","","","","","Return the minimum packed double precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMINPD xmm1, xmm2, xmm3/m128","VMINPD xmm3/m128, xmm2, xmm1","vminpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum double precision floating-point values between xmm2 and xmm3/mem."
"VMINPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VMINPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vminpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 5D /r","V","V","AVX512VL AVX512F","","","","","Return the minimum packed double precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMINPD ymm1, ymm2, ymm3/m256","VMINPD ymm3/m256, ymm2, ymm1","vminpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum packed double precision floating-point values between ymm2 and ymm3/mem."
"VMINPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","VMINPD zmm3/m512/m64bcst{sae}, zmm2, zmm1 {k1}{z}","vminpd zmm3/m512/m64bcst{sae}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 5D /r","V","V","AVX512F","","","","","Return the minimum packed double precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"VMINPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VMINPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vminps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 5D /r","V","V","AVX512VL AVX512F","","","","","Return the minimum packed single precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMINPS xmm1, xmm2, xmm3/m128","VMINPS xmm3/m128, xmm2, xmm1","vminps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum single precision floating-point values between xmm2 and xmm3/mem."
"VMINPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VMINPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vminps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 5D /r","V","V","AVX512VL AVX512F","","","","","Return the minimum packed single precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMINPS ymm1, ymm2, ymm3/m256","VMINPS ymm3/m256, ymm2, ymm1","vminps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum single double precision floating-point values between ymm2 and ymm3/mem."
"VMINPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","VMINPS zmm3/m512/m32bcst{sae}, zmm2, zmm1 {k1}{z}","vminps zmm3/m512/m32bcst{sae}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 5D /r","V","V","AVX512F","","","","","Return the minimum packed single precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"VMINSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","VMINSD xmm3/m64{sae}, xmm2, xmm1 {k1}{z}","vminsd xmm3/m64{sae}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 5D /r","V","V","AVX512F","","","","","Return the minimum scalar double precision floating- point value between xmm3/m64 and xmm2."
"VMINSD xmm1, xmm2, xmm3/m64","VMINSD xmm3/m64, xmm2, xmm1","vminsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum scalar double precision floating- point value between xmm3/m64 and xmm2."
"VMINSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","VMINSS xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","vminss xmm3/m32{sae}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 5D /r","V","V","AVX512F","","","","","Return the minimum scalar single precision floating-point value between xmm3/m32 and xmm2."
"VMINSS xmm1, xmm2, xmm3/m32","VMINSS xmm3/m32, xmm2, xmm1","vminss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 5D /r","V","V","AVX","","","","","Return the minimum scalar single precision floating-point value between xmm3/m32 and xmm2."
"VMOVAPD xmm1 {k1}{z}, xmm2/m128","VMOVAPD xmm2/m128, xmm1 {k1}{z}","vmovapd xmm2/m128, xmm1 {k1}{z}","EVEX.128.66.0F.W1 28 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed double precision floating- point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPD xmm1, xmm2/m128","VMOVAPD xmm2/m128, xmm1","vmovapd xmm2/m128, xmm1","VEX.128.66.0F.WIG 28 /r","V","V","AVX","","","","","Move aligned packed double precision floating- point values from xmm2/mem to xmm1."
"VMOVAPD xmm2/m128 {k1}{z}, xmm1","VMOVAPD xmm1, xmm2/m128 {k1}{z}","vmovapd xmm1, xmm2/m128 {k1}{z}","EVEX.128.66.0F.W1 29 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed double precision floating- point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPD xmm2/m128, xmm1","VMOVAPD xmm1, xmm2/m128","vmovapd xmm1, xmm2/m128","VEX.128.66.0F.WIG 29 /r","V","V","AVX","","","","","Move aligned packed double precision floating- point values from xmm1 to xmm2/mem."
"VMOVAPD ymm1 {k1}{z}, ymm2/m256","VMOVAPD ymm2/m256, ymm1 {k1}{z}","vmovapd ymm2/m256, ymm1 {k1}{z}","EVEX.256.66.0F.W1 28 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed double precision floating- point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPD ymm1, ymm2/m256","VMOVAPD ymm2/m256, ymm1","vmovapd ymm2/m256, ymm1","VEX.256.66.0F.WIG 28 /r","V","V","AVX","","","","","Move aligned packed double precision floating- point values from ymm2/mem to ymm1."
"VMOVAPD ymm2/m256 {k1}{z}, ymm1","VMOVAPD ymm1, ymm2/m256 {k1}{z}","vmovapd ymm1, ymm2/m256 {k1}{z}","EVEX.256.66.0F.W1 29 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed double precision floating- point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPD ymm2/m256, ymm1","VMOVAPD ymm1, ymm2/m256","vmovapd ymm1, ymm2/m256","VEX.256.66.0F.WIG 29 /r","V","V","AVX","","","","","Move aligned packed double precision floating- point values from ymm1 to ymm2/mem."
"VMOVAPD zmm1 {k1}{z}, zmm2/m512","VMOVAPD zmm2/m512, zmm1 {k1}{z}","vmovapd zmm2/m512, zmm1 {k1}{z}","EVEX.512.66.0F.W1 28 /r","V","V","AVX512F","","","","","Move aligned packed double precision floating- point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPD zmm2/m512 {k1}{z}, zmm1","VMOVAPD zmm1, zmm2/m512 {k1}{z}","vmovapd zmm1, zmm2/m512 {k1}{z}","EVEX.512.66.0F.W1 29 /r","V","V","AVX512F","","","","","Move aligned packed double precision floating- point values from zmm1 to zmm2/m512 using writemask k1."
"VMOVAPS xmm1 {k1}{z}, xmm2/m128","VMOVAPS xmm2/m128, xmm1 {k1}{z}","vmovaps xmm2/m128, xmm1 {k1}{z}","EVEX.128.0F.W0 28 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed single precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPS xmm1, xmm2/m128","VMOVAPS xmm2/m128, xmm1","vmovaps xmm2/m128, xmm1","VEX.128.0F.WIG 28 /r","V","V","AVX","","","","","Move aligned packed single precision floating-point values from xmm2/mem to xmm1."
"VMOVAPS xmm2/m128 {k1}{z}, xmm1","VMOVAPS xmm1, xmm2/m128 {k1}{z}","vmovaps xmm1, xmm2/m128 {k1}{z}","EVEX.128.0F.W0 29 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed single precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPS xmm2/m128, xmm1","VMOVAPS xmm1, xmm2/m128","vmovaps xmm1, xmm2/m128","VEX.128.0F.WIG 29 /r","V","V","AVX","","","","","Move aligned packed single precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPS ymm1 {k1}{z}, ymm2/m256","VMOVAPS ymm2/m256, ymm1 {k1}{z}","vmovaps ymm2/m256, ymm1 {k1}{z}","EVEX.256.0F.W0 28 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed single precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPS ymm1, ymm2/m256","VMOVAPS ymm2/m256, ymm1","vmovaps ymm2/m256, ymm1","VEX.256.0F.WIG 28 /r","V","V","AVX","","","","","Move aligned packed single precision floating-point values from ymm2/mem to ymm1."
"VMOVAPS ymm2/m256 {k1}{z}, ymm1","VMOVAPS ymm1, ymm2/m256 {k1}{z}","vmovaps ymm1, ymm2/m256 {k1}{z}","EVEX.256.0F.W0 29 /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed single precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPS ymm2/m256, ymm1","VMOVAPS ymm1, ymm2/m256","vmovaps ymm1, ymm2/m256","VEX.256.0F.WIG 29 /r","V","V","AVX","","","","","Move aligned packed single precision floating-point values from ymm1 to ymm2/mem."
"VMOVAPS zmm1 {k1}{z}, zmm2/m512","VMOVAPS zmm2/m512, zmm1 {k1}{z}","vmovaps zmm2/m512, zmm1 {k1}{z}","EVEX.512.0F.W0 28 /r","V","V","AVX512F","","","","","Move aligned packed single precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPS zmm2/m512 {k1}{z}, zmm1","VMOVAPS zmm1, zmm2/m512 {k1}{z}","vmovaps zmm1, zmm2/m512 {k1}{z}","EVEX.512.0F.W0 29 /r","V","V","AVX512F","","","","","Move aligned packed single precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"VMOVD xmm1, r/m32","VMOVD r/m32, xmm1","vmovd r/m32, xmm1","VEX.128.66.0F.W0 6E /r","V","V","AVX","","","","","Move doubleword from r/m32 to xmm1."
"VMOVDQA xmm1, xmm2/m128","VMOVDQA xmm2/m128, xmm1","vmovdqa xmm2/m128, xmm1","VEX.128.66.0F.WIG 6F /r","V","V","AVX","","","","","Move aligned packed integer values from xmm2/mem to xmm1."
"VMOVDQA xmm2/m128, xmm1","VMOVDQA xmm1, xmm2/m128","vmovdqa xmm1, xmm2/m128","VEX.128.66.0F.WIG 7F /r","V","V","AVX","","","","","Move aligned packed integer values from xmm1 to xmm2/mem."
"VMOVDQA ymm1, ymm2/m256","VMOVDQA ymm2/m256, ymm1","vmovdqa ymm2/m256, ymm1","VEX.256.66.0F.WIG 6F /r","V","V","AVX","","","","","Move aligned packed integer values from ymm2/mem to ymm1."
"VMOVDQA ymm2/m256, ymm1","VMOVDQA ymm1, ymm2/m256","vmovdqa ymm1, ymm2/m256","VEX.256.66.0F.WIG 7F /r","V","V","AVX","","","","","Move aligned packed integer values from ymm1 to ymm2/mem."
"VMOVDQA32 xmm1 {k1}{z}, xmm2/m128","VMOVDQA32 xmm2/m128, xmm1 {k1}{z}","vmovdqa32 xmm2/m128, xmm1 {k1}{z}","EVEX.128.66.0F.W0 6F /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA32 xmm2/m128 {k1}{z}, xmm1","VMOVDQA32 xmm1, xmm2/m128 {k1}{z}","vmovdqa32 xmm1, xmm2/m128 {k1}{z}","EVEX.128.66.0F.W0 7F /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA32 ymm1 {k1}{z}, ymm2/m256","VMOVDQA32 ymm2/m256, ymm1 {k1}{z}","vmovdqa32 ymm2/m256, ymm1 {k1}{z}","EVEX.256.66.0F.W0 6F /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA32 ymm2/m256 {k1}{z}, ymm1","VMOVDQA32 ymm1, ymm2/m256 {k1}{z}","vmovdqa32 ymm1, ymm2/m256 {k1}{z}","EVEX.256.66.0F.W0 7F /r","V","V","AVX512VL AVX512F","","","","","Move aligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA32 zmm1 {k1}{z}, zmm2/m512","VMOVDQA32 zmm2/m512, zmm1 {k1}{z}","vmovdqa32 zmm2/m512, zmm1 {k1}{z}","EVEX.512.66.0F.W0 6F /r","V","V","AVX512F","","","","","Move aligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA32 zmm2/m512 {k1}{z}, zmm1","VMOVDQA32 zmm1, zmm2/m512 {k1}{z}","vmovdqa32 zmm1, zmm2/m512 {k1}{z}","EVEX.512.66.0F.W0 7F /r","V","V","AVX512F","","","","","Move aligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQA64 xmm1 {k1}{z}, xmm2/m128","VMOVDQA64 xmm2/m128, xmm1 {k1}{z}","vmovdqa64 xmm2/m128, xmm1 {k1}{z}","EVEX.128.66.0F.W1 6F /r","V","V","AVX512VL AVX512F","","","Y","128","Move aligned packed quadword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA64 xmm2/m128 {k1}{z}, xmm1","VMOVDQA64 xmm1, xmm2/m128 {k1}{z}","vmovdqa64 xmm1, xmm2/m128 {k1}{z}","EVEX.128.66.0F.W1 7F /r","V","V","AVX512VL AVX512F","","","Y","","Move aligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA64 ymm1 {k1}{z}, ymm2/m256","VMOVDQA64 ymm2/m256, ymm1 {k1}{z}","vmovdqa64 ymm2/m256, ymm1 {k1}{z}","EVEX.256.66.0F.W1 6F /r","V","V","AVX512VL AVX512F","","","Y","256","Move aligned packed quadword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA64 ymm2/m256 {k1}{z}, ymm1","VMOVDQA64 ymm1, ymm2/m256 {k1}{z}","vmovdqa64 ymm1, ymm2/m256 {k1}{z}","EVEX.256.66.0F.W1 7F /r","V","V","AVX512VL AVX512F","","","Y","","Move aligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA64 zmm1 {k1}{z}, zmm2/m512","VMOVDQA64 zmm2/m512, zmm1 {k1}{z}","vmovdqa64 zmm2/m512, zmm1 {k1}{z}","EVEX.512.66.0F.W1 6F /r","V","V","AVX512F","","","Y","","Move aligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA64 zmm2/m512 {k1}{z}, zmm1","VMOVDQA64 zmm1, zmm2/m512 {k1}{z}","vmovdqa64 zmm1, zmm2/m512 {k1}{z}","EVEX.512.66.0F.W1 7F /r","V","V","AVX512F","","","Y","","Move aligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVHLPS xmm1, xmm2, xmm3","VMOVHLPS xmm3, xmm2, xmm1","vmovhlps xmm3, xmm2, xmm1","EVEX.128.0F.W0 12 /r","V","V","AVX512F","","","Y","","Merge two packed single precision floating-point values from high quadword of xmm3 and low quadword of xmm2."
"VMOVHLPS xmm1, xmm2, xmm3","VMOVHLPS xmm3, xmm2, xmm1","vmovhlps xmm3, xmm2, xmm1","VEX.128.0F.WIG 12 /r","V","V","AVX","","","Y","","Merge two packed single precision floating-point values from high quadword of xmm3 and low quadword of xmm2."
"VMOVLHPS xmm1, xmm2, xmm3","VMOVLHPS xmm3, xmm2, xmm1","vmovlhps xmm3, xmm2, xmm1","EVEX.128.0F.W0 16 /r","V","V","AVX512F","","","Y","","Merge two packed single precision floating-point values from low quadword of xmm3 and low quadword of xmm2."
"VMOVLHPS xmm1, xmm2, xmm3","VMOVLHPS xmm3, xmm2, xmm1","vmovlhps xmm3, xmm2, xmm1","VEX.128.0F.WIG 16 /r","V","V","AVX","","","Y","","Merge two packed single precision floating-point values from low quadword of xmm3 and low quadword of xmm2."
"VMOVMSKPD reg, xmm2","VMOVMSKPD xmm2, reg","vmovmskpd xmm2, reg","VEX.128.66.0F.WIG 50 /r","V","V","AVX","","","","","Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVMSKPD reg, ymm2","VMOVMSKPD ymm2, reg","vmovmskpd ymm2, reg","VEX.256.66.0F.WIG 50 /r","V","V","AVX","","","","","Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVMSKPS reg, xmm2","VMOVMSKPS xmm2, reg","vmovmskps xmm2, reg","VEX.128.0F.WIG 50 /r","V","V","AVX","","","","","Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVMSKPS reg, ymm2","VMOVMSKPS ymm2, reg","vmovmskps ymm2, reg","VEX.256.0F.WIG 50 /r","V","V","AVX","","","","","Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVNTDQ m128, xmm1","VMOVNTDQ xmm1, m128","vmovntdq xmm1, m128","EVEX.128.66.0F.W0 E7 /r","V","V","AVX512VL AVX512F","","","Y","","Move packed integer values in xmm1 to m128 using non- temporal hint."
"VMOVNTDQ m128, xmm1","VMOVNTDQ xmm1, m128","vmovntdq xmm1, m128","VEX.128.66.0F.WIG E7 /r","V","V","AVX","","","Y","","Move packed integer values in xmm1 to m128 using non- temporal hint."
"VMOVNTDQ m256, ymm1","VMOVNTDQ ymm1, m256","vmovntdq ymm1, m256","EVEX.256.66.0F.W0 E7 /r","V","V","AVX512VL AVX512F","","","Y","","Move packed integer values in zmm1 to m256 using non- temporal hint."
"VMOVNTDQ m256, ymm1","VMOVNTDQ ymm1, m256","vmovntdq ymm1, m256","VEX.256.66.0F.WIG E7 /r","V","V","AVX","","","Y","","Move packed integer values in ymm1 to m256 using non- temporal hint."
"VMOVNTDQ m512, zmm1","VMOVNTDQ zmm1, m512","vmovntdq zmm1, m512","EVEX.512.66.0F.W0 E7 /r","V","V","AVX512F","","","Y","","Move packed integer values in zmm1 to m512 using non- temporal hint."
"VMOVNTDQA xmm1, m128","VMOVNTDQA m128, xmm1","vmovntdqa m128, xmm1","EVEX.128.66.0F38.W0 2A /r","V","V","AVX512VL AVX512F","","","Y","","Move 128-bit data from m128 to xmm using non-temporal hint if WC memory type."
"VMOVNTDQA xmm1, m128","VMOVNTDQA m128, xmm1","vmovntdqa m128, xmm1","VEX.128.66.0F38.WIG 2A /r","V","V","AVX","","","Y","","Move double quadword from m128 to xmm using non- temporal hint if WC memory type."
"VMOVNTDQA ymm1, m256","VMOVNTDQA m256, ymm1","vmovntdqa m256, ymm1","EVEX.256.66.0F38.W0 2A /r","V","V","AVX512VL AVX512F","","","Y","","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMOVNTDQA ymm1, m256","VMOVNTDQA m256, ymm1","vmovntdqa m256, ymm1","VEX.256.66.0F38.WIG 2A /r","V","V","AVX2","","","Y","","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMOVNTDQA zmm1, m512","VMOVNTDQA m512, zmm1","vmovntdqa m512, zmm1","EVEX.512.66.0F38.W0 2A /r","V","V","AVX512F","","","Y","","Move 512-bit data from m512 to zmm using non-temporal hint if WC memory type."
"VMOVNTPD m128, xmm1","VMOVNTPD xmm1, m128","vmovntpd xmm1, m128","EVEX.128.66.0F.W1 2B /r","V","V","AVX512VL AVX512F","","","Y","","Move packed double precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m128, xmm1","VMOVNTPD xmm1, m128","vmovntpd xmm1, m128","VEX.128.66.0F.WIG 2B /r","V","V","AVX","","","Y","","Move packed double precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m256, ymm1","VMOVNTPD ymm1, m256","vmovntpd ymm1, m256","EVEX.256.66.0F.W1 2B /r","V","V","AVX512VL AVX512F","","","Y","","Move packed double precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPD m256, ymm1","VMOVNTPD ymm1, m256","vmovntpd ymm1, m256","VEX.256.66.0F.WIG 2B /r","V","V","AVX","","","Y","","Move packed double precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPD m512, zmm1","VMOVNTPD zmm1, m512","vmovntpd zmm1, m512","EVEX.512.66.0F.W1 2B /r","V","V","AVX512F","","","Y","","Move packed double precision values in zmm1 to m512 using non-temporal hint."
"VMOVNTPS m128, xmm1","VMOVNTPS xmm1, m128","vmovntps xmm1, m128","EVEX.128.0F.W0 2B /r","V","V","AVX512VL AVX512F","","","Y","","Move packed single precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPS m128, xmm1","VMOVNTPS xmm1, m128","vmovntps xmm1, m128","VEX.128.0F.WIG 2B /r","V","V","AVX","","","Y","","Move packed single precision values xmm1 to mem using non-temporal hint."
"VMOVNTPS m256, ymm1","VMOVNTPS ymm1, m256","vmovntps ymm1, m256","EVEX.256.0F.W0 2B /r","V","V","AVX512VL AVX512F","","","Y","","Move packed single precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPS m256, ymm1","VMOVNTPS ymm1, m256","vmovntps ymm1, m256","VEX.256.0F.WIG 2B /r","V","V","AVX","","","Y","","Move packed single precision values ymm1 to mem using non-temporal hint."
"VMOVNTPS m512, zmm1","VMOVNTPS zmm1, m512","vmovntps zmm1, m512","EVEX.512.0F.W0 2B /r","V","V","AVX512F","","","Y","","Move packed single precision values in zmm1 to m512 using non-temporal hint."
"VMOVQ xmm1, xmm2/m64","VMOVQ xmm2/m64, xmm1","vmovq xmm2/m64, xmm1","EVEX.128.F3.0F.W1 7E /r","V","V","AVX512F","","","Y","","Move quadword from xmm2/m64 to xmm1."
"VMOVQ xmm1, xmm2/m64","VMOVQ xmm2/m64, xmm1","vmovq xmm2/m64, xmm1","VEX.128.F3.0F.WIG 7E /r","V","V","AVX","","","Y","","Move quadword from xmm2 to xmm1."
"VMOVQ xmm1/m64, xmm2","VMOVQ xmm2, xmm1/m64","vmovq xmm2, xmm1/m64","EVEX.128.66.0F.W1 D6 /r","V","V","AVX512F","","","Y","","Move quadword from xmm2 register to xmm1/m64."
"VMOVQ xmm1/m64, xmm2","VMOVQ xmm2, xmm1/m64","vmovq xmm2, xmm1/m64","VEX.128.66.0F.WIG D6 /r","V","V","AVX","","","Y","","Move quadword from xmm2 register to xmm1/m64."
"VMOVSHDUP xmm1 {k1}{z}, xmm2/m128","VMOVSHDUP xmm2/m128, xmm1 {k1}{z}","vmovshdup xmm2/m128, xmm1 {k1}{z}","EVEX.128.F3.0F.W0 16 /r","V","V","AVX512VL AVX512F","","","","","Move odd index single precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask."
"VMOVSHDUP xmm1, xmm2/m128","VMOVSHDUP xmm2/m128, xmm1","vmovshdup xmm2/m128, xmm1","VEX.128.F3.0F.WIG 16 /r","V","V","AVX","","","","","Move odd index single precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSHDUP ymm1 {k1}{z}, ymm2/m256","VMOVSHDUP ymm2/m256, ymm1 {k1}{z}","vmovshdup ymm2/m256, ymm1 {k1}{z}","EVEX.256.F3.0F.W0 16 /r","V","V","AVX512VL AVX512F","","","","","Move odd index single precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask."
"VMOVSHDUP ymm1, ymm2/m256","VMOVSHDUP ymm2/m256, ymm1","vmovshdup ymm2/m256, ymm1","VEX.256.F3.0F.WIG 16 /r","V","V","AVX","","","","","Move odd index single precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVSHDUP zmm1 {k1}{z}, zmm2/m512","VMOVSHDUP zmm2/m512, zmm1 {k1}{z}","vmovshdup zmm2/m512, zmm1 {k1}{z}","EVEX.512.F3.0F.W0 16 /r","V","V","AVX512F","","","","","Move odd index single precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask."
"VMPSADBW xmm1, xmm2, xmm3/m128, imm8","VMPSADBW imm8, xmm3/m128, xmm2, xmm1","vmpsadbw imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 42 /r ib","V","V","AVX","","","","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8."
"VMPSADBW ymm1, ymm2, ymm3/m256, imm8","VMPSADBW imm8, ymm3/m256, ymm2, ymm1","vmpsadbw imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 42 /r ib","V","V","AVX2","","","","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by imm8."
"VMULPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VMULPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vmulpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 59 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed double precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1."
"VMULPD xmm1, xmm2, xmm3/m128","VMULPD xmm3/m128, xmm2, xmm1","vmulpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 59 /r","V","V","AVX","","","","","Multiply packed double precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VMULPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vmulpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 59 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed double precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1."
"VMULPD ymm1, ymm2, ymm3/m256","VMULPD ymm3/m256, ymm2, ymm1","vmulpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 59 /r","V","V","AVX","","","","","Multiply packed double precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","VMULPD zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","vmulpd zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 59 /r","V","V","AVX512F","","","","","Multiply packed double precision floating-point values in zmm3/m512/m64bcst with zmm2 and store result in zmm1."
"VMULPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VMULPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vmulps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 59 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed single precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1."
"VMULPS xmm1, xmm2, xmm3/m128","VMULPS xmm3/m128, xmm2, xmm1","vmulps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 59 /r","V","V","AVX","","","","","Multiply packed single precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VMULPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vmulps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 59 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed single precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1."
"VMULPS ymm1, ymm2, ymm3/m256","VMULPS ymm3/m256, ymm2, ymm1","vmulps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 59 /r","V","V","AVX","","","","","Multiply packed single precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","VMULPS zmm3/m512/m32bcst {er}, zmm2, zmm1 {k1}{z}","vmulps zmm3/m512/m32bcst {er}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 59 /r","V","V","AVX512F","","","","","Multiply packed single precision floating-point values in zmm3/m512/m32bcst with zmm2 and store result in zmm1."
"VMULSD xmm1 {k1}{z}, xmm2, xmm3/m64 {er}","VMULSD xmm3/m64 {er}, xmm2, xmm1 {k1}{z}","vmulsd xmm3/m64 {er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 59 /r","V","V","AVX512F","","","","","Multiply the low double precision floating-point value in xmm3/m64 by low double precision floating-point value in xmm2."
"VMULSD xmm1, xmm2, xmm3/m64","VMULSD xmm3/m64, xmm2, xmm1","vmulsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 59 /r","V","V","AVX","","","","","Multiply the low double precision floating-point value in xmm3/m64 by low double precision floating-point value in xmm2."
"VMULSS xmm1 {k1}{z}, xmm2, xmm3/m32 {er}","VMULSS xmm3/m32 {er}, xmm2, xmm1 {k1}{z}","vmulss xmm3/m32 {er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 59 /r","V","V","AVX512F","","","","","Multiply the low single precision floating-point value in xmm3/m32 by the low single precision floating-point value in xmm2."
"VMULSS xmm1, xmm2, xmm3/m32","VMULSS xmm3/m32, xmm2, xmm1","vmulss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 59 /r","V","V","AVX","","","","","Multiply the low single precision floating-point value in xmm3/m32 by the low single precision floating-point value in xmm2."
"VORPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VORPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vorpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 56 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical OR of packed double precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VORPD xmm1, xmm2, xmm3/m128","VORPD xmm3/m128, xmm2, xmm1","vorpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F 56 /r","V","V","AVX","","","","","Return the bitwise logical OR of packed double precision floating-point values in xmm2 and xmm3/mem."
"VORPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VORPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vorpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 56 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical OR of packed double precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VORPD ymm1, ymm2, ymm3/m256","VORPD ymm3/m256, ymm2, ymm1","vorpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F 56 /r","V","V","AVX","","","","","Return the bitwise logical OR of packed double precision floating-point values in ymm2 and ymm3/mem."
"VORPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VORPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vorpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 56 /r","V","V","AVX512DQ","","","","","Return the bitwise logical OR of packed double precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VORPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VORPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vorps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 56 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical OR of packed single precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VORPS xmm1, xmm2, xmm3/m128","VORPS xmm3/m128, xmm2, xmm1","vorps xmm3/m128, xmm2, xmm1","VEX.128.0F 56 /r","V","V","AVX","","","","","Return the bitwise logical OR of packed single precision floating-point values in xmm2 and xmm3/mem."
"VORPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VORPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vorps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 56 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical OR of packed single precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VORPS ymm1, ymm2, ymm3/m256","VORPS ymm3/m256, ymm2, ymm1","vorps ymm3/m256, ymm2, ymm1","VEX.256.0F 56 /r","V","V","AVX","","","","","Return the bitwise logical OR of packed single precision floating-point values in ymm2 and ymm3/mem."
"VORPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VORPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vorps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 56 /r","V","V","AVX512DQ","","","","","Return the bitwise logical OR of packed single precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"VPABSB xmm1 {k1}{z}, xmm2/m128","VPABSB xmm2/m128, xmm1 {k1}{z}","vpabsb xmm2/m128, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 1C /r","V","V","AVX512VL AVX512BW","","","","","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSB xmm1, xmm2/m128","VPABSB xmm2/m128, xmm1","vpabsb xmm2/m128, xmm1","VEX.128.66.0F38.WIG 1C /r","V","V","AVX","","","","","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSB ymm1 {k1}{z}, ymm2/m256","VPABSB ymm2/m256, ymm1 {k1}{z}","vpabsb ymm2/m256, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 1C /r","V","V","AVX512VL AVX512BW","","","","","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1."
"VPABSB ymm1, ymm2/m256","VPABSB ymm2/m256, ymm1","vpabsb ymm2/m256, ymm1","VEX.256.66.0F38.WIG 1C /r","V","V","AVX2","","","","","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSB zmm1 {k1}{z}, zmm2/m512","VPABSB zmm2/m512, zmm1 {k1}{z}","vpabsb zmm2/m512, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 1C /r","V","V","AVX512BW","","","","","Compute the absolute value of bytes in zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1."
"VPABSD xmm1, xmm2/m128","VPABSD xmm2/m128, xmm1","vpabsd xmm2/m128, xmm1","VEX.128.66.0F38.WIG 1E /r","V","V","AVX","","","","","Compute the absolute value of 32- bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSD ymm1, ymm2/m256","VPABSD ymm2/m256, ymm1","vpabsd ymm2/m256, ymm1","VEX.256.66.0F38.WIG 1E /r","V","V","AVX2","","","","","Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSW xmm1 {k1}{z}, xmm2/m128","VPABSW xmm2/m128, xmm1 {k1}{z}","vpabsw xmm2/m128, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 1D /r","V","V","AVX512VL AVX512BW","","","","","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSW xmm1, xmm2/m128","VPABSW xmm2/m128, xmm1","vpabsw xmm2/m128, xmm1","VEX.128.66.0F38.WIG 1D /r","V","V","AVX","","","","","Compute the absolute value of 16- bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSW ymm1, ymm2/m256","VPABSW ymm2/m256, ymm1","vpabsw ymm2/m256, ymm1","VEX.256.66.0F38.WIG 1D /r","V","V","AVX2","","","","","Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPACKSSDW xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPACKSSDW xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpackssdw xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 6B /r","V","V","AVX512VL AVX512BW","","","","","Converts packed signed doubleword integers from xmm2 and from xmm3/m128/m32bcst into packed signed word integers in xmm1 using signed saturation under writemask k1."
"VPACKSSDW xmm1, xmm2, xmm3/m128","VPACKSSDW xmm3/m128, xmm2, xmm1","vpackssdw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 6B /r","V","V","AVX","","","","","Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation."
"VPACKSSDW ymm1, ymm2, ymm3/m256","VPACKSSDW ymm3/m256, ymm2, ymm1","vpackssdw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 6B /r","V","V","AVX2","","","","","Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation."
"VPACKSSWB xmm1 {k1}{z}, xmm2, xmm3/m128","VPACKSSWB xmm3/m128, xmm2, xmm1 {k1}{z}","vpacksswb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG 63 /r","V","V","AVX512VL AVX512BW","","","","","Converts packed signed word integers from xmm2 and from xmm3/m128 into packed signed byte integers in xmm1 using signed saturation under writemask k1."
"VPACKSSWB xmm1, xmm2, xmm3/m128","VPACKSSWB xmm3/m128, xmm2, xmm1","vpacksswb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 63 /r","V","V","AVX","","","","","Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation."
"VPACKSSWB ymm1 {k1}{z}, ymm2, ymm3/m256","VPACKSSWB ymm3/m256, ymm2, ymm1 {k1}{z}","vpacksswb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG 63 /r","V","V","AVX512VL AVX512BW","","","","","Converts packed signed word integers from ymm2 and from ymm3/m256 into packed signed byte integers in ymm1 using signed saturation under writemask k1."
"VPACKSSWB ymm1, ymm2, ymm3/m256","VPACKSSWB ymm3/m256, ymm2, ymm1","vpacksswb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 63 /r","V","V","AVX2","","","","","Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation."
"VPACKSSWB zmm1 {k1}{z}, zmm2, zmm3/m512","VPACKSSWB zmm3/m512, zmm2, zmm1 {k1}{z}","vpacksswb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG 63 /r","V","V","AVX512BW","","","","","Converts packed signed word integers from zmm2 and from zmm3/m512 into packed signed byte integers in zmm1 using signed saturation under writemask k1."
"VPACKUSDW xmm1, xmm2, xmm3/m128","VPACKUSDW xmm3/m128, xmm2, xmm1","vpackusdw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38 2B /r","V","V","AVX","","","","","Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation."
"VPACKUSDW xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","VPACKUSDW xmm3/m128/m32bcst, xmm2, xmm1{k1}{z}","vpackusdw xmm3/m128/m32bcst, xmm2, xmm1{k1}{z}","EVEX.128.66.0F38.W0 2B /r","V","V","AVX512VL AVX512BW","","","","","Convert packed signed doubleword integers from xmm2 and packed signed doubleword integers from xmm3/m128/m32bcst into packed unsigned word integers in xmm1 using unsigned saturation under writemask k1."
"VPACKUSDW ymm1, ymm2, ymm3/m256","VPACKUSDW ymm3/m256, ymm2, ymm1","vpackusdw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38 2B /r","V","V","AVX2","","","","","Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m256 into 16 packed unsigned word integers in ymm1 using unsigned saturation."
"VPACKUSDW ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst","VPACKUSDW ymm3/m256/m32bcst, ymm2, ymm1{k1}{z}","vpackusdw ymm3/m256/m32bcst, ymm2, ymm1{k1}{z}","EVEX.256.66.0F38.W0 2B /r","V","V","AVX512VL AVX512BW","","","","","Convert packed signed doubleword integers from ymm2 and packed signed doubleword integers from ymm3/m256/m32bcst into packed unsigned word integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSDW zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","VPACKUSDW zmm3/m512/m32bcst, zmm2, zmm1{k1}{z}","vpackusdw zmm3/m512/m32bcst, zmm2, zmm1{k1}{z}","EVEX.512.66.0F38.W0 2B /r","V","V","AVX512BW","","","","","Convert packed signed doubleword integers from zmm2 and packed signed doubleword integers from zmm3/m512/m32bcst into packed unsigned word integers in zmm1 using unsigned saturation under writemask k1."
"VPACKUSWB xmm1, xmm2, xmm3/m128","VPACKUSWB xmm3/m128, xmm2, xmm1","vpackuswb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 67 /r","V","V","AVX","","","","","Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation."
"VPACKUSWB xmm1{k1}{z}, xmm2, xmm3/m128","VPACKUSWB xmm3/m128, xmm2, xmm1{k1}{z}","vpackuswb xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F.WIG 67 /r","V","V","AVX512VL AVX512BW","","","","","Converts signed word integers from xmm2 and signed word integers from xmm3/m128 into unsigned byte integers in xmm1 using unsigned saturation under writemask k1."
"VPACKUSWB ymm1, ymm2, ymm3/m256","VPACKUSWB ymm3/m256, ymm2, ymm1","vpackuswb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 67 /r","V","V","AVX2","","","","","Converts 16 signed word integers from ymm2 and 16signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation."
"VPACKUSWB ymm1{k1}{z}, ymm2, ymm3/m256","VPACKUSWB ymm3/m256, ymm2, ymm1{k1}{z}","vpackuswb ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F.WIG 67 /r","V","V","AVX512VL AVX512BW","","","","","Converts signed word integers from ymm2 and signed word integers from ymm3/m256 into unsigned byte integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSWB zmm1{k1}{z}, zmm2, zmm3/m512","VPACKUSWB zmm3/m512, zmm2, zmm1{k1}{z}","vpackuswb zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F.WIG 67 /r","V","V","AVX512BW","","","","","Converts signed word integers from zmm2 and signed word integers from zmm3/m512 into unsigned byte integers in zmm1 using unsigned saturation under writemask k1."
"VPADDSB xmm1 {k1}{z}, xmm2, xmm3/m128","VPADDSB xmm3/m128, xmm2, xmm1 {k1}{z}","vpaddsb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG EC /r","V","V","AVX512VL AVX512BW","","","","","Add packed signed byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSB xmm1, xmm2, xmm3/m128","VPADDSB xmm3/m128, xmm2, xmm1","vpaddsb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG EC /r","V","V","AVX","","","","","Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results."
"VPADDSB ymm1 {k1}{z}, ymm2, ymm3/m256","VPADDSB ymm3/m256, ymm2, ymm1 {k1}{z}","vpaddsb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG EC /r","V","V","AVX512VL AVX512BW","","","","","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSB ymm1, ymm2, ymm3/m256","VPADDSB ymm3/m256, ymm2, ymm1","vpaddsb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG EC /r","V","V","AVX2","","","","","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDSB zmm1 {k1}{z}, zmm2, zmm3/m512","VPADDSB zmm3/m512, zmm2, zmm1 {k1}{z}","vpaddsb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG EC /r","V","V","AVX512BW","","","","","Add packed signed byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPADDSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpaddsw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG ED /r","V","V","AVX512VL AVX512BW","","","","","Add packed signed word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSW xmm1, xmm2, xmm3/m128","VPADDSW xmm3/m128, xmm2, xmm1","vpaddsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG ED /r","V","V","AVX","","","","","Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results."
"VPADDSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPADDSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpaddsw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG ED /r","V","V","AVX512VL AVX512BW","","","","","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSW ymm1, ymm2, ymm3/m256","VPADDSW ymm3/m256, ymm2, ymm1","vpaddsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG ED /r","V","V","AVX2","","","","","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPADDSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpaddsw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG ED /r","V","V","AVX512BW","","","","","Add packed signed word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDUSB xmm1 {k1}{z}, xmm2, xmm3/m128","VPADDUSB xmm3/m128, xmm2, xmm1 {k1}{z}","vpaddusb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG DC /r","V","V","AVX512VL AVX512BW","","","","","Add packed unsigned byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSB xmm1, xmm2, xmm3/m128","VPADDUSB xmm3/m128, xmm2, xmm1","vpaddusb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG DC /r","V","V","AVX","","","","","Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results."
"VPADDUSB ymm1 {k1}{z}, ymm2, ymm3/m256","VPADDUSB ymm3/m256, ymm2, ymm1 {k1}{z}","vpaddusb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG DC /r","V","V","AVX512VL AVX512BW","","","","","Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSB ymm1, ymm2, ymm3/m256","VPADDUSB ymm3/m256, ymm2, ymm1","vpaddusb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG DC /r","V","V","AVX2","","","","","Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSB zmm1 {k1}{z}, zmm2, zmm3/m512","VPADDUSB zmm3/m512, zmm2, zmm1 {k1}{z}","vpaddusb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG DC /r","V","V","AVX512BW","","","","","Add packed unsigned byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDUSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPADDUSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpaddusw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG DD /r","V","V","AVX512VL AVX512BW","","","","","Add packed unsigned word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSW xmm1, xmm2, xmm3/m128","VPADDUSW xmm3/m128, xmm2, xmm1","vpaddusw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG DD /r","V","V","AVX","","","","","Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results."
"VPADDUSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPADDUSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpaddusw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG DD /r","V","V","AVX512VL AVX512BW","","","","","Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSW ymm1, ymm2, ymm3/m256","VPADDUSW ymm3/m256, ymm2, ymm1","vpaddusw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG DD /r","V","V","AVX2","","","","","Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPADDUSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpaddusw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG DD /r","V","V","AVX512BW","","","","","Add packed unsigned word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPALIGNR xmm1 {k1}{z}, xmm2, xmm3/m128, imm8","VPALIGNR imm8, xmm3/m128, xmm2, xmm1 {k1}{z}","vpalignr imm8, xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F3A.WIG 0F /r ib","V","V","AVX512VL AVX512BW","","","","","Concatenate xmm2 and xmm3/m128 into a 32- byte intermediate result, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1."
"VPALIGNR xmm1, xmm2, xmm3/m128, imm8","VPALIGNR imm8, xmm3/m128, xmm2, xmm1","vpalignr imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 0F /r ib","V","V","AVX","","","","","Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1."
"VPALIGNR ymm1 {k1}{z}, ymm2, ymm3/m256, imm8","VPALIGNR imm8, ymm3/m256, ymm2, ymm1 {k1}{z}","vpalignr imm8, ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F3A.WIG 0F /r ib","V","V","AVX512VL AVX512BW","","","","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1."
"VPALIGNR ymm1, ymm2, ymm3/m256, imm8","VPALIGNR imm8, ymm3/m256, ymm2, ymm1","vpalignr imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 0F /r ib","V","V","AVX2","","","","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1."
"VPALIGNR zmm1 {k1}{z}, zmm2, zmm3/m512, imm8","VPALIGNR imm8, zmm3/m512, zmm2, zmm1 {k1}{z}","vpalignr imm8, zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F3A.WIG 0F /r ib","V","V","AVX512BW","","","","","Concatenate pairs of 16 bytes in zmm2 and zmm3/m512 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and four 16-byte results are stored in zmm1."
"VPAND xmm1, xmm2, xmm3/m128","VPAND xmm3/m128, xmm2, xmm1","vpand xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG DB /r","V","V","AVX","","","","","Bitwise AND of xmm3/m128 and xmm."
"VPAND ymm1, ymm2, ymm3/m256","VPAND ymm3/m256, ymm2, ymm1","vpand ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG DB /r","V","V","AVX2","","","","","Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1."
"VPANDD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPANDD xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpandd xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 DB /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPANDD ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpandd ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W0 DB /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPANDD zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpandd zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W0 DB /r","V","V","AVX512F","","","","","Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDN xmm1, xmm2, xmm3/m128","VPANDN xmm3/m128, xmm2, xmm1","vpandn xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG DF /r","V","V","AVX","","","","","Bitwise AND NOT of xmm3/m128 and xmm2."
"VPANDN ymm1, ymm2, ymm3/m256","VPANDN ymm3/m256, ymm2, ymm1","vpandn ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG DF /r","V","V","AVX2","","","","","Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1."
"VPANDND xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPANDND xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpandnd xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 DF /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND NOT of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDND ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPANDND ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpandnd ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W0 DF /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND NOT of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDND zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPANDND zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpandnd zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W0 DF /r","V","V","AVX512F","","","","","Bitwise AND NOT of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDNQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPANDNQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpandnq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 DF /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND NOT of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDNQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPANDNQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpandnq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 DF /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND NOT of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDNQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPANDNQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpandnq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 DF /r","V","V","AVX512F","","","","","Bitwise AND NOT of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"VPANDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPANDQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpandq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 DB /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPANDQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpandq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 DB /r","V","V","AVX512VL AVX512F","","","","","Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPANDQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpandq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 DB /r","V","V","AVX512F","","","","","Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"VPAVGB xmm1 {k1}{z}, xmm2, xmm3/m128","VPAVGB xmm3/m128, xmm2, xmm1 {k1}{z}","vpavgb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E0 /r","V","V","AVX512VL AVX512BW","","","","","Average packed unsigned byte integers from xmm2, and xmm3/m128 with rounding and store to xmm1 under writemask k1."
"VPAVGB xmm1, xmm2, xmm3/m128","VPAVGB xmm3/m128, xmm2, xmm1","vpavgb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E0 /r","V","V","AVX","","","","","Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding."
"VPAVGB ymm1 {k1}{z}, ymm2, ymm3/m256","VPAVGB ymm3/m256, ymm2, ymm1 {k1}{z}","vpavgb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E0 /r","V","V","AVX512VL AVX512BW","","","","","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1 under writemask k1."
"VPAVGB ymm1, ymm2, ymm3/m256","VPAVGB ymm3/m256, ymm2, ymm1","vpavgb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E0 /r","V","V","AVX2","","","","","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1."
"VPAVGB zmm1 {k1}{z}, zmm2, zmm3/m512","VPAVGB zmm3/m512, zmm2, zmm1 {k1}{z}","vpavgb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E0 /r","V","V","AVX512BW","","","","","Average packed unsigned byte integers from zmm2, and zmm3/m512 with rounding and store to zmm1 under writemask k1."
"VPAVGW xmm1 {k1}{z}, xmm2, xmm3/m128","VPAVGW xmm3/m128, xmm2, xmm1 {k1}{z}","vpavgw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E3 /r","V","V","AVX512VL AVX512BW","","","","","Average packed unsigned word integers from xmm2, xmm3/m128 with rounding to xmm1 under writemask k1."
"VPAVGW xmm1, xmm2, xmm3/m128","VPAVGW xmm3/m128, xmm2, xmm1","vpavgw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E3 /r","V","V","AVX","","","","","Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding."
"VPAVGW ymm1 {k1}{z}, ymm2, ymm3/m256","VPAVGW ymm3/m256, ymm2, ymm1 {k1}{z}","vpavgw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E3 /r","V","V","AVX512VL AVX512BW","","","","","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1 under writemask k1."
"VPAVGW ymm1, ymm2, ymm3/m256","VPAVGW ymm3/m256, ymm2, ymm1","vpavgw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E3 /r","V","V","AVX2","","","","","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1."
"VPAVGW zmm1 {k1}{z}, zmm2, zmm3/m512","VPAVGW zmm3/m512, zmm2, zmm1 {k1}{z}","vpavgw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E3 /r","V","V","AVX512BW","","","","","Average packed unsigned word integers from zmm2, zmm3/m512 with rounding to zmm1 under writemask k1."
"VPBLENDVB xmm1, xmm2, xmm3/m128, xmm4","VPBLENDVB xmm4, xmm3/m128, xmm2, xmm1","vpblendvb xmm4, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.W0 4C /r /is4","V","V","AVX","","","","","Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1."
"VPBLENDVB ymm1, ymm2, ymm3/m256, ymm4","VPBLENDVB ymm4, ymm3/m256, ymm2, ymm1","vpblendvb ymm4, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.W0 4C /r /is4","V","V","AVX2","","","","","Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1."
"VPBLENDW xmm1, xmm2, xmm3/m128, imm8","VPBLENDW imm8, xmm3/m128, xmm2, xmm1","vpblendw imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 0E /r ib","V","V","AVX","","","","","Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1."
"VPBLENDW ymm1, ymm2, ymm3/m256, imm8","VPBLENDW imm8, ymm3/m256, ymm2, ymm1","vpblendw imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 0E /r ib","V","V","AVX2","","","","","Select words from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1."
"VPCLMULQDQ xmm1, xmm2, xmm3/m128, imm8","VPCLMULQDQ imm8, xmm3/m128, xmm2, xmm1","vpclmulqdq imm8, xmm3/m128, xmm2, xmm1","EVEX.128.66.0F3A.WIG 44 /r /ib","V","V","VPCLMULQDQ AVX512VL","","","Y","","Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The imme- diate is used to determine which quadwords of xmm2 and xmm3/m128 should be used."
"VPCLMULQDQ xmm1, xmm2, xmm3/m128, imm8","VPCLMULQDQ imm8, xmm3/m128, xmm2, xmm1","vpclmulqdq imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F3A.WIG 44 /r ib","V","V","PCLMULQDQ AVX","","","Y","","Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The imme- diate is used to determine which quadwords of xmm2 and xmm3/m128 should be used."
"VPCLMULQDQ ymm1, ymm2, ymm3/m256, imm8","VPCLMULQDQ imm8, ymm3/m256, ymm2, ymm1","vpclmulqdq imm8, ymm3/m256, ymm2, ymm1","EVEX.256.66.0F3A.WIG 44 /r /ib","V","V","VPCLMULQDQ AVX512VL","","","Y","","Carry-less multiplication of one quadword of ymm2 by one quadword of ymm3/m256, stores the 128-bit result in ymm1. The imme- diate is used to determine which quadwords of ymm2 and ymm3/m256 should be used."
"VPCLMULQDQ ymm1, ymm2, ymm3/m256, imm8","VPCLMULQDQ imm8, ymm3/m256, ymm2, ymm1","vpclmulqdq imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F3A.WIG 44 /r /ib","V","V","VPCLMULQDQ","","","Y","","Carry-less multiplication of one quadword of ymm2 by one quadword of ymm3/m256, stores the 128-bit result in ymm1. The imme- diate is used to determine which quadwords of ymm2 and ymm3/m256 should be used."
"VPCLMULQDQ zmm1, zmm2, zmm3/m512, imm8","VPCLMULQDQ imm8, zmm3/m512, zmm2, zmm1","vpclmulqdq imm8, zmm3/m512, zmm2, zmm1","EVEX.512.66.0F3A.WIG 44 /r /ib","V","V","VPCLMULQDQ AVX512F","","","Y","","Carry-less multiplication of one quadword of zmm2 by one quadword of zmm3/m512, stores the 128-bit result in zmm1. The imme- diate is used to determine which quadwords of zmm2 and zmm3/m512 should be used."
"VPCMPEQB k1 {k2}, xmm2, xmm3/m128","VPCMPEQB xmm3/m128, xmm2, k1 {k2}","vpcmpeqb xmm3/m128, xmm2, k1 {k2}","EVEX.128.66.0F.WIG 74 /r","V","V","AVX512VL AVX512BW","","","","","Compare packed bytes in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQB xmm1, xmm2, xmm3/m128","VPCMPEQB xmm3/m128, xmm2, xmm1","vpcmpeqb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 74 /r","V","V","AVX","","","","","Compare packed bytes in xmm3/m128 and xmm2 for equality."
"VPCMPEQB ymm1, ymm2, ymm3/m256","VPCMPEQB ymm3/m256, ymm2, ymm1","vpcmpeqb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 74 /r","V","V","AVX2","","","","","Compare packed bytes in ymm3/m256 and ymm2 for equality."
"VPCMPEQD k1 {k2}, xmm2, xmm3/m128/m32bcst","VPCMPEQD xmm3/m128/m32bcst, xmm2, k1 {k2}","vpcmpeqd xmm3/m128/m32bcst, xmm2, k1 {k2}","EVEX.128.66.0F.W0 76 /r","V","V","AVX512VL AVX512F","","","","","Compare Equal between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2}, ymm2, ymm3/m256/m32bcst","VPCMPEQD ymm3/m256/m32bcst, ymm2, k1 {k2}","vpcmpeqd ymm3/m256/m32bcst, ymm2, k1 {k2}","EVEX.256.66.0F.W0 76 /r","V","V","AVX512VL AVX512F","","","","","Compare Equal between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2}, zmm2, zmm3/m512/m32bcst","VPCMPEQD zmm3/m512/m32bcst, zmm2, k1 {k2}","vpcmpeqd zmm3/m512/m32bcst, zmm2, k1 {k2}","EVEX.512.66.0F.W0 76 /r","V","V","AVX512F","","","","","Compare Equal between int32 vectors in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask k2."
"VPCMPEQD xmm1, xmm2, xmm3/m128","VPCMPEQD xmm3/m128, xmm2, xmm1","vpcmpeqd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 76 /r","V","V","AVX","","","","","Compare packed doublewords in xmm3/m128 and xmm2 for equality."
"VPCMPEQD ymm1, ymm2, ymm3/m256","VPCMPEQD ymm3/m256, ymm2, ymm1","vpcmpeqd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 76 /r","V","V","AVX2","","","","","Compare packed doublewords in ymm3/m256 and ymm2 for equality."
"VPCMPEQQ k1 {k2}, xmm2, xmm3/m128/m64bcst","VPCMPEQQ xmm3/m128/m64bcst, xmm2, k1 {k2}","vpcmpeqq xmm3/m128/m64bcst, xmm2, k1 {k2}","EVEX.128.66.0F38.W1 29 /r","V","V","AVX512VL AVX512F","","","","","Compare Equal between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, ymm2, ymm3/m256/m64bcst","VPCMPEQQ ymm3/m256/m64bcst, ymm2, k1 {k2}","vpcmpeqq ymm3/m256/m64bcst, ymm2, k1 {k2}","EVEX.256.66.0F38.W1 29 /r","V","V","AVX512VL AVX512F","","","","","Compare Equal between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, zmm2, zmm3/m512/m64bcst","VPCMPEQQ zmm3/m512/m64bcst, zmm2, k1 {k2}","vpcmpeqq zmm3/m512/m64bcst, zmm2, k1 {k2}","EVEX.512.66.0F38.W1 29 /r","V","V","AVX512F","","","","","Compare Equal between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ xmm1, xmm2, xmm3/m128","VPCMPEQQ xmm3/m128, xmm2, xmm1","vpcmpeqq xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 29 /r","V","V","AVX","","","","","Compare packed quadwords in xmm3/m128 and xmm2 for equality."
"VPCMPEQQ ymm1, ymm2, ymm3/m256","VPCMPEQQ ymm3/m256, ymm2, ymm1","vpcmpeqq ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 29 /r","V","V","AVX2","","","","","Compare packed quadwords in ymm3/m256 and ymm2 for equality."
"VPCMPEQW xmm1, xmm2, xmm3/m128","VPCMPEQW xmm3/m128, xmm2, xmm1","vpcmpeqw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 75 /r","V","V","AVX","","","","","Compare packed words in xmm3/m128 and xmm2 for equality."
"VPCMPEQW ymm1, ymm2, ymm3/m256","VPCMPEQW ymm3/m256, ymm2, ymm1","vpcmpeqw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 75 /r","V","V","AVX2","","","","","Compare packed words in ymm3/m256 and ymm2 for equality."
"VPCMPESTRI xmm1, xmm2/m128, imm8","VPCMPESTRI imm8, xmm2/m128, xmm1","vpcmpestri imm8, xmm2/m128, xmm1","VEX.128.66.0F3A 61 /r ib","V","V","AVX","","","","","Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX."
"VPCMPESTRM xmm1, xmm2/m128, imm8","VPCMPESTRM imm8, xmm2/m128, xmm1","vpcmpestrm imm8, xmm2/m128, xmm1","VEX.128.66.0F3A 60 /r ib","V","V","AVX","","","","","Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0."
"VPCMPGTB k1 {k2}, xmm2, xmm3/m128","VPCMPGTB xmm3/m128, xmm2, k1 {k2}","vpcmpgtb xmm3/m128, xmm2, k1 {k2}","EVEX.128.66.0F.WIG 64 /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTB k1 {k2}, ymm2, ymm3/m256","VPCMPGTB ymm3/m256, ymm2, k1 {k2}","vpcmpgtb ymm3/m256, ymm2, k1 {k2}","EVEX.256.66.0F.WIG 64 /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTB xmm1, xmm2, xmm3/m128","VPCMPGTB xmm3/m128, xmm2, xmm1","vpcmpgtb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 64 /r","V","V","AVX","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than."
"VPCMPGTB ymm1, ymm2, ymm3/m256","VPCMPGTB ymm3/m256, ymm2, ymm1","vpcmpgtb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 64 /r","V","V","AVX2","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than."
"VPCMPGTD k1 {k2}, xmm2, xmm3/m128/m32bcst","VPCMPGTD xmm3/m128/m32bcst, xmm2, k1 {k2}","vpcmpgtd xmm3/m128/m32bcst, xmm2, k1 {k2}","EVEX.128.66.0F.W0 66 /r","V","V","AVX512VL AVX512F","","","","","Compare Greater between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTD k1 {k2}, ymm2, ymm3/m256/m32bcst","VPCMPGTD ymm3/m256/m32bcst, ymm2, k1 {k2}","vpcmpgtd ymm3/m256/m32bcst, ymm2, k1 {k2}","EVEX.256.66.0F.W0 66 /r","V","V","AVX512VL AVX512F","","","","","Compare Greater between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTD k1 {k2}, zmm2, zmm3/m512/m32bcst","VPCMPGTD zmm3/m512/m32bcst, zmm2, k1 {k2}","vpcmpgtd zmm3/m512/m32bcst, zmm2, k1 {k2}","EVEX.512.66.0F.W0 66 /r","V","V","AVX512F","","","","","Compare Greater between int32 elements in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask. k2."
"VPCMPGTD xmm1, xmm2, xmm3/m128","VPCMPGTD xmm3/m128, xmm2, xmm1","vpcmpgtd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 66 /r","V","V","AVX","","","","","Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than."
"VPCMPGTD ymm1, ymm2, ymm3/m256","VPCMPGTD ymm3/m256, ymm2, ymm1","vpcmpgtd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 66 /r","V","V","AVX2","","","","","Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than."
"VPCMPGTQ k1 {k2}, xmm2, xmm3/m128/m64bcst","VPCMPGTQ xmm3/m128/m64bcst, xmm2, k1 {k2}","vpcmpgtq xmm3/m128/m64bcst, xmm2, k1 {k2}","EVEX.128.66.0F38.W1 37 /r","V","V","AVX512VL AVX512F","","","","","Compare Greater between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, ymm2, ymm3/m256/m64bcst","VPCMPGTQ ymm3/m256/m64bcst, ymm2, k1 {k2}","vpcmpgtq ymm3/m256/m64bcst, ymm2, k1 {k2}","EVEX.256.66.0F38.W1 37 /r","V","V","AVX512VL AVX512F","","","","","Compare Greater between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, zmm2, zmm3/m512/m64bcst","VPCMPGTQ zmm3/m512/m64bcst, zmm2, k1 {k2}","vpcmpgtq zmm3/m512/m64bcst, zmm2, k1 {k2}","EVEX.512.66.0F38.W1 37 /r","V","V","AVX512F","","","","","Compare Greater between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ xmm1, xmm2, xmm3/m128","VPCMPGTQ xmm3/m128, xmm2, xmm1","vpcmpgtq xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 37 /r","V","V","AVX","","","","","Compare packed signed qwords in xmm2 and xmm3/m128 for greater than."
"VPCMPGTQ ymm1, ymm2, ymm3/m256","VPCMPGTQ ymm3/m256, ymm2, ymm1","vpcmpgtq ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 37 /r","V","V","AVX2","","","","","Compare packed signed qwords in ymm2 and ymm3/m256 for greater than."
"VPCMPGTW xmm1, xmm2, xmm3/m128","VPCMPGTW xmm3/m128, xmm2, xmm1","vpcmpgtw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 65 /r","V","V","AVX","","","","","Compare packed signed word integers in xmm2 and xmm3/m128 for greater than."
"VPCMPGTW ymm1, ymm2, ymm3/m256","VPCMPGTW ymm3/m256, ymm2, ymm1","vpcmpgtw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 65 /r","V","V","AVX2","","","","","Compare packed signed word integers in ymm2 and ymm3/m256 for greater than."
"VPCMPISTRI xmm1, xmm2/m128, imm8","VPCMPISTRI imm8, xmm2/m128, xmm1","vpcmpistri imm8, xmm2/m128, xmm1","VEX.128.66.0F3A.WIG 63 /r ib","V","V","AVX","","","","","Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX."
"VPCMPISTRM xmm1, xmm2/m128, imm8","VPCMPISTRM imm8, xmm2/m128, xmm1","vpcmpistrm imm8, xmm2/m128, xmm1","VEX.128.66.0F3A.WIG 62 /r ib","V","V","AVX","","","","","Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0."
"VPEXTRB reg/m8, xmm2, imm8","VPEXTRB imm8, xmm2, reg/m8","vpextrb imm8, xmm2, reg/m8","EVEX.128.66.0F3A.WIG 14 /r ib","V","V","AVX512BW","","","Y","","Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros."
"VPEXTRB reg/m8, xmm2, imm8","VPEXTRB imm8, xmm2, reg/m8","vpextrb imm8, xmm2, reg/m8","VEX.128.66.0F3A.W0 14 /r ib","V","V","AVX","","","Y","","Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros."
"VPEXTRD r/m32, xmm2, imm8","VPEXTRDL imm8, xmm2, r/m32","vpextrdl imm8, xmm2, r/m32","EVEX.128.66.0F3A.W0 16 /r ib","V","V","AVX512DQ","","","Y","32","Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32."
"VPEXTRD r/m32, xmm2, imm8","VPEXTRDL imm8, xmm2, r/m32","vpextrdl imm8, xmm2, r/m32","VEX.128.66.0F3A.W0 16 /r ib","V","V","AVX","","","Y","32","Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32."
"VPEXTRQ r/m64, xmm2, imm8","VPEXTRQQ imm8, xmm2, r/m64","vpextrqq imm8, xmm2, r/m64","EVEX.128.66.0F3A.W1 16 /r ib","N.E.","V","AVX512DQ","","","Y","64","Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64."
"VPEXTRQ r/m64, xmm2, imm8","VPEXTRQQ imm8, xmm2, r/m64","vpextrqq imm8, xmm2, r/m64","VEX.128.66.0F3A.W1 16 /r ib","I","V","AVX","","","Y","64","Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64."
"VPEXTRW reg, xmm1, imm8","VPEXTRW imm8, xmm1, reg","vpextrw imm8, xmm1, reg","EVEX.128.66.0F.WIG C5 /r ib","V","V","AVX512BW","","","Y","","Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg, xmm1, imm8","VPEXTRW imm8, xmm1, reg","vpextrw imm8, xmm1, reg","VEX.128.66.0F.W0 C5 /r ib","V","V","AVX","","","Y","","Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg/m16, xmm2, imm8","VPEXTRW imm8, xmm2, reg/m16","vpextrw imm8, xmm2, reg/m16","EVEX.128.66.0F3A.WIG 15 /r ib","V","V","AVX512BW","","","Y","","Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg/m16, xmm2, imm8","VPEXTRW imm8, xmm2, reg/m16","vpextrw imm8, xmm2, reg/m16","VEX.128.66.0F3A.W0 15 /r ib","V","V","AVX","","","Y","","Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros."
"VPHADDD xmm1, xmm2, xmm3/m128","VPHADDD xmm3/m128, xmm2, xmm1","vphaddd xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 02 /r","V","V","AVX","","","","","Add 32-bit integers horizontally, pack to xmm1."
"VPHADDD ymm1, ymm2, ymm3/m256","VPHADDD ymm3/m256, ymm2, ymm1","vphaddd ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 02 /r","V","V","AVX2","","","","","Add 32-bit signed integers horizontally, pack to ymm1."
"VPHADDSW xmm1, xmm2, xmm3/m128","VPHADDSW xmm3/m128, xmm2, xmm1","vphaddsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 03 /r","V","V","AVX","","","","","Add 16-bit signed integers horizontally, pack saturated integers to xmm1."
"VPHADDSW ymm1, ymm2, ymm3/m256","VPHADDSW ymm3/m256, ymm2, ymm1","vphaddsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 03 /r","V","V","AVX2","","","","","Add 16-bit signed integers horizontally, pack saturated integers to ymm1."
"VPHADDW xmm1, xmm2, xmm3/m128","VPHADDW xmm3/m128, xmm2, xmm1","vphaddw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 01 /r","V","V","AVX","","","","","Add 16-bit integers horizontally, pack to xmm1."
"VPHADDW ymm1, ymm2, ymm3/m256","VPHADDW ymm3/m256, ymm2, ymm1","vphaddw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 01 /r","V","V","AVX2","","","","","Add 16-bit signed integers horizontally, pack to ymm1."
"VPHMINPOSUW xmm1, xmm2/m128","VPHMINPOSUW xmm2/m128, xmm1","vphminposuw xmm2/m128, xmm1","VEX.128.66.0F38.WIG 41 /r","V","V","AVX","","","","","Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second- lowest word of xmm1."
"VPHSUBD xmm1, xmm2, xmm3/m128","VPHSUBD xmm3/m128, xmm2, xmm1","vphsubd xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 06 /r","V","V","AVX","","","","","Subtract 32-bit signed integers horizontally, pack to xmm1."
"VPHSUBD ymm1, ymm2, ymm3/m256","VPHSUBD ymm3/m256, ymm2, ymm1","vphsubd ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 06 /r","V","V","AVX2","","","","","Subtract 32-bit signed integers horizontally, pack to ymm1."
"VPHSUBSW xmm1, xmm2, xmm3/m128","VPHSUBSW xmm3/m128, xmm2, xmm1","vphsubsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 07 /r","V","V","AVX","","","","","Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1."
"VPHSUBSW ymm1, ymm2, ymm3/m256","VPHSUBSW ymm3/m256, ymm2, ymm1","vphsubsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 07 /r","V","V","AVX2","","","","","Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1."
"VPHSUBW xmm1, xmm2, xmm3/m128","VPHSUBW xmm3/m128, xmm2, xmm1","vphsubw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 05 /r","V","V","AVX","","","","","Subtract 16-bit signed integers horizontally, pack to xmm1."
"VPHSUBW ymm1, ymm2, ymm3/m256","VPHSUBW ymm3/m256, ymm2, ymm1","vphsubw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 05 /r","V","V","AVX2","","","","","Subtract 16-bit signed integers horizontally, pack to ymm1."
"VPINSRB xmm1, xmm2, r32/m8, imm8","VPINSRB imm8, r32/m8, xmm2, xmm1","vpinsrb imm8, r32/m8, xmm2, xmm1","EVEX.128.66.0F3A.WIG 20 /r ib","V","V","AVX512BW","","","Y","","Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8."
"VPINSRB xmm1, xmm2, r32/m8, imm8","VPINSRB imm8, r32/m8, xmm2, xmm1","vpinsrb imm8, r32/m8, xmm2, xmm1","VEX.128.66.0F3A.W0 20 /r ib","V","V","AVX","","","Y","","Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8."
"VPINSRD xmm1, xmm2, r/m32, imm8","VPINSRDL imm8, r/m32, xmm2, xmm1","vpinsrdl imm8, r/m32, xmm2, xmm1","EVEX.128.66.0F3A.W0 22 /r ib","V","V","AVX512DQ","","","Y","32","Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8."
"VPINSRD xmm1, xmm2, r/m32, imm8","VPINSRDL imm8, r/m32, xmm2, xmm1","vpinsrdl imm8, r/m32, xmm2, xmm1","VEX.128.66.0F3A.W0 22 /r ib","V","V","AVX","","","Y","32","Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8."
"VPINSRQ xmm1, xmm2, r/m64, imm8","VPINSRQQ imm8, r/m64, xmm2, xmm1","vpinsrqq imm8, r/m64, xmm2, xmm1","EVEX.128.66.0F3A.W1 22 /r ib","N.E.","V","AVX512DQ","","","Y","64","Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8."
"VPINSRQ xmm1, xmm2, r/m64, imm8","VPINSRQQ imm8, r/m64, xmm2, xmm1","vpinsrqq imm8, r/m64, xmm2, xmm1","VEX.128.66.0F3A.W1 22 /r ib","I","V","AVX","","","Y","64","Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8."
"VPINSRW xmm1, xmm2, r32/m16, imm8","VPINSRW imm8, r32/m16, xmm2, xmm1","vpinsrw imm8, r32/m16, xmm2, xmm1","EVEX.128.66.0F.WIG C4 /r ib","V","V","AVX512BW","","","Y","","Insert the word from r32/m16 at the offset indicated by imm8 into the value from xmm2 and store result in xmm1."
"VPINSRW xmm1, xmm2, r32/m16, imm8","VPINSRW imm8, r32/m16, xmm2, xmm1","vpinsrw imm8, r32/m16, xmm2, xmm1","VEX.128.66.0F.W0 C4 /r ib","V","V","AVX","","","Y","","Insert the word from r32/m16 at the offset indicated by imm8 into the value from xmm2 and store result in xmm1."
"VPMADDUBSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPMADDUBSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpmaddubsw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 04 /r","V","V","AVX512VL AVX512BW","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1 under writemask k1."
"VPMADDUBSW xmm1, xmm2, xmm3/m128","VPMADDUBSW xmm3/m128, xmm2, xmm1","vpmaddubsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 04 /r","V","V","AVX","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1."
"VPMADDUBSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPMADDUBSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpmaddubsw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 04 /r","V","V","AVX512VL AVX512BW","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1 under writemask k1."
"VPMADDUBSW ymm1, ymm2, ymm3/m256","VPMADDUBSW ymm3/m256, ymm2, ymm1","vpmaddubsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 04 /r","V","V","AVX2","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1."
"VPMADDUBSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPMADDUBSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpmaddubsw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 04 /r","V","V","AVX512BW","","","","","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to zmm1 under writemask k1."
"VPMADDWD xmm1 {k1}{z}, xmm2, xmm3/m128","VPMADDWD xmm3/m128, xmm2, xmm1 {k1}{z}","vpmaddwd xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG F5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1 under writemask k1."
"VPMADDWD xmm1, xmm2, xmm3/m128","VPMADDWD xmm3/m128, xmm2, xmm1","vpmaddwd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F5 /r","V","V","AVX","","","","","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1."
"VPMADDWD ymm1 {k1}{z}, ymm2, ymm3/m256","VPMADDWD ymm3/m256, ymm2, ymm1 {k1}{z}","vpmaddwd ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG F5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1 under writemask k1."
"VPMADDWD ymm1, ymm2, ymm3/m256","VPMADDWD ymm3/m256, ymm2, ymm1","vpmaddwd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG F5 /r","V","V","AVX2","","","","","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1."
"VPMADDWD zmm1 {k1}{z}, zmm2, zmm3/m512","VPMADDWD zmm3/m512, zmm2, zmm1 {k1}{z}","vpmaddwd zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG F5 /r","V","V","AVX512BW","","","","","Multiply the packed word integers in zmm2 by the packed word integers in zmm3/m512, add adjacent doubleword results, and store in zmm1 under writemask k1."
"VPMAXSB xmm1, xmm2, xmm3/m128","VPMAXSB xmm3/m128, xmm2, xmm1","vpmaxsb xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 3C /r","V","V","AVX","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSB xmm1{k1}{z}, xmm2, xmm3/m128","VPMAXSB xmm3/m128, xmm2, xmm1{k1}{z}","vpmaxsb xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F38.WIG 3C /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSB ymm1, ymm2, ymm3/m256","VPMAXSB ymm3/m256, ymm2, ymm1","vpmaxsb ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 3C /r","V","V","AVX2","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSB ymm1{k1}{z}, ymm2, ymm3/m256","VPMAXSB ymm3/m256, ymm2, ymm1{k1}{z}","vpmaxsb ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F38.WIG 3C /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSB zmm1{k1}{z}, zmm2, zmm3/m512","VPMAXSB zmm3/m512, zmm2, zmm1{k1}{z}","vpmaxsb zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F38.WIG 3C /r","V","V","AVX512BW","","","","","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXSD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPMAXSD xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpmaxsd xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.W0 3D /r","V","V","AVX512VL AVX512F","","","","","Compare packed signed dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSD xmm1, xmm2, xmm3/m128","VPMAXSD xmm3/m128, xmm2, xmm1","vpmaxsd xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 3D /r","V","V","AVX","","","","","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPMAXSD ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpmaxsd ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.W0 3D /r","V","V","AVX512VL AVX512F","","","","","Compare packed signed dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 using writemask k1."
"VPMAXSD ymm1, ymm2, ymm3/m256","VPMAXSD ymm3/m256, ymm2, ymm1","vpmaxsd ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 3D /r","V","V","AVX2","","","","","Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPMAXSD zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpmaxsd zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.W0 3D /r","V","V","AVX512F","","","","","Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 using writemask k1."
"VPMAXSQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPMAXSQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpmaxsq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.W1 3D /r","V","V","AVX512VL AVX512F","","","","","Compare packed signed qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 using writemask k1."
"VPMAXSQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPMAXSQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpmaxsq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.W1 3D /r","V","V","AVX512VL AVX512F","","","","","Compare packed signed qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 using writemask k1."
"VPMAXSQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPMAXSQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpmaxsq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.W1 3D /r","V","V","AVX512F","","","","","Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 using writemask k1."
"VPMAXSW xmm1, xmm2, xmm3/m128","VPMAXSW xmm3/m128, xmm2, xmm1","vpmaxsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG EE /r","V","V","AVX","","","","","Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1."
"VPMAXSW xmm1{k1}{z}, xmm2, xmm3/m128","VPMAXSW xmm3/m128, xmm2, xmm1{k1}{z}","vpmaxsw xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F.WIG EE /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSW ymm1, ymm2, ymm3/m256","VPMAXSW ymm3/m256, ymm2, ymm1","vpmaxsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG EE /r","V","V","AVX2","","","","","Compare packed signed word integers in ymm3/m256 and ymm2 and store packed maximum values in ymm1."
"VPMAXSW ymm1{k1}{z}, ymm2, ymm3/m256","VPMAXSW ymm3/m256, ymm2, ymm1{k1}{z}","vpmaxsw ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F.WIG EE /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSW zmm1{k1}{z}, zmm2, zmm3/m512","VPMAXSW zmm3/m512, zmm2, zmm1{k1}{z}","vpmaxsw zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F.WIG EE /r","V","V","AVX512BW","","","","","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXUB xmm1, xmm2, xmm3/m128","VPMAXUB xmm3/m128, xmm2, xmm1","vpmaxub xmm3/m128, xmm2, xmm1","VEX.128.66.0F DE /r","V","V","AVX","","","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXUB xmm1{k1}{z}, xmm2, xmm3/m128","VPMAXUB xmm3/m128, xmm2, xmm1{k1}{z}","vpmaxub xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F.WIG DE /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUB ymm1, ymm2, ymm3/m256","VPMAXUB ymm3/m256, ymm2, ymm1","vpmaxub ymm3/m256, ymm2, ymm1","VEX.256.66.0F DE /r","V","V","AVX2","","","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXUB ymm1{k1}{z}, ymm2, ymm3/m256","VPMAXUB ymm3/m256, ymm2, ymm1{k1}{z}","vpmaxub ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F.WIG DE /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUB zmm1{k1}{z}, zmm2, zmm3/m512","VPMAXUB zmm3/m512, zmm2, zmm1{k1}{z}","vpmaxub zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F.WIG DE /r","V","V","AVX512BW","","","","","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXUW xmm1, xmm2, xmm3/m128","VPMAXUW xmm3/m128, xmm2, xmm1","vpmaxuw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38 3E /r","V","V","AVX","","","","","Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1."
"VPMAXUW xmm1{k1}{z}, xmm2, xmm3/m128","VPMAXUW xmm3/m128, xmm2, xmm1{k1}{z}","vpmaxuw xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F38.WIG 3E /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUW ymm1, ymm2, ymm3/m256","VPMAXUW ymm3/m256, ymm2, ymm1","vpmaxuw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38 3E /r","V","V","AVX2","","","","","Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1."
"VPMAXUW ymm1{k1}{z}, ymm2, ymm3/m256","VPMAXUW ymm3/m256, ymm2, ymm1{k1}{z}","vpmaxuw ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F38.WIG 3E /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUW zmm1{k1}{z}, zmm2, zmm3/m512","VPMAXUW zmm3/m512, zmm2, zmm1{k1}{z}","vpmaxuw zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F38.WIG 3E /r","V","V","AVX512BW","","","","","Compare packed unsigned word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMINSB xmm1, xmm2, xmm3/m128","VPMINSB xmm3/m128, xmm2, xmm1","vpminsb xmm3/m128, xmm2, xmm1","VEX.128.66.0F38 38 /r","V","V","AVX","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINSB xmm1{k1}{z}, xmm2, xmm3/m128","VPMINSB xmm3/m128, xmm2, xmm1{k1}{z}","vpminsb xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F38.WIG 38 /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSB ymm1, ymm2, ymm3/m256","VPMINSB ymm3/m256, ymm2, ymm1","vpminsb ymm3/m256, ymm2, ymm1","VEX.256.66.0F38 38 /r","V","V","AVX2","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINSB ymm1{k1}{z}, ymm2, ymm3/m256","VPMINSB ymm3/m256, ymm2, ymm1{k1}{z}","vpminsb ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F38.WIG 38 /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSB zmm1{k1}{z}, zmm2, zmm3/m512","VPMINSB zmm3/m512, zmm2, zmm1{k1}{z}","vpminsb zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F38.WIG 38 /r","V","V","AVX512BW","","","","","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINSW xmm1, xmm2, xmm3/m128","VPMINSW xmm3/m128, xmm2, xmm1","vpminsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F EA /r","V","V","AVX","","","","","Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMINSW xmm1{k1}{z}, xmm2, xmm3/m128","VPMINSW xmm3/m128, xmm2, xmm1{k1}{z}","vpminsw xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F.WIG EA /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSW ymm1, ymm2, ymm3/m256","VPMINSW ymm3/m256, ymm2, ymm1","vpminsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F EA /r","V","V","AVX2","","","","","Compare packed signed word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMINSW ymm1{k1}{z}, ymm2, ymm3/m256","VPMINSW ymm3/m256, ymm2, ymm1{k1}{z}","vpminsw ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F.WIG EA /r","V","V","AVX512VL AVX512BW","","","","","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSW zmm1{k1}{z}, zmm2, zmm3/m512","VPMINSW zmm3/m512, zmm2, zmm1{k1}{z}","vpminsw zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F.WIG EA /r","V","V","AVX512BW","","","","","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINUB xmm1 {k1}{z}, xmm2, xmm3/m128","VPMINUB xmm3/m128, xmm2, xmm1 {k1}{z}","vpminub xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F DA /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINUB xmm1, xmm2, xmm3/m128","VPMINUB xmm3/m128, xmm2, xmm1","vpminub xmm3/m128, xmm2, xmm1","VEX.128.66.0F DA /r","V","V","AVX","","","","","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUB ymm1 {k1}{z}, ymm2, ymm3/m256","VPMINUB ymm3/m256, ymm2, ymm1 {k1}{z}","vpminub ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F DA /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINUB ymm1, ymm2, ymm3/m256","VPMINUB ymm3/m256, ymm2, ymm1","vpminub ymm3/m256, ymm2, ymm1","VEX.256.66.0F DA /r","V","V","AVX2","","","","","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUB zmm1 {k1}{z}, zmm2, zmm3/m512","VPMINUB zmm3/m512, zmm2, zmm1 {k1}{z}","vpminub zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F DA /r","V","V","AVX512BW","","","","","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINUD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPMINUD xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpminud xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.W0 3B /r","V","V","AVX512VL AVX512F","","","","","Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUD xmm1, xmm2, xmm3/m128","VPMINUD xmm3/m128, xmm2, xmm1","vpminud xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 3B /r","V","V","AVX","","","","","Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPMINUD ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpminud ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.W0 3B /r","V","V","AVX512VL AVX512F","","","","","Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUD ymm1, ymm2, ymm3/m256","VPMINUD ymm3/m256, ymm2, ymm1","vpminud ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 3B /r","V","V","AVX2","","","","","Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPMINUD zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpminud zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.W0 3B /r","V","V","AVX512F","","","","","Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINUQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPMINUQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpminuq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.W1 3B /r","V","V","AVX512VL AVX512F","","","","","Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPMINUQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpminuq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.W1 3B /r","V","V","AVX512VL AVX512F","","","","","Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPMINUQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpminuq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.W1 3B /r","V","V","AVX512F","","","","","Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINUW xmm1, xmm2, xmm3/m128","VPMINUW xmm3/m128, xmm2, xmm1","vpminuw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38 3A /r","V","V","AVX","","","","","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMINUW xmm1{k1}{z}, xmm2, xmm3/m128","VPMINUW xmm3/m128, xmm2, xmm1{k1}{z}","vpminuw xmm3/m128, xmm2, xmm1{k1}{z}","EVEX.128.66.0F38 3A /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1 under writemask k1."
"VPMINUW ymm1, ymm2, ymm3/m256","VPMINUW ymm3/m256, ymm2, ymm1","vpminuw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38 3A /r","V","V","AVX2","","","","","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMINUW ymm1{k1}{z}, ymm2, ymm3/m256","VPMINUW ymm3/m256, ymm2, ymm1{k1}{z}","vpminuw ymm3/m256, ymm2, ymm1{k1}{z}","EVEX.256.66.0F38 3A /r","V","V","AVX512VL AVX512BW","","","","","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1 under writemask k1."
"VPMINUW zmm1{k1}{z}, zmm2, zmm3/m512","VPMINUW zmm3/m512, zmm2, zmm1{k1}{z}","vpminuw zmm3/m512, zmm2, zmm1{k1}{z}","EVEX.512.66.0F38 3A /r","V","V","AVX512BW","","","","","Compare packed unsigned word integers in zmm3/m512 and zmm2 and return packed minimum values in zmm1 under writemask k1."
"VPMOVMSKB reg, xmm1","VPMOVMSKB xmm1, reg","vpmovmskb xmm1, reg","VEX.128.66.0F.WIG D7 /r","V","V","AVX","","","","","Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros."
"VPMOVMSKB reg, ymm1","VPMOVMSKB ymm1, reg","vpmovmskb ymm1, reg","VEX.256.66.0F.WIG D7 /r","V","V","AVX2","","","","","Move a 32-bit mask of ymm1 to reg. The upper bits of r64 are filled with zeros."
"VPMOVSXBD xmm1 {k1}{z}, xmm2/m32","VPMOVSXBD xmm2/m32, xmm1 {k1}{z}","vpmovsxbd xmm2/m32, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 21 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVSXBD xmm1, xmm2/m32","VPMOVSXBD xmm2/m32, xmm1","vpmovsxbd xmm2/m32, xmm1","VEX.128.66.0F38.WIG 21 /r","V","V","AVX","","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVSXBD ymm1 {k1}{z}, xmm2/m64","VPMOVSXBD xmm2/m64, ymm1 {k1}{z}","vpmovsxbd xmm2/m64, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 21 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBD ymm1, xmm2/m64","VPMOVSXBD xmm2/m64, ymm1","vpmovsxbd xmm2/m64, ymm1","VEX.256.66.0F38.WIG 21 /r","V","V","AVX2","","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVSXBD zmm1 {k1}{z}, xmm2/m128","VPMOVSXBD xmm2/m128, zmm1 {k1}{z}","vpmovsxbd xmm2/m128, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 21 /r","V","V","AVX512F","","","","","Sign extend 16 packed 8-bit integers in the low 16 bytes of xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXBQ xmm1 {k1}{z}, xmm2/m16","VPMOVSXBQ xmm2/m16, xmm1 {k1}{z}","vpmovsxbq xmm2/m16, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 22 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXBQ xmm1, xmm2/m16","VPMOVSXBQ xmm2/m16, xmm1","vpmovsxbq xmm2/m16, xmm1","VEX.128.66.0F38.WIG 22 /r","V","V","AVX","","","","","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVSXBQ ymm1 {k1}{z}, xmm2/m32","VPMOVSXBQ xmm2/m32, ymm1 {k1}{z}","vpmovsxbq xmm2/m32, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 22 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBQ ymm1, xmm2/m32","VPMOVSXBQ xmm2/m32, ymm1","vpmovsxbq xmm2/m32, ymm1","VEX.256.66.0F38.WIG 22 /r","V","V","AVX2","","","","","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVSXBQ zmm1 {k1}{z}, xmm2/m64","VPMOVSXBQ xmm2/m64, zmm1 {k1}{z}","vpmovsxbq xmm2/m64, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 22 /r","V","V","AVX512F","","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVSXBW xmm1 {k1}{z}, xmm2/m64","VPMOVSXBW xmm2/m64, xmm1 {k1}{z}","vpmovsxbw xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 20 /r","V","V","AVX512VL AVX512BW","","","","","Sign extend 8 packed 8-bit integers in xmm2/m64 to 8 packed 16-bit integers in zmm1."
"VPMOVSXBW xmm1, xmm2/m64","VPMOVSXBW xmm2/m64, xmm1","vpmovsxbw xmm2/m64, xmm1","VEX.128.66.0F38.WIG 20 /r","V","V","AVX","","","","","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVSXBW ymm1 {k1}{z}, xmm2/m128","VPMOVSXBW xmm2/m128, ymm1 {k1}{z}","vpmovsxbw xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 20 /r","V","V","AVX512VL AVX512BW","","","","","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXBW ymm1, xmm2/m128","VPMOVSXBW xmm2/m128, ymm1","vpmovsxbw xmm2/m128, ymm1","VEX.256.66.0F38.WIG 20 /r","V","V","AVX2","","","","","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXBW zmm1 {k1}{z}, ymm2/m256","VPMOVSXBW ymm2/m256, zmm1 {k1}{z}","vpmovsxbw ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 20 /r","V","V","AVX512BW","","","","","Sign extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMOVSXDQ xmm1 {k1}{z}, xmm2/m64","VPMOVSXDQ xmm2/m64, xmm1 {k1}{z}","vpmovsxdq xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38.W0 25 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ xmm1, xmm2/m64","VPMOVSXDQ xmm2/m64, xmm1","vpmovsxdq xmm2/m64, xmm1","VEX.128.66.0F38.WIG 25 /r","V","V","AVX","","","","","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVSXDQ ymm1 {k1}{z}, xmm2/m128","VPMOVSXDQ xmm2/m128, ymm1 {k1}{z}","vpmovsxdq xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.W0 25 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ ymm1, xmm2/m128","VPMOVSXDQ xmm2/m128, ymm1","vpmovsxdq xmm2/m128, ymm1","VEX.256.66.0F38.WIG 25 /r","V","V","AVX2","","","","","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1."
"VPMOVSXDQ zmm1 {k1}{z}, ymm2/m256","VPMOVSXDQ ymm2/m256, zmm1 {k1}{z}","vpmovsxdq ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.W0 25 /r","V","V","AVX512F","","","","","Sign extend 8 packed 32-bit integers in the low 32 bytes of ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXWD xmm1 {k1}{z}, xmm2/m64","VPMOVSXWD xmm2/m64, xmm1 {k1}{z}","vpmovsxwd xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 23 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of ymm2/mem to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWD xmm1, xmm2/m64","VPMOVSXWD xmm2/m64, xmm1","vpmovsxwd xmm2/m64, xmm1","VEX.128.66.0F38.WIG 23 /r","V","V","AVX","","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVSXWD ymm1 {k1}{z}, xmm2/m128","VPMOVSXWD xmm2/m128, ymm1 {k1}{z}","vpmovsxwd xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 23 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 8 packed 16-bit integers in the low 16 bytes of ymm2/m128 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWD ymm1, xmm2/m128","VPMOVSXWD xmm2/m128, ymm1","vpmovsxwd xmm2/m128, ymm1","VEX.256.66.0F38.WIG 23 /r","V","V","AVX2","","","","","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVSXWD zmm1 {k1}{z}, ymm2/m256","VPMOVSXWD ymm2/m256, zmm1 {k1}{z}","vpmovsxwd ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 23 /r","V","V","AVX512F","","","","","Sign extend 16 packed 16-bit integers in the low 32 bytes of ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXWQ xmm1 {k1}{z}, xmm2/m32","VPMOVSXWQ xmm2/m32, xmm1 {k1}{z}","vpmovsxwq xmm2/m32, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 24 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWQ xmm1, xmm2/m32","VPMOVSXWQ xmm2/m32, xmm1","vpmovsxwq xmm2/m32, xmm1","VEX.128.66.0F38.WIG 24 /r","V","V","AVX","","","","","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMOVSXWQ ymm1 {k1}{z}, xmm2/m64","VPMOVSXWQ xmm2/m64, ymm1 {k1}{z}","vpmovsxwq xmm2/m64, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 24 /r","V","V","AVX512VL AVX512F","","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWQ ymm1, xmm2/m64","VPMOVSXWQ xmm2/m64, ymm1","vpmovsxwq xmm2/m64, ymm1","VEX.256.66.0F38.WIG 24 /r","V","V","AVX2","","","","","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1."
"VPMOVSXWQ zmm1 {k1}{z}, xmm2/m128","VPMOVSXWQ xmm2/m128, zmm1 {k1}{z}","vpmovsxwq xmm2/m128, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 24 /r","V","V","AVX512F","","","","","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVZXBD xmm1 {k1}{z}, xmm2/m32","VPMOVZXBD xmm2/m32, xmm1 {k1}{z}","vpmovzxbd xmm2/m32, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 31 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBD xmm1, xmm2/m32","VPMOVZXBD xmm2/m32, xmm1","vpmovzxbd xmm2/m32, xmm1","VEX.128.66.0F38.WIG 31 /r","V","V","AVX","","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVZXBD ymm1 {k1}{z}, xmm2/m64","VPMOVZXBD xmm2/m64, ymm1 {k1}{z}","vpmovzxbd xmm2/m64, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 31 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBD ymm1, xmm2/m64","VPMOVZXBD xmm2/m64, ymm1","vpmovzxbd xmm2/m64, ymm1","VEX.256.66.0F38.WIG 31 /r","V","V","AVX2","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVZXBD zmm1 {k1}{z}, xmm2/m128","VPMOVZXBD xmm2/m128, zmm1 {k1}{z}","vpmovzxbd xmm2/m128, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 31 /r","V","V","AVX512F","","","","","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXBQ xmm1 {k1}{z}, xmm2/m16","VPMOVZXBQ xmm2/m16, xmm1 {k1}{z}","vpmovzxbq xmm2/m16, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 32 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBQ xmm1, xmm2/m16","VPMOVZXBQ xmm2/m16, xmm1","vpmovzxbq xmm2/m16, xmm1","VEX.128.66.0F38.WIG 32 /r","V","V","AVX","","","","","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVZXBQ ymm1 {k1}{z}, xmm2/m32","VPMOVZXBQ xmm2/m32, ymm1 {k1}{z}","vpmovzxbq xmm2/m32, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 32 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBQ ymm1, xmm2/m32","VPMOVZXBQ xmm2/m32, ymm1","vpmovzxbq xmm2/m32, ymm1","VEX.256.66.0F38.WIG 32 /r","V","V","AVX2","","","","","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVZXBQ zmm1 {k1}{z}, xmm2/m64","VPMOVZXBQ xmm2/m64, zmm1 {k1}{z}","vpmovzxbq xmm2/m64, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 32 /r","V","V","AVX512F","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVZXBW xmm1 {k1}{z}, xmm2/m64","VPMOVZXBW xmm2/m64, xmm1 {k1}{z}","vpmovzxbw xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38 30.WIG /r","V","V","AVX512VL AVX512BW","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXBW xmm1, xmm2/m64","VPMOVZXBW xmm2/m64, xmm1","vpmovzxbw xmm2/m64, xmm1","VEX.128.66.0F38.WIG 30 /r","V","V","AVX","","","","","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXBW ymm1 {k1}{z}, xmm2/m128","VPMOVZXBW xmm2/m128, ymm1 {k1}{z}","vpmovzxbw xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 30 /r","V","V","AVX512VL AVX512BW","","","","","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXBW ymm1, xmm2/m128","VPMOVZXBW xmm2/m128, ymm1","vpmovzxbw xmm2/m128, ymm1","VEX.256.66.0F38.WIG 30 /r","V","V","AVX2","","","","","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXBW zmm1 {k1}{z}, ymm2/m256","VPMOVZXBW ymm2/m256, zmm1 {k1}{z}","vpmovzxbw ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 30 /r","V","V","AVX512BW","","","","","Zero extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMOVZXDQ xmm1 {k1}{z}, xmm2/m64","VPMOVZXDQ xmm2/m64, xmm1 {k1}{z}","vpmovzxdq xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38.W0 35 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVZXDQ xmm1, xmm2/m64","VPMOVZXDQ xmm2/m64, xmm1","vpmovzxdq xmm2/m64, xmm1","VEX.128.66.0F 38.WIG 35 /r","V","V","AVX","","","","","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVZXDQ ymm1 {k1}{z}, xmm2/m128","VPMOVZXDQ xmm2/m128, ymm1 {k1}{z}","vpmovzxdq xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.W0 35 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVZXDQ ymm1, xmm2/m128","VPMOVZXDQ xmm2/m128, ymm1","vpmovzxdq xmm2/m128, ymm1","VEX.256.66.0F38.WIG 35 /r","V","V","AVX2","","","","","Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in ymm1."
"VPMOVZXDQ zmm1 {k1}{z}, ymm2/m256","VPMOVZXDQ ymm2/m256, zmm1 {k1}{z}","vpmovzxdq ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.W0 35 /r","V","V","AVX512F","","","","","Zero extend 8 packed 32-bit integers in ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVZXWD xmm1 {k1}{z}, xmm2/m64","VPMOVZXWD xmm2/m64, xmm1 {k1}{z}","vpmovzxwd xmm2/m64, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 33 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWD xmm1, xmm2/m64","VPMOVZXWD xmm2/m64, xmm1","vpmovzxwd xmm2/m64, xmm1","VEX.128.66.0F38.WIG 33 /r","V","V","AVX","","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVZXWD ymm1 {k1}{z}, xmm2/m128","VPMOVZXWD xmm2/m128, ymm1 {k1}{z}","vpmovzxwd xmm2/m128, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 33 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWD ymm1, xmm2/m128","VPMOVZXWD xmm2/m128, ymm1","vpmovzxwd xmm2/m128, ymm1","VEX.256.66.0F38.WIG 33 /r","V","V","AVX2","","","","","Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVZXWD zmm1 {k1}{z}, ymm2/m256","VPMOVZXWD ymm2/m256, zmm1 {k1}{z}","vpmovzxwd ymm2/m256, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 33 /r","V","V","AVX512F","","","","","Zero extend 16 packed 16-bit integers in ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWQ xmm1 {k1}{z}, xmm2/m32","VPMOVZXWQ xmm2/m32, xmm1 {k1}{z}","vpmovzxwq xmm2/m32, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 34 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWQ xmm1, xmm2/m32","VPMOVZXWQ xmm2/m32, xmm1","vpmovzxwq xmm2/m32, xmm1","VEX.128.66.0F38.WIG 34 /r","V","V","AVX","","","","","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMOVZXWQ ymm1 {k1}{z}, xmm2/m64","VPMOVZXWQ xmm2/m64, ymm1 {k1}{z}","vpmovzxwq xmm2/m64, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 34 /r","V","V","AVX512VL AVX512F","","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXWQ ymm1, xmm2/m64","VPMOVZXWQ xmm2/m64, ymm1","vpmovzxwq xmm2/m64, ymm1","VEX.256.66.0F38.WIG 34 /r","V","V","AVX2","","","","","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1."
"VPMOVZXWQ zmm1 {k1}{z}, xmm2/m128","VPMOVZXWQ xmm2/m128, zmm1 {k1}{z}","vpmovzxwq xmm2/m128, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 34 /r","V","V","AVX512F","","","","","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMULDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPMULDQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpmuldq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.W1 28 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 using writemask k1."
"VPMULDQ xmm1, xmm2, xmm3/m128","VPMULDQ xmm3/m128, xmm2, xmm1","vpmuldq xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 28 /r","V","V","AVX","","","","","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPMULDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPMULDQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpmuldq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.W1 28 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 using writemask k1."
"VPMULDQ ymm1, ymm2, ymm3/m256","VPMULDQ ymm3/m256, ymm2, ymm1","vpmuldq ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 28 /r","V","V","AVX2","","","","","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPMULDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPMULDQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpmuldq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.W1 28 /r","V","V","AVX512F","","","","","Multiply packed signed doubleword integers in zmm2 by packed signed doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 using writemask k1."
"VPMULHRSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPMULHRSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpmulhrsw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 0B /r","V","V","AVX512VL AVX512BW","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1 under writemask k1."
"VPMULHRSW xmm1, xmm2, xmm3/m128","VPMULHRSW xmm3/m128, xmm2, xmm1","vpmulhrsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 0B /r","V","V","AVX","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1."
"VPMULHRSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPMULHRSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpmulhrsw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 0B /r","V","V","AVX512VL AVX512BW","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1 under writemask k1."
"VPMULHRSW ymm1, ymm2, ymm3/m256","VPMULHRSW ymm3/m256, ymm2, ymm1","vpmulhrsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 0B /r","V","V","AVX2","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1."
"VPMULHRSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPMULHRSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpmulhrsw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 0B /r","V","V","AVX512BW","","","","","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to zmm1 under writemask k1."
"VPMULHUW xmm1 {k1}{z}, xmm2, xmm3/m128","VPMULHUW xmm3/m128, xmm2, xmm1 {k1}{z}","vpmulhuw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E4 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHUW xmm1, xmm2, xmm3/m128","VPMULHUW xmm3/m128, xmm2, xmm1","vpmulhuw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E4 /r","V","V","AVX","","","","","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULHUW ymm1 {k1}{z}, ymm2, ymm3/m256","VPMULHUW ymm3/m256, ymm2, ymm1 {k1}{z}","vpmulhuw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E4 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHUW ymm1, ymm2, ymm3/m256","VPMULHUW ymm3/m256, ymm2, ymm1","vpmulhuw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E4 /r","V","V","AVX2","","","","","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULHUW zmm1 {k1}{z}, zmm2, zmm3/m512","VPMULHUW zmm3/m512, zmm2, zmm1 {k1}{z}","vpmulhuw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E4 /r","V","V","AVX512BW","","","","","Multiply the packed unsigned word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"VPMULHW xmm1 {k1}{z}, xmm2, xmm3/m128","VPMULHW xmm3/m128, xmm2, xmm1 {k1}{z}","vpmulhw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHW xmm1, xmm2, xmm3/m128","VPMULHW xmm3/m128, xmm2, xmm1","vpmulhw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E5 /r","V","V","AVX","","","","","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULHW ymm1 {k1}{z}, ymm2, ymm3/m256","VPMULHW ymm3/m256, ymm2, ymm1 {k1}{z}","vpmulhw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHW ymm1, ymm2, ymm3/m256","VPMULHW ymm3/m256, ymm2, ymm1","vpmulhw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E5 /r","V","V","AVX2","","","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULHW zmm1 {k1}{z}, zmm2, zmm3/m512","VPMULHW zmm3/m512, zmm2, zmm1 {k1}{z}","vpmulhw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E5 /r","V","V","AVX512BW","","","","","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"VPMULLW xmm1 {k1}{z}, xmm2, xmm3/m128","VPMULLW xmm3/m128, xmm2, xmm1 {k1}{z}","vpmullw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG D5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the low 16 bits of the results in xmm1 under writemask k1."
"VPMULLW xmm1, xmm2, xmm3/m128","VPMULLW xmm3/m128, xmm2, xmm1","vpmullw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D5 /r","V","V","AVX","","","","","Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1."
"VPMULLW ymm1 {k1}{z}, ymm2, ymm3/m256","VPMULLW ymm3/m256, ymm2, ymm1 {k1}{z}","vpmullw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG D5 /r","V","V","AVX512VL AVX512BW","","","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1 under writemask k1."
"VPMULLW ymm1, ymm2, ymm3/m256","VPMULLW ymm3/m256, ymm2, ymm1","vpmullw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG D5 /r","V","V","AVX2","","","","","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1."
"VPMULLW zmm1 {k1}{z}, zmm2, zmm3/m512","VPMULLW zmm3/m512, zmm2, zmm1 {k1}{z}","vpmullw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG D5 /r","V","V","AVX512BW","","","","","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the low 16 bits of the results in zmm1 under writemask k1."
"VPMULUDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPMULUDQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpmuludq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 F4 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 under writemask k1."
"VPMULUDQ xmm1, xmm2, xmm3/m128","VPMULUDQ xmm3/m128, xmm2, xmm1","vpmuludq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F4 /r","V","V","AVX","","","","","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPMULUDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPMULUDQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpmuludq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 F4 /r","V","V","AVX512VL AVX512F","","","","","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 under writemask k1."
"VPMULUDQ ymm1, ymm2, ymm3/m256","VPMULUDQ ymm3/m256, ymm2, ymm1","vpmuludq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG F4 /r","V","V","AVX2","","","","","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPMULUDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPMULUDQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpmuludq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 F4 /r","V","V","AVX512F","","","","","Multiply packed unsigned doubleword integers in zmm2 by packed unsigned doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 under writemask k1."
"VPOR xmm1, xmm2, xmm3/m128","VPOR xmm3/m128, xmm2, xmm1","vpor xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG EB /r","V","V","AVX","","","","","Bitwise OR of xmm2/m128 and xmm3."
"VPOR ymm1, ymm2, ymm3/m256","VPOR ymm3/m256, ymm2, ymm1","vpor ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG EB /r","V","V","AVX2","","","","","Bitwise OR of ymm2/m256 and ymm3."
"VPORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPORD xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpord xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 EB /r","V","V","AVX512VL AVX512F","","","","","Bitwise OR of packed doubleword integers in xmm2 and xmm3/m128/m32bcst using writemask k1."
"VPORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPORD ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpord ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W0 EB /r","V","V","AVX512VL AVX512F","","","","","Bitwise OR of packed doubleword integers in ymm2 and ymm3/m256/m32bcst using writemask k1."
"VPORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPORD zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpord zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W0 EB /r","V","V","AVX512F","","","","","Bitwise OR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPORQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vporq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 EB /r","V","V","AVX512VL AVX512F","","","","","Bitwise OR of packed quadword integers in xmm2 and xmm3/m128/m64bcst using writemask k1."
"VPORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPORQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vporq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 EB /r","V","V","AVX512VL AVX512F","","","","","Bitwise OR of packed quadword integers in ymm2 and ymm3/m256/m64bcst using writemask k1."
"VPORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPORQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vporq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 EB /r","V","V","AVX512F","","","","","Bitwise OR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"VPSHUFB xmm1 {k1}{z}, xmm2, xmm3/m128","VPSHUFB xmm3/m128, xmm2, xmm1 {k1}{z}","vpshufb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F38.WIG 00 /r","V","V","AVX512VL AVX512BW","","","","","Shuffle bytes in xmm2 according to contents of xmm3/m128 under write mask k1."
"VPSHUFB xmm1, xmm2, xmm3/m128","VPSHUFB xmm3/m128, xmm2, xmm1","vpshufb xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 00 /r","V","V","AVX","","","","","Shuffle bytes in xmm2 according to contents of xmm3/m128."
"VPSHUFB ymm1 {k1}{z}, ymm2, ymm3/m256","VPSHUFB ymm3/m256, ymm2, ymm1 {k1}{z}","vpshufb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F38.WIG 00 /r","V","V","AVX512VL AVX512BW","","","","","Shuffle bytes in ymm2 according to contents of ymm3/m256 under write mask k1."
"VPSHUFB ymm1, ymm2, ymm3/m256","VPSHUFB ymm3/m256, ymm2, ymm1","vpshufb ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 00 /r","V","V","AVX2","","","","","Shuffle bytes in ymm2 according to contents of ymm3/m256."
"VPSHUFB zmm1 {k1}{z}, zmm2, zmm3/m512","VPSHUFB zmm3/m512, zmm2, zmm1 {k1}{z}","vpshufb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F38.WIG 00 /r","V","V","AVX512BW","","","","","Shuffle bytes in zmm2 according to contents of zmm3/m512 under write mask k1."
"VPSHUFD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","VPSHUFD imm8, xmm2/m128/m32bcst, xmm1 {k1}{z}","vpshufd imm8, xmm2/m128/m32bcst, xmm1 {k1}{z}","EVEX.128.66.0F.W0 70 /r ib","V","V","AVX512VL AVX512F","","","","","Shuffle the doublewords in xmm2/m128/m32bcst based on the encoding in imm8 and store the result in xmm1 using writemask k1."
"VPSHUFD xmm1, xmm2/m128, imm8","VPSHUFD imm8, xmm2/m128, xmm1","vpshufd imm8, xmm2/m128, xmm1","VEX.128.66.0F.WIG 70 /r ib","V","V","AVX","","","","","Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","VPSHUFD imm8, ymm2/m256/m32bcst, ymm1 {k1}{z}","vpshufd imm8, ymm2/m256/m32bcst, ymm1 {k1}{z}","EVEX.256.66.0F.W0 70 /r ib","V","V","AVX512VL AVX512F","","","","","Shuffle the doublewords in ymm2/m256/m32bcst based on the encoding in imm8 and store the result in ymm1 using writemask k1."
"VPSHUFD ymm1, ymm2/m256, imm8","VPSHUFD imm8, ymm2/m256, ymm1","vpshufd imm8, ymm2/m256, ymm1","VEX.256.66.0F.WIG 70 /r ib","V","V","AVX2","","","","","Shuffle the doublewords in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","VPSHUFD imm8, zmm2/m512/m32bcst, zmm1 {k1}{z}","vpshufd imm8, zmm2/m512/m32bcst, zmm1 {k1}{z}","EVEX.512.66.0F.W0 70 /r ib","V","V","AVX512F","","","","","Shuffle the doublewords in zmm2/m512/m32bcst based on the encoding in imm8 and store the result in zmm1 using writemask k1."
"VPSHUFHW xmm1 {k1}{z}, xmm2/m128, imm8","VPSHUFHW imm8, xmm2/m128, xmm1 {k1}{z}","vpshufhw imm8, xmm2/m128, xmm1 {k1}{z}","EVEX.128.F3.0F.WIG 70 /r ib","V","V","AVX512VL AVX512BW","","","","","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1."
"VPSHUFHW xmm1, xmm2/m128, imm8","VPSHUFHW imm8, xmm2/m128, xmm1","vpshufhw imm8, xmm2/m128, xmm1","VEX.128.F3.0F.WIG 70 /r ib","V","V","AVX","","","","","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFHW ymm1 {k1}{z}, ymm2/m256, imm8","VPSHUFHW imm8, ymm2/m256, ymm1 {k1}{z}","vpshufhw imm8, ymm2/m256, ymm1 {k1}{z}","EVEX.256.F3.0F.WIG 70 /r ib","V","V","AVX512VL AVX512BW","","","","","Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1."
"VPSHUFHW ymm1, ymm2/m256, imm8","VPSHUFHW imm8, ymm2/m256, ymm1","vpshufhw imm8, ymm2/m256, ymm1","VEX.256.F3.0F.WIG 70 /r ib","V","V","AVX2","","","","","Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFHW zmm1 {k1}{z}, zmm2/m512, imm8","VPSHUFHW imm8, zmm2/m512, zmm1 {k1}{z}","vpshufhw imm8, zmm2/m512, zmm1 {k1}{z}","EVEX.512.F3.0F.WIG 70 /r ib","V","V","AVX512BW","","","","","Shuffle the high words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1."
"VPSHUFLW xmm1 {k1}{z}, xmm2/m128, imm8","VPSHUFLW imm8, xmm2/m128, xmm1 {k1}{z}","vpshuflw imm8, xmm2/m128, xmm1 {k1}{z}","EVEX.128.F2.0F.WIG 70 /r ib","V","V","AVX512VL AVX512BW","","","","","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1."
"VPSHUFLW xmm1, xmm2/m128, imm8","VPSHUFLW imm8, xmm2/m128, xmm1","vpshuflw imm8, xmm2/m128, xmm1","VEX.128.F2.0F.WIG 70 /r ib","V","V","AVX","","","","","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFLW ymm1 {k1}{z}, ymm2/m256, imm8","VPSHUFLW imm8, ymm2/m256, ymm1 {k1}{z}","vpshuflw imm8, ymm2/m256, ymm1 {k1}{z}","EVEX.256.F2.0F.WIG 70 /r ib","V","V","AVX512VL AVX512BW","","","","","Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1."
"VPSHUFLW ymm1, ymm2/m256, imm8","VPSHUFLW imm8, ymm2/m256, ymm1","vpshuflw imm8, ymm2/m256, ymm1","VEX.256.F2.0F.WIG 70 /r ib","V","V","AVX2","","","","","Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFLW zmm1 {k1}{z}, zmm2/m512, imm8","VPSHUFLW imm8, zmm2/m512, zmm1 {k1}{z}","vpshuflw imm8, zmm2/m512, zmm1 {k1}{z}","EVEX.512.F2.0F.WIG 70 /r ib","V","V","AVX512BW","","","","","Shuffle the low words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1."
"VPSIGNB xmm1, xmm2, xmm3/m128","VPSIGNB xmm3/m128, xmm2, xmm1","vpsignb xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 08 /r","V","V","AVX","","","","","Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGNB ymm1, ymm2, ymm3/m256","VPSIGNB ymm3/m256, ymm2, ymm1","vpsignb ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 08 /r","V","V","AVX2","","","","","Negate packed byte integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGND xmm1, xmm2, xmm3/m128","VPSIGND xmm3/m128, xmm2, xmm1","vpsignd xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 0A /r","V","V","AVX","","","","","Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGND ymm1, ymm2, ymm3/m256","VPSIGND ymm3/m256, ymm2, ymm1","vpsignd ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 0A /r","V","V","AVX2","","","","","Negate packed doubleword integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGNW xmm1, xmm2, xmm3/m128","VPSIGNW xmm3/m128, xmm2, xmm1","vpsignw xmm3/m128, xmm2, xmm1","VEX.128.66.0F38.WIG 09 /r","V","V","AVX","","","","","Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGNW ymm1, ymm2, ymm3/m256","VPSIGNW ymm3/m256, ymm2, ymm1","vpsignw ymm3/m256, ymm2, ymm1","VEX.256.66.0F38.WIG 09 /r","V","V","AVX2","","","","","Negate packed 16-bit integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSLLD xmm1, xmm2, imm8","VPSLLD imm8, xmm2, xmm1","vpslld imm8, xmm2, xmm1","VEX.128.66.0F.WIG 72 /6 ib","V","V","AVX","","","","","Shift doublewords in xmm2 left by imm8 while shifting in 0s."
"VPSLLD xmm1, xmm2, xmm3/m128","VPSLLD xmm3/m128, xmm2, xmm1","vpslld xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F2 /r","V","V","AVX","","","","","Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLDQ xmm1, xmm2, imm8","VPSLLDQ imm8, xmm2, xmm1","vpslldq imm8, xmm2, xmm1","VEX.128.66.0F.WIG 73 /7 ib","V","V","AVX","","","","","Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSLLDQ xmm1, xmm2/ m128, imm8","VPSLLDQ imm8, xmm2/ m128, xmm1","vpslldq imm8, xmm2/ m128, xmm1","EVEX.128.66.0F.WIG 73 /7 ib","V","V","AVX512VL AVX512BW","","","","","Shift xmm2/m128 left by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSLLDQ ymm1, ymm2, imm8","VPSLLDQ imm8, ymm2, ymm1","vpslldq imm8, ymm2, ymm1","VEX.256.66.0F.WIG 73 /7 ib","V","V","AVX2","","","","","Shift ymm2 left by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSLLDQ ymm1, ymm2/m256, imm8","VPSLLDQ imm8, ymm2/m256, ymm1","vpslldq imm8, ymm2/m256, ymm1","EVEX.256.66.0F.WIG 73 /7 ib","V","V","AVX512VL AVX512BW","","","","","Shift ymm2/m256 left by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSLLDQ zmm1, zmm2/m512, imm8","VPSLLDQ imm8, zmm2/m512, zmm1","vpslldq imm8, zmm2/m512, zmm1","EVEX.512.66.0F.WIG 73 /7 ib","V","V","AVX512BW","","","","","Shift zmm2/m512 left by imm8 bytes while shifting in 0s and store result in zmm1."
"VPSLLQ xmm1, xmm2, imm8","VPSLLQ imm8, xmm2, xmm1","vpsllq imm8, xmm2, xmm1","VEX.128.66.0F.WIG 73 /6 ib","V","V","AVX","","","","","Shift quadwords in xmm2 left by imm8 while shifting in 0s."
"VPSLLQ xmm1, xmm2, xmm3/m128","VPSLLQ xmm3/m128, xmm2, xmm1","vpsllq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F3 /r","V","V","AVX","","","","","Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLW xmm1, xmm2, imm8","VPSLLW imm8, xmm2, xmm1","vpsllw imm8, xmm2, xmm1","VEX.128.66.0F.WIG 71 /6 ib","V","V","AVX","","","","","Shift words in xmm2 left by imm8 while shifting in 0s."
"VPSLLW xmm1, xmm2, xmm3/m128","VPSLLW xmm3/m128, xmm2, xmm1","vpsllw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F1 /r","V","V","AVX","","","","","Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLW ymm1, ymm2, imm8","VPSLLW imm8, ymm2, ymm1","vpsllw imm8, ymm2, ymm1","VEX.256.66.0F.WIG 71 /6 ib","V","V","AVX2","","","","","Shift words in ymm2 left by imm8 while shifting in 0s."
"VPSLLW ymm1, ymm2, xmm3/m128","VPSLLW xmm3/m128, ymm2, ymm1","vpsllw xmm3/m128, ymm2, ymm1","VEX.256.66.0F.WIG F1 /r","V","V","AVX2","","","","","Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSRAD xmm1, xmm2, imm8","VPSRAD imm8, xmm2, xmm1","vpsrad imm8, xmm2, xmm1","VEX.128.66.0F.WIG 72 /4 ib","V","V","AVX","","","","","Shift doublewords in xmm2 right by imm8 while shifting in sign bits."
"VPSRAD xmm1, xmm2, xmm3/m128","VPSRAD xmm3/m128, xmm2, xmm1","vpsrad xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E2 /r","V","V","AVX","","","","","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAD ymm1, ymm2, imm8","VPSRAD imm8, ymm2, ymm1","vpsrad imm8, ymm2, ymm1","VEX.256.66.0F.WIG 72 /4 ib","V","V","AVX2","","","","","Shift doublewords in ymm2 right by imm8 while shifting in sign bits."
"VPSRAD ymm1, ymm2, xmm3/m128","VPSRAD xmm3/m128, ymm2, ymm1","vpsrad xmm3/m128, ymm2, ymm1","VEX.256.66.0F.WIG E2 /r","V","V","AVX2","","","","","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAW xmm1 {k1}{z}, xmm2, xmm3/m128","VPSRAW xmm3/m128, xmm2, xmm1 {k1}{z}","vpsraw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E1 /r","V","V","AVX512VL AVX512BW","","","","","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAW xmm1, xmm2, imm8","VPSRAW imm8, xmm2, xmm1","vpsraw imm8, xmm2, xmm1","VEX.128.66.0F.WIG 71 /4 ib","V","V","AVX","","","","","Shift words in xmm2 right by imm8 while shifting in sign bits."
"VPSRAW xmm1, xmm2, xmm3/m128","VPSRAW xmm3/m128, xmm2, xmm1","vpsraw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E1 /r","V","V","AVX","","","","","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAW ymm1 {k1}{z}, ymm2, xmm3/m128","VPSRAW xmm3/m128, ymm2, ymm1 {k1}{z}","vpsraw xmm3/m128, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E1 /r","V","V","AVX512VL AVX512BW","","","","","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAW ymm1, ymm2, imm8","VPSRAW imm8, ymm2, ymm1","vpsraw imm8, ymm2, ymm1","VEX.256.66.0F.WIG 71 /4 ib","V","V","AVX2","","","","","Shift words in ymm2 right by imm8 while shifting in sign bits."
"VPSRAW ymm1, ymm2, xmm3/m128","VPSRAW xmm3/m128, ymm2, ymm1","vpsraw xmm3/m128, ymm2, ymm1","VEX.256.66.0F.WIG E1 /r","V","V","AVX2","","","","","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits."
"VPSRAW zmm1 {k1}{z}, zmm2, xmm3/m128","VPSRAW xmm3/m128, zmm2, zmm1 {k1}{z}","vpsraw xmm3/m128, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E1 /r","V","V","AVX512BW","","","","","Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRLD xmm1, xmm2, imm8","VPSRLD imm8, xmm2, xmm1","vpsrld imm8, xmm2, xmm1","VEX.128.66.0F.WIG 72 /2 ib","V","V","AVX","","","","","Shift doublewords in xmm2 right by imm8 while shifting in 0s."
"VPSRLD xmm1, xmm2, xmm3/m128","VPSRLD xmm3/m128, xmm2, xmm1","vpsrld xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D2 /r","V","V","AVX","","","","","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLDQ xmm1, xmm2, imm8","VPSRLDQ imm8, xmm2, xmm1","vpsrldq imm8, xmm2, xmm1","VEX.128.66.0F.WIG 73 /3 ib","V","V","AVX","","","","","Shift xmm2 right by imm8 bytes while shifting in 0s."
"VPSRLDQ xmm1, xmm2/m128, imm8","VPSRLDQ imm8, xmm2/m128, xmm1","vpsrldq imm8, xmm2/m128, xmm1","EVEX.128.66.0F.WIG 73 /3 ib","V","V","AVX512VL AVX512BW","","","","","Shift xmm2/m128 right by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSRLDQ ymm1, ymm2, imm8","VPSRLDQ imm8, ymm2, ymm1","vpsrldq imm8, ymm2, ymm1","VEX.256.66.0F.WIG 73 /3 ib","V","V","AVX2","","","","","Shift ymm1 right by imm8 bytes while shifting in 0s."
"VPSRLDQ ymm1, ymm2/m256, imm8","VPSRLDQ imm8, ymm2/m256, ymm1","vpsrldq imm8, ymm2/m256, ymm1","EVEX.256.66.0F.WIG 73 /3 ib","V","V","AVX512VL AVX512BW","","","","","Shift ymm2/m256 right by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSRLDQ zmm1, zmm2/m512, imm8","VPSRLDQ imm8, zmm2/m512, zmm1","vpsrldq imm8, zmm2/m512, zmm1","EVEX.512.66.0F.WIG 73 /3 ib","V","V","AVX512BW","","","","","Shift zmm2/m512 right by imm8 bytes while shifting in 0s and store result in zmm1."
"VPSRLQ xmm1, xmm2, imm8","VPSRLQ imm8, xmm2, xmm1","vpsrlq imm8, xmm2, xmm1","VEX.128.66.0F.WIG 73 /2 ib","V","V","AVX","","","","","Shift quadwords in xmm2 right by imm8 while shifting in 0s."
"VPSRLQ xmm1, xmm2, xmm3/m128","VPSRLQ xmm3/m128, xmm2, xmm1","vpsrlq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D3 /r","V","V","AVX","","","","","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLW xmm1, xmm2, imm8","VPSRLW imm8, xmm2, xmm1","vpsrlw imm8, xmm2, xmm1","VEX.128.66.0F.WIG 71 /2 ib","V","V","AVX","","","","","Shift words in xmm2 right by imm8 while shifting in 0s."
"VPSRLW xmm1, xmm2, xmm3/m128","VPSRLW xmm3/m128, xmm2, xmm1","vpsrlw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D1 /r","V","V","AVX","","","","","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLW ymm1, ymm2, imm8","VPSRLW imm8, ymm2, ymm1","vpsrlw imm8, ymm2, ymm1","VEX.256.66.0F.WIG 71 /2 ib","V","V","AVX2","","","","","Shift words in ymm2 right by imm8 while shifting in 0s."
"VPSRLW ymm1, ymm2, xmm3/m128","VPSRLW xmm3/m128, ymm2, ymm1","vpsrlw xmm3/m128, ymm2, ymm1","VEX.256.66.0F.WIG D1 /r","V","V","AVX2","","","","","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSUBB xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBB xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG F8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed byte integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBB xmm1, xmm2, xmm3/m128","VPSUBB xmm3/m128, xmm2, xmm1","vpsubb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F8 /r","V","V","AVX","","","","","Subtract packed byte integers in xmm3/m128 from xmm2."
"VPSUBB ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBB ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG F8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed byte integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBB ymm1, ymm2, ymm3/m256","VPSUBB ymm3/m256, ymm2, ymm1","vpsubb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG F8 /r","V","V","AVX2","","","","","Subtract packed byte integers in ymm3/m256 from ymm2."
"VPSUBB zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBB zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG F8 /r","V","V","AVX512BW","","","","","Subtract packed byte integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1."
"VPSUBD xmm1, xmm2, xmm3/m128","VPSUBD xmm3/m128, xmm2, xmm1","vpsubd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG FA /r","V","V","AVX","","","","","Subtract packed doubleword integers in xmm3/m128 from xmm2."
"VPSUBD ymm1, ymm2, ymm3/m256","VPSUBD ymm3/m256, ymm2, ymm1","vpsubd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG FA /r","V","V","AVX2","","","","","Subtract packed doubleword integers in ymm3/m256 from ymm2."
"VPSUBQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPSUBQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpsubq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 FB /r","V","V","AVX512VL AVX512F","","","","","Subtract packed quadword integers in xmm3/m128/m64bcst from xmm2 and store in xmm1 using writemask k1."
"VPSUBQ xmm1, xmm2, xmm3/m128","VPSUBQ xmm3/m128, xmm2, xmm1","vpsubq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG FB /r","V","V","AVX","","","","","Subtract packed quadword integers in xmm3/m128 from xmm2."
"VPSUBQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPSUBQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpsubq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 FB /r","V","V","AVX512VL AVX512F","","","","","Subtract packed quadword integers in ymm3/m256/m64bcst from ymm2 and store in ymm1 using writemask k1."
"VPSUBQ ymm1, ymm2, ymm3/m256","VPSUBQ ymm3/m256, ymm2, ymm1","vpsubq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG FB /r","V","V","AVX2","","","","","Subtract packed quadword integers in ymm3/m256 from ymm2."
"VPSUBQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPSUBQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpsubq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 FB /r","V","V","AVX512F","","","","","Subtract packed quadword integers in zmm3/m512/m64bcst from zmm2 and store in zmm1 using writemask k1."
"VPSUBSB xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBSB xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubsb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSB xmm1, xmm2, xmm3/m128","VPSUBSB xmm3/m128, xmm2, xmm1","vpsubsb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E8 /r","V","V","AVX","","","","","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results."
"VPSUBSB ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBSB ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubsb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results and store in ymm1 using writemask k1."
"VPSUBSB ymm1, ymm2, ymm3/m256","VPSUBSB ymm3/m256, ymm2, ymm1","vpsubsb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E8 /r","V","V","AVX2","","","","","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results."
"VPSUBSB zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBSB zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubsb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E8 /r","V","V","AVX512BW","","","","","Subtract packed signed byte integers in zmm3/m512 from packed signed byte integers in zmm2 and saturate results and store in zmm1 using writemask k1."
"VPSUBSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubsw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG E9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSW xmm1, xmm2, xmm3/m128","VPSUBSW xmm3/m128, xmm2, xmm1","vpsubsw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG E9 /r","V","V","AVX","","","","","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results."
"VPSUBSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubsw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG E9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results and store in ymm1 using writemask k1."
"VPSUBSW ymm1, ymm2, ymm3/m256","VPSUBSW ymm3/m256, ymm2, ymm1","vpsubsw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG E9 /r","V","V","AVX2","","","","","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results."
"VPSUBSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubsw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG E9 /r","V","V","AVX512BW","","","","","Subtract packed signed word integers in zmm3/m512 from packed signed word integers in zmm2 and saturate results and store in zmm1 using writemask k1."
"VPSUBUSB xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBUSB xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubusb xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG D8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2, saturate results and store in xmm1 using writemask k1."
"VPSUBUSB xmm1, xmm2, xmm3/m128","VPSUBUSB xmm3/m128, xmm2, xmm1","vpsubusb xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D8 /r","V","V","AVX","","","","","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result."
"VPSUBUSB ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBUSB ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubusb ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG D8 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2, saturate results and store in ymm1 using writemask k1."
"VPSUBUSB ymm1, ymm2, ymm3/m256","VPSUBUSB ymm3/m256, ymm2, ymm1","vpsubusb ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG D8 /r","V","V","AVX2","","","","","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result."
"VPSUBUSB zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBUSB zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubusb zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG D8 /r","V","V","AVX512BW","","","","","Subtract packed unsigned byte integers in zmm3/m512 from packed unsigned byte integers in zmm2, saturate results and store in zmm1 using writemask k1."
"VPSUBUSW xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBUSW xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubusw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG D9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBUSW xmm1, xmm2, xmm3/m128","VPSUBUSW xmm3/m128, xmm2, xmm1","vpsubusw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG D9 /r","V","V","AVX","","","","","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result."
"VPSUBUSW ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBUSW ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubusw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG D9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2, saturate results and store in ymm1 using writemask k1."
"VPSUBUSW ymm1, ymm2, ymm3/m256","VPSUBUSW ymm3/m256, ymm2, ymm1","vpsubusw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG D9 /r","V","V","AVX2","","","","","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result."
"VPSUBUSW zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBUSW zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubusw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG D9 /r","V","V","AVX512BW","","","","","Subtract packed unsigned word integers in zmm3/m512 from packed unsigned word integers in zmm2, saturate results and store in zmm1 using writemask k1."
"VPSUBW xmm1 {k1}{z}, xmm2, xmm3/m128","VPSUBW xmm3/m128, xmm2, xmm1 {k1}{z}","vpsubw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG F9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed word integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBW xmm1, xmm2, xmm3/m128","VPSUBW xmm3/m128, xmm2, xmm1","vpsubw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG F9 /r","V","V","AVX","","","","","Subtract packed word integers in xmm3/m128 from xmm2."
"VPSUBW ymm1 {k1}{z}, ymm2, ymm3/m256","VPSUBW ymm3/m256, ymm2, ymm1 {k1}{z}","vpsubw ymm3/m256, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.WIG F9 /r","V","V","AVX512VL AVX512BW","","","","","Subtract packed word integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBW ymm1, ymm2, ymm3/m256","VPSUBW ymm3/m256, ymm2, ymm1","vpsubw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG F9 /r","V","V","AVX2","","","","","Subtract packed word integers in ymm3/m256 from ymm2."
"VPSUBW zmm1 {k1}{z}, zmm2, zmm3/m512","VPSUBW zmm3/m512, zmm2, zmm1 {k1}{z}","vpsubw zmm3/m512, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.WIG F9 /r","V","V","AVX512BW","","","","","Subtract packed word integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1."
"VPTEST xmm1, xmm2/m128","VPTEST xmm2/m128, xmm1","vptest xmm2/m128, xmm1","VEX.128.66.0F38.WIG 17 /r","V","V","AVX","","","","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"VPTEST ymm1, ymm2/m256","VPTEST ymm2/m256, ymm1","vptest ymm2/m256, ymm1","VEX.256.66.0F38.WIG 17 /r","V","V","AVX","","","","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"VPUNPCKHBW xmm1 {k1}{z}, xmm2, xmm3/m128","VPUNPCKHBW xmm3/m128, xmm2, xmm1 {k1}{z}","vpunpckhbw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG 68 /r","V","V","AVX512VL AVX512BW","","","","","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKHBW xmm1, xmm2, xmm3/m128","VPUNPCKHBW xmm3/m128, xmm2, xmm1","vpunpckhbw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 68 /r","V","V","AVX","","","","","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHBW ymm1, ymm2, ymm3/m256","VPUNPCKHBW ymm3/m256, ymm2, ymm1","vpunpckhbw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 68 /r","V","V","AVX2","","","","","Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPUNPCKHDQ xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpunpckhdq xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 6A /r","V","V","AVX512VL AVX512F","","","","","Interleave high-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register using k1 write mask."
"VPUNPCKHDQ xmm1, xmm2, xmm3/m128","VPUNPCKHDQ xmm3/m128, xmm2, xmm1","vpunpckhdq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 6A /r","V","V","AVX","","","","","Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHDQ ymm1, ymm2, ymm3/m256","VPUNPCKHDQ ymm3/m256, ymm2, ymm1","vpunpckhdq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 6A /r","V","V","AVX2","","","","","Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPUNPCKHQDQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpunpckhqdq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 6D /r","V","V","AVX512VL AVX512F","","","","","Interleave high-order quadword from xmm2 and xmm3/m128/m64bcst into xmm1 register using k1 write mask."
"VPUNPCKHQDQ xmm1, xmm2, xmm3/m128","VPUNPCKHQDQ xmm3/m128, xmm2, xmm1","vpunpckhqdq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 6D /r","V","V","AVX","","","","","Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKHQDQ ymm1, ymm2, ymm3/m256","VPUNPCKHQDQ ymm3/m256, ymm2, ymm1","vpunpckhqdq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 6D /r","V","V","AVX2","","","","","Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHWD xmm1 {k1}{z}, xmm2, xmm3/m128","VPUNPCKHWD xmm3/m128, xmm2, xmm1 {k1}{z}","vpunpckhwd xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG 69 /r","V","V","AVX512VL AVX512BW","","","","","Interleave high-order words from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKHWD xmm1, xmm2, xmm3/m128","VPUNPCKHWD xmm3/m128, xmm2, xmm1","vpunpckhwd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 69 /r","V","V","AVX","","","","","Interleave high-order words from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHWD ymm1, ymm2, ymm3/m256","VPUNPCKHWD ymm3/m256, ymm2, ymm1","vpunpckhwd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 69 /r","V","V","AVX2","","","","","Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLBW xmm1 {k1}{z}, xmm2, xmm3/m128","VPUNPCKLBW xmm3/m128, xmm2, xmm1 {k1}{z}","vpunpcklbw xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG 60 /r","V","V","AVX512VL AVX512BW","","","","","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
"VPUNPCKLBW xmm1, xmm2, xmm3/m128","VPUNPCKLBW xmm3/m128, xmm2, xmm1","vpunpcklbw xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 60 /r","V","V","AVX","","","","","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLBW ymm1, ymm2, ymm3/m256","VPUNPCKLBW ymm3/m256, ymm2, ymm1","vpunpcklbw ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 60 /r","V","V","AVX2","","","","","Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPUNPCKLDQ xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpunpckldq xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 62 /r","V","V","AVX512VL AVX512F","","","","","Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1."
"VPUNPCKLDQ xmm1, xmm2, xmm3/m128","VPUNPCKLDQ xmm3/m128, xmm2, xmm1","vpunpckldq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 62 /r","V","V","AVX","","","","","Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLDQ ymm1, ymm2, ymm3/m256","VPUNPCKLDQ ymm3/m256, ymm2, ymm1","vpunpckldq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 62 /r","V","V","AVX2","","","","","Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPUNPCKLQDQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpunpcklqdq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 6C /r","V","V","AVX512VL AVX512F","","","","","Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1."
"VPUNPCKLQDQ xmm1, xmm2, xmm3/m128","VPUNPCKLQDQ xmm3/m128, xmm2, xmm1","vpunpcklqdq xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 6C /r","V","V","AVX","","","","","Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKLQDQ ymm1, ymm2, ymm3/m256","VPUNPCKLQDQ ymm3/m256, ymm2, ymm1","vpunpcklqdq ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 6C /r","V","V","AVX2","","","","","Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLWD xmm1 {k1}{z}, xmm2, xmm3/m128","VPUNPCKLWD xmm3/m128, xmm2, xmm1 {k1}{z}","vpunpcklwd xmm3/m128, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.WIG 61 /r","V","V","AVX512VL AVX512BW","","","","","Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
"VPUNPCKLWD xmm1, xmm2, xmm3/m128","VPUNPCKLWD xmm3/m128, xmm2, xmm1","vpunpcklwd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 61 /r","V","V","AVX","","","","","Interleave low-order words from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLWD ymm1, ymm2, ymm3/m256","VPUNPCKLWD ymm3/m256, ymm2, ymm1","vpunpcklwd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 61 /r","V","V","AVX2","","","","","Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPXOR xmm1, xmm2, xmm3/m128","VPXOR xmm3/m128, xmm2, xmm1","vpxor xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG EF /r","V","V","AVX","","","","","Bitwise XOR of xmm3/m128 and xmm2."
"VPXOR ymm1, ymm2, ymm3/m256","VPXOR ymm3/m256, ymm2, ymm1","vpxor ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG EF /r","V","V","AVX2","","","","","Bitwise XOR of ymm3/m256 and ymm2."
"VPXORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VPXORD xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vpxord xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W0 EF /r","V","V","AVX512VL AVX512F","","","","","Bitwise XOR of packed doubleword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VPXORD ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vpxord ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W0 EF /r","V","V","AVX512VL AVX512F","","","","","Bitwise XOR of packed doubleword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VPXORD zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vpxord zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W0 EF /r","V","V","AVX512F","","","","","Bitwise XOR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPXORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VPXORQ xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vpxorq xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 EF /r","V","V","AVX512VL AVX512F","","","","","Bitwise XOR of packed quadword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VPXORQ ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vpxorq ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 EF /r","V","V","AVX512VL AVX512F","","","","","Bitwise XOR of packed quadword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VPXORQ zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vpxorq zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 EF /r","V","V","AVX512F","","","","","Bitwise XOR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"VRCPPS xmm1, xmm2/m128","VRCPPS xmm2/m128, xmm1","vrcpps xmm2/m128, xmm1","VEX.128.0F.WIG 53 /r","V","V","AVX","","","","","Computes the approximate reciprocals of packed single precision values in xmm2/mem and stores the results in xmm1."
"VRCPPS ymm1, ymm2/m256","VRCPPS ymm2/m256, ymm1","vrcpps ymm2/m256, ymm1","VEX.256.0F.WIG 53 /r","V","V","AVX","","","","","Computes the approximate reciprocals of packed single precision values in ymm2/mem and stores the results in ymm1."
"VRCPSS xmm1, xmm2, xmm3/m32","VRCPSS xmm3/m32, xmm2, xmm1","vrcpss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 53 /r","V","V","AVX","","","","","Computes the approximate reciprocal of the scalar single precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VROUNDPD xmm1, xmm2/m128, imm8","VROUNDPD imm8, xmm2/m128, xmm1","vroundpd imm8, xmm2/m128, xmm1","VEX.128.66.0F3A.WIG 09 /r ib","V","V","AVX","","","","","Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPD ymm1, ymm2/m256, imm8","VROUNDPD imm8, ymm2/m256, ymm1","vroundpd imm8, ymm2/m256, ymm1","VEX.256.66.0F3A.WIG 09 /r ib","V","V","AVX","","","","","Round packed double precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8."
"VROUNDPS xmm1, xmm2/m128, imm8","VROUNDPS imm8, xmm2/m128, xmm1","vroundps imm8, xmm2/m128, xmm1","VEX.128.66.0F3A.WIG 08 /r ib","V","V","AVX","","","","","Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPS ymm1, ymm2/m256, imm8","VROUNDPS imm8, ymm2/m256, ymm1","vroundps imm8, ymm2/m256, ymm1","VEX.256.66.0F3A.WIG 08 /r ib","V","V","AVX","","","","","Round packed single precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8."
"VROUNDSD xmm1, xmm2, xmm3/m64, imm8","VROUNDSD imm8, xmm3/m64, xmm2, xmm1","vroundsd imm8, xmm3/m64, xmm2, xmm1","VEX.LIG.66.0F3A.WIG 0B /r ib","V","V","AVX","","","","","Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VROUNDSS xmm1, xmm2, xmm3/m32, imm8","VROUNDSS imm8, xmm3/m32, xmm2, xmm1","vroundss imm8, xmm3/m32, xmm2, xmm1","VEX.LIG.66.0F3A.WIG 0A /r ib","V","V","AVX","","","","","Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VRSQRTPS xmm1, xmm2/m128","VRSQRTPS xmm2/m128, xmm1","vrsqrtps xmm2/m128, xmm1","VEX.128.0F.WIG 52 /r","V","V","AVX","","","","","Computes the approximate reciprocals of the square roots of packed single precision values in xmm2/mem and stores the results in xmm1."
"VRSQRTPS ymm1, ymm2/m256","VRSQRTPS ymm2/m256, ymm1","vrsqrtps ymm2/m256, ymm1","VEX.256.0F.WIG 52 /r","V","V","AVX","","","","","Computes the approximate reciprocals of the square roots of packed single precision values in ymm2/mem and stores the results in ymm1."
"VRSQRTSS xmm1, xmm2, xmm3/m32","VRSQRTSS xmm3/m32, xmm2, xmm1","vrsqrtss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 52 /r","V","V","AVX","","","","","Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSHUFPD xmm1, xmm2, xmm3/m128, imm8","VSHUFPD imm8, xmm3/m128, xmm2, xmm1","vshufpd imm8, xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG C6 /r ib","V","V","AVX","","","","","Shuffle two pairs of double precision floating-point values from xmm2 and xmm3/m128 using imm8 to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","VSHUFPD imm8, xmm3/m128/m64bcst, xmm2, xmm1{k1}{z}","vshufpd imm8, xmm3/m128/m64bcst, xmm2, xmm1{k1}{z}","EVEX.128.66.0F.W1 C6 /r ib","V","V","AVX512VL AVX512F","","","","","Shuffle two paris of double precision floating-point values from xmm2 and xmm3/m128/m64bcst using imm8 to select from each pair. store interleaved results in xmm1 subject to writemask k1."
"VSHUFPD ymm1, ymm2, ymm3/m256, imm8","VSHUFPD imm8, ymm3/m256, ymm2, ymm1","vshufpd imm8, ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG C6 /r ib","V","V","AVX","","","","","Shuffle four pairs of double precision floating-point values from ymm2 and ymm3/m256 using imm8 to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","VSHUFPD imm8, ymm3/m256/m64bcst, ymm2, ymm1{k1}{z}","vshufpd imm8, ymm3/m256/m64bcst, ymm2, ymm1{k1}{z}","EVEX.256.66.0F.W1 C6 /r ib","V","V","AVX512VL AVX512F","","","","","Shuffle four paris of double precision floating-point values from ymm2 and ymm3/m256/m64bcst using imm8 to select from each pair. store interleaved results in ymm1 subject to writemask k1."
"VSHUFPD zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","VSHUFPD imm8, zmm3/m512/m64bcst, zmm2, zmm1{k1}{z}","vshufpd imm8, zmm3/m512/m64bcst, zmm2, zmm1{k1}{z}","EVEX.512.66.0F.W1 C6 /r ib","V","V","AVX512F","","","","","Shuffle eight paris of double precision floating-point values from zmm2 and zmm3/m512/m64bcst using imm8 to select from each pair. store interleaved results in zmm1 subject to writemask k1."
"VSHUFPS xmm1, xmm2, xmm3/m128, imm8","VSHUFPS imm8, xmm3/m128, xmm2, xmm1","vshufps imm8, xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG C6 /r ib","V","V","AVX","","","","","Select from quadruplet of single precision floating- point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1."
"VSHUFPS xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","VSHUFPS imm8, xmm3/m128/m32bcst, xmm2, xmm1{k1}{z}","vshufps imm8, xmm3/m128/m32bcst, xmm2, xmm1{k1}{z}","EVEX.128.0F.W0 C6 /r ib","V","V","AVX512VL AVX512F","","","","","Select from quadruplet of single precision floating- point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1, subject to writemask k1."
"VSHUFPS ymm1, ymm2, ymm3/m256, imm8","VSHUFPS imm8, ymm3/m256, ymm2, ymm1","vshufps imm8, ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG C6 /r ib","V","V","AVX","","","","","Select from quadruplet of single precision floating- point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1."
"VSHUFPS ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","VSHUFPS imm8, ymm3/m256/m32bcst, ymm2, ymm1{k1}{z}","vshufps imm8, ymm3/m256/m32bcst, ymm2, ymm1{k1}{z}","EVEX.256.0F.W0 C6 /r ib","V","V","AVX512VL AVX512F","","","","","Select from quadruplet of single precision floating- point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1, subject to writemask k1."
"VSHUFPS zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","VSHUFPS imm8, zmm3/m512/m32bcst, zmm2, zmm1{k1}{z}","vshufps imm8, zmm3/m512/m32bcst, zmm2, zmm1{k1}{z}","EVEX.512.0F.W0 C6 /r ib","V","V","AVX512F","","","","","Select from quadruplet of single precision floating- point values in zmm2 and zmm3/m512 using imm8, interleaved result pairs are stored in zmm1, subject to writemask k1."
"VSQRTPD xmm1 {k1}{z}, xmm2/m128/m64bcst","VSQRTPD xmm2/m128/m64bcst, xmm1 {k1}{z}","vsqrtpd xmm2/m128/m64bcst, xmm1 {k1}{z}","EVEX.128.66.0F.W1 51 /r","V","V","AVX512VL AVX512F","","","","","Computes Square Roots of the packed double precision floating-point values in xmm2/m128/m64bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPD xmm1, xmm2/m128","VSQRTPD xmm2/m128, xmm1","vsqrtpd xmm2/m128, xmm1","VEX.128.66.0F.WIG 51 /r","V","V","AVX","","","","","Computes Square Roots of the packed double precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPD ymm1 {k1}{z}, ymm2/m256/m64bcst","VSQRTPD ymm2/m256/m64bcst, ymm1 {k1}{z}","vsqrtpd ymm2/m256/m64bcst, ymm1 {k1}{z}","EVEX.256.66.0F.W1 51 /r","V","V","AVX512VL AVX512F","","","","","Computes Square Roots of the packed double precision floating-point values in ymm2/m256/m64bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPD ymm1, ymm2/m256","VSQRTPD ymm2/m256, ymm1","vsqrtpd ymm2/m256, ymm1","VEX.256.66.0F.WIG 51 /r","V","V","AVX","","","","","Computes Square Roots of the packed double precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTPD zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","VSQRTPD zmm2/m512/m64bcst{er}, zmm1 {k1}{z}","vsqrtpd zmm2/m512/m64bcst{er}, zmm1 {k1}{z}","EVEX.512.66.0F.W1 51 /r","V","V","AVX512F","","","","","Computes Square Roots of the packed double precision floating-point values in zmm2/m512/m64bcst and stores the result in zmm1 subject to writemask k1."
"VSQRTPS xmm1 {k1}{z}, xmm2/m128/m32bcst","VSQRTPS xmm2/m128/m32bcst, xmm1 {k1}{z}","vsqrtps xmm2/m128/m32bcst, xmm1 {k1}{z}","EVEX.128.0F.W0 51 /r","V","V","AVX512VL AVX512F","","","","","Computes Square Roots of the packed single precision floating-point values in xmm2/m128/m32bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPS xmm1, xmm2/m128","VSQRTPS xmm2/m128, xmm1","vsqrtps xmm2/m128, xmm1","VEX.128.0F.WIG 51 /r","V","V","AVX","","","","","Computes Square Roots of the packed single precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPS ymm1 {k1}{z}, ymm2/m256/m32bcst","VSQRTPS ymm2/m256/m32bcst, ymm1 {k1}{z}","vsqrtps ymm2/m256/m32bcst, ymm1 {k1}{z}","EVEX.256.0F.W0 51 /r","V","V","AVX512VL AVX512F","","","","","Computes Square Roots of the packed single precision floating-point values in ymm2/m256/m32bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPS ymm1, ymm2/m256","VSQRTPS ymm2/m256, ymm1","vsqrtps ymm2/m256, ymm1","VEX.256.0F.WIG 51 /r","V","V","AVX","","","","","Computes Square Roots of the packed single precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTPS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","VSQRTPS zmm2/m512/m32bcst{er}, zmm1 {k1}{z}","vsqrtps zmm2/m512/m32bcst{er}, zmm1 {k1}{z}","EVEX.512.0F.W0 51 /r","V","V","AVX512F","","","","","Computes Square Roots of the packed single precision floating-point values in zmm2/m512/m32bcst and stores the result in zmm1 subject to writemask k1."
"VSQRTSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","VSQRTSD xmm3/m64{er}, xmm2, xmm1 {k1}{z}","vsqrtsd xmm3/m64{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 51 /r","V","V","AVX512F","","","","","Computes square root of the low double precision floating- point value in xmm3/m64 and stores the results in xmm1 under writemask k1. Also, upper double precision floating- point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VSQRTSD xmm1, xmm2, xmm3/m64","VSQRTSD xmm3/m64, xmm2, xmm1","vsqrtsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 51 /r","V","V","AVX","","","","","Computes square root of the low double precision floating- point value in xmm3/m64 and stores the results in xmm1. Also, upper double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VSQRTSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","VSQRTSS xmm3/m32{er}, xmm2, xmm1 {k1}{z}","vsqrtss xmm3/m32{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 51 /r","V","V","AVX512F","","","","","Computes square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1 under writemask k1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSQRTSS xmm1, xmm2, xmm3/m32","VSQRTSS xmm3/m32, xmm2, xmm1","vsqrtss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 51 /r","V","V","AVX","","","","","Computes square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSTMXCSR m32","VSTMXCSR m32","vstmxcsr m32","VEX.LZ.0F.WIG AE /3","V","V","AVX","","","","","Store contents of MXCSR register to m32."
"VSUBPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VSUBPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vsubpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 5C /r","V","V","AVX512VL AVX512F","","","","","Subtract packed double precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VSUBPD xmm1, xmm2, xmm3/m128","VSUBPD xmm3/m128, xmm2, xmm1","vsubpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 5C /r","V","V","AVX","","","","","Subtract packed double precision floating-point values in xmm3/mem from xmm2 and store result in xmm1."
"VSUBPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VSUBPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vsubpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 5C /r","V","V","AVX512VL AVX512F","","","","","Subtract packed double precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VSUBPD ymm1, ymm2, ymm3/m256","VSUBPD ymm3/m256, ymm2, ymm1","vsubpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 5C /r","V","V","AVX","","","","","Subtract packed double precision floating-point values in ymm3/mem from ymm2 and store result in ymm1."
"VSUBPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","VSUBPD zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","vsubpd zmm3/m512/m64bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 5C /r","V","V","AVX512F","","","","","Subtract packed double precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"VSUBPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VSUBPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vsubps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 5C /r","V","V","AVX512VL AVX512F","","","","","Subtract packed single precision floating-point values from xmm3/m128/m32bcst to xmm2 and stores result in xmm1 with writemask k1."
"VSUBPS xmm1, xmm2, xmm3/m128","VSUBPS xmm3/m128, xmm2, xmm1","vsubps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 5C /r","V","V","AVX","","","","","Subtract packed single precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1."
"VSUBPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VSUBPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vsubps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 5C /r","V","V","AVX512VL AVX512F","","","","","Subtract packed single precision floating-point values from ymm3/m256/m32bcst to ymm2 and stores result in ymm1 with writemask k1."
"VSUBPS ymm1, ymm2, ymm3/m256","VSUBPS ymm3/m256, ymm2, ymm1","vsubps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 5C /r","V","V","AVX","","","","","Subtract packed single precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1."
"VSUBPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","VSUBPS zmm3/m512/m32bcst{er}, zmm2, zmm1 {k1}{z}","vsubps zmm3/m512/m32bcst{er}, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 5C /r","V","V","AVX512F","","","","","Subtract packed single precision floating-point values in zmm3/m512/m32bcst from zmm2 and stores result in zmm1 with writemask k1."
"VSUBSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","VSUBSD xmm3/m64{er}, xmm2, xmm1 {k1}{z}","vsubsd xmm3/m64{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F2.0F.W1 5C /r","V","V","AVX512F","","","","","Subtract the low double precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1 under writemask k1."
"VSUBSD xmm1, xmm2, xmm3/m64","VSUBSD xmm3/m64, xmm2, xmm1","vsubsd xmm3/m64, xmm2, xmm1","VEX.LIG.F2.0F.WIG 5C /r","V","V","AVX","","","","","Subtract the low double precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1."
"VSUBSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","VSUBSS xmm3/m32{er}, xmm2, xmm1 {k1}{z}","vsubss xmm3/m32{er}, xmm2, xmm1 {k1}{z}","EVEX.LLIG.F3.0F.W0 5C /r","V","V","AVX512F","","","","","Subtract the low single precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1 under writemask k1."
"VSUBSS xmm1, xmm2, xmm3/m32","VSUBSS xmm3/m32, xmm2, xmm1","vsubss xmm3/m32, xmm2, xmm1","VEX.LIG.F3.0F.WIG 5C /r","V","V","AVX","","","","","Subtract the low single precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1."
"VUCOMISD xmm1, xmm2/m64","VUCOMISD xmm2/m64, xmm1","vucomisd xmm2/m64, xmm1","VEX.LIG.66.0F.WIG 2E /r","V","V","AVX","","","","","Compare low double precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VUCOMISD xmm1, xmm2/m64{sae}","VUCOMISD xmm2/m64{sae}, xmm1","vucomisd xmm2/m64{sae}, xmm1","EVEX.LLIG.66.0F.W1 2E /r","V","V","AVX512F","","","","","Compare low double precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32","VUCOMISS xmm2/m32, xmm1","vucomiss xmm2/m32, xmm1","VEX.LIG.0F.WIG 2E /r","V","V","AVX","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32{sae}","VUCOMISS xmm2/m32{sae}, xmm1","vucomiss xmm2/m32{sae}, xmm1","EVEX.LLIG.0F.W0 2E /r","V","V","AVX512F","","","","","Compare low single precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUNPCKHPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VUNPCKHPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vunpckhpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 15 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VUNPCKHPD xmm1, xmm2, xmm3/m128","VUNPCKHPD xmm3/m128, xmm2, xmm1","vunpckhpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 15 /r","V","V","AVX","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VUNPCKHPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vunpckhpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 15 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VUNPCKHPD ymm1, ymm2, ymm3/m256","VUNPCKHPD ymm3/m256, ymm2, ymm1","vunpckhpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 15 /r","V","V","AVX","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKHPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VUNPCKHPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vunpckhpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 15 /r","V","V","AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from high quadwords of zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VUNPCKHPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VUNPCKHPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vunpckhps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 15 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of xmm2 and xmm3/m128/m32bcst and write result to xmm1 subject to writemask k1."
"VUNPCKHPS xmm1, xmm2, xmm3/m128","VUNPCKHPS xmm3/m128, xmm2, xmm1","vunpckhps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 15 /r","V","V","AVX","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VUNPCKHPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vunpckhps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 15 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of ymm2 and ymm3/m256/m32bcst and write result to ymm1 subject to writemask k1."
"VUNPCKHPS ymm1, ymm2, ymm3/m256","VUNPCKHPS ymm3/m256, ymm2, ymm1","vunpckhps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 15 /r","V","V","AVX","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKHPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VUNPCKHPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vunpckhps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 15 /r","V","V","AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from high quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to writemask k1."
"VUNPCKLPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VUNPCKLPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vunpcklpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 14 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of xmm2 and xmm3/m128/m64bcst subject to write mask k1."
"VUNPCKLPD xmm1, xmm2, xmm3/m128","VUNPCKLPD xmm3/m128, xmm2, xmm1","vunpcklpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 14 /r","V","V","AVX","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VUNPCKLPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vunpcklpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 14 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of ymm2 and ymm3/m256/m64bcst subject to write mask k1."
"VUNPCKLPD ymm1, ymm2, ymm3/m256","VUNPCKLPD ymm3/m256, ymm2, ymm1","vunpcklpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 14 /r","V","V","AVX","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VUNPCKLPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VUNPCKLPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vunpcklpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 14 /r","V","V","AVX512F","","","","","Unpacks and Interleaves double precision floating-point values from low quadwords of zmm2 and zmm3/m512/m64bcst subject to write mask k1."
"VUNPCKLPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VUNPCKLPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vunpcklps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 14 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of xmm2 and xmm3/mem and write result to xmm1 subject to write mask k1."
"VUNPCKLPS xmm1, xmm2, xmm3/m128","VUNPCKLPS xmm3/m128, xmm2, xmm1","vunpcklps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 14 /r","V","V","AVX","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VUNPCKLPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vunpcklps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 14 /r","V","V","AVX512VL AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of ymm2 and ymm3/mem and write result to ymm1 subject to write mask k1."
"VUNPCKLPS ymm1, ymm2, ymm3/m256","VUNPCKLPS ymm3/m256, ymm2, ymm1","vunpcklps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 14 /r","V","V","AVX","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VUNPCKLPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VUNPCKLPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vunpcklps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 14 /r","V","V","AVX512F","","","","","Unpacks and Interleaves single precision floating-point values from low quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to write mask k1."
"VXORPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","VXORPD xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","vxorpd xmm3/m128/m64bcst, xmm2, xmm1 {k1}{z}","EVEX.128.66.0F.W1 57 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical XOR of packed double precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VXORPD xmm1, xmm2, xmm3/m128","VXORPD xmm3/m128, xmm2, xmm1","vxorpd xmm3/m128, xmm2, xmm1","VEX.128.66.0F.WIG 57 /r","V","V","AVX","","","","","Return the bitwise logical XOR of packed double precision floating-point values in xmm2 and xmm3/mem."
"VXORPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","VXORPD ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","vxorpd ymm3/m256/m64bcst, ymm2, ymm1 {k1}{z}","EVEX.256.66.0F.W1 57 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical XOR of packed double precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VXORPD ymm1, ymm2, ymm3/m256","VXORPD ymm3/m256, ymm2, ymm1","vxorpd ymm3/m256, ymm2, ymm1","VEX.256.66.0F.WIG 57 /r","V","V","AVX","","","","","Return the bitwise logical XOR of packed double precision floating-point values in ymm2 and ymm3/mem."
"VXORPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","VXORPD zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","vxorpd zmm3/m512/m64bcst, zmm2, zmm1 {k1}{z}","EVEX.512.66.0F.W1 57 /r","V","V","AVX512DQ","","","","","Return the bitwise logical XOR of packed double precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"VXORPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","VXORPS xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","vxorps xmm3/m128/m32bcst, xmm2, xmm1 {k1}{z}","EVEX.128.0F.W0 57 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VXORPS xmm1, xmm2, xmm3/m128","VXORPS xmm3/m128, xmm2, xmm1","vxorps xmm3/m128, xmm2, xmm1","VEX.128.0F.WIG 57 /r","V","V","AVX","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in xmm2 and xmm3/mem."
"VXORPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","VXORPS ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","vxorps ymm3/m256/m32bcst, ymm2, ymm1 {k1}{z}","EVEX.256.0F.W0 57 /r","V","V","AVX512VL AVX512DQ","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VXORPS ymm1, ymm2, ymm3/m256","VXORPS ymm3/m256, ymm2, ymm1","vxorps ymm3/m256, ymm2, ymm1","VEX.256.0F.WIG 57 /r","V","V","AVX","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in ymm2 and ymm3/mem."
"VXORPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","VXORPS zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","vxorps zmm3/m512/m32bcst, zmm2, zmm1 {k1}{z}","EVEX.512.0F.W0 57 /r","V","V","AVX512DQ","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"WAIT","WAIT","wait","9B","V","V","","pseudo","","","","Check pending unmasked floating-point exceptions."
"WBINVD","WBINVD","wbinvd","0F 09","V","V","486","","","","","Write back and flush Internal caches; initiate writing-back and flushing of external caches."
"WRFSBASE r32","WRFSBASE r32","wrfsbase r32","F3 0F AE /2","I","V","FSGSBASE","operand16,operand32","","Y","32","Load the FS base address with the 32-bit value in the source register."
"WRFSBASE r64","WRFSBASE r64","wrfsbase r64","F3 REX.W 0F AE /2","I","V","FSGSBASE","","","Y","64","Load the FS base address with the 64-bit value in the source register."
"WRGSBASE r32","WRGSBASE r32","wrgsbase r32","F3 0F AE /3","I","V","FSGSBASE","operand16,operand32","","Y","32","Load the GS base address with the 32-bit value in the source register."
"WRGSBASE r64","WRGSBASE r64","wrgsbase r64","F3 REX.W 0F AE /3","I","V","FSGSBASE","","","Y","64","Load the GS base address with the 64-bit value in the source register."
"WRMSR","WRMSR","wrmsr","0F 30","V","V","Pentium","","","","","Write the value in EDX:EAX to MSR specified by ECX."
"WRPKRU","WRPKRU","wrpkru","NP 0F 01 EF","V","V","OSPKE","","","","","Writes EAX into PKRU."
"WRSSD m32, r32","WRSSD r32, m32","wrssd r32, m32","0F 38 F6 !(11):rrr:bbb","V","V","CET_SS","operand16,operand32","","","","Write 4 bytes to shadow stack."
"WRSSQ m64, r64","WRSSQ r64, m64","wrssq r64, m64","REX.W 0F 38 F6 !(11):rrr:bbb","N.E.","V","CET_SS","","","","","Write 8 bytes to shadow stack."
"WRUSSD m32, r32","WRUSSD r32, m32","wrussd r32, m32","66 0F 38 F5 !(11):rrr:bbb","V","V","CET_SS","operand16,operand32","","","","Write 4 bytes to shadow stack."
"WRUSSQ m64, r64","WRUSSQ r64, m64","wrussq r64, m64","66 REX.W 0F 38 F5 !(11):rrr:bbb","N.E.","V","CET_SS","","","","","Write 8 bytes to shadow stack."
"XABORT imm8","XABORT imm8","xabort imm8","C6 F8 ib","V","V","RTM","","r","","","Causes an RTM abort if in RTM execution."
"XACQUIRE","XACQUIRE","xacquire","F2","V","V","HLE","pseudo","","","","A hint used with an “XACQUIRE-enabled“ instruction to start lock elision on the instruction memory operand address."
"XADD r/m16, r16","XADDW r16, r/m16","xaddw r16, r/m16","0F C1 /r","V","V","","operand16","","Y","16","Exchange r16 and r/m16; load sum into r/m16."
"XADD r/m32, r32","XADDL r32, r/m32","xaddl r32, r/m32","0F C1 /r","V","V","","operand32","","Y","32","Exchange r32 and r/m32; load sum into r/m32."
"XADD r/m64, r64","XADDQ r64, r/m64","xaddq r64, r/m64","REX.W 0F C1 /r","N.E.","V","","","","Y","64","Exchange r64 and r/m64; load sum into r/m64."
"XADD r/m8, r8","XADDB r8, r/m8","xaddb r8, r/m8","0F C0 /r","V","V","","","","Y","8","Exchange r8 and r/m8; load sum into r/m8."
"XADD r/m8, r8","XADDB r8, r/m8","xaddb r8, r/m8","REX 0F C0 /r","N.E.","V","","pseudo64","","Y","8","Exchange r8 and r/m8; load sum into r/m8."
"XBEGIN rel16","XBEGIN rel16","xbegin rel16","C7 F8","V","V","RTM","operand16","","","","Specifies the start of an RTM region. Provides a 16-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort."
"XBEGIN rel32","XBEGIN rel32","xbegin rel32","C7 F8","V","V","RTM","operand32","","","","Specifies the start of an RTM region. Provides a 32-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort."
"XCHG AX, r16op","XCHGW r16op, AX","xchgw r16op, AX","90+rw","V","V","","operand16,pseudo","rw,rw","Y","16","Exchange r16 with AX."
"XCHG EAX, r32op","XCHGL r32op, EAX","xchgl r32op, EAX","90+rd","V","V","","operand32,pseudo","rw,rw","Y","32","Exchange r32 with EAX."
"XCHG RAX, r64op","XCHGQ r64op, RAX","xchgq r64op, RAX","REX.W 90+rd","N.E.","V","","pseudo","rw,rw","Y","64","Exchange r64 with RAX."
"XCHG r/m16, r16","XCHGW r16, r/m16","xchgw r16, r/m16","87 /r","V","V","","operand16","","Y","16","Exchange r16 with word from r/m16."
"XCHG r/m32, r32","XCHGL r32, r/m32","xchgl r32, r/m32","87 /r","V","V","","operand32,pseudo","","Y","32","Exchange r32 with doubleword from r/m32."
"XCHG r/m64, r64","XCHGQ r64, r/m64","xchgq r64, r/m64","REX.W 87 /r","N.E.","V","","pseudo","","Y","64","Exchange r64 with quadword from r/m64."
"XCHG r/m8, r8","XCHGB r8, r/m8","xchgb r8, r/m8","86 /r","V","V","","pseudo","","Y","8","Exchange r8 (byte register) with byte from r/m8."
"XCHG r/m8, r8","XCHGB r8, r/m8","xchgb r8, r/m8","REX 86 /r","N.E.","V","","pseudo","","Y","8","Exchange r8 (byte register) with byte from r/m8."
"XCHG r16, AX","XCHGW AX, r16","xchgw AX, r16","90+rw","V","V","","operand16","","Y","16","Exchange AX with r16."
"XCHG r16, r/m16","XCHGW r/m16, r16","xchgw r/m16, r16","87 /r","V","V","","operand16,pseudo","","Y","16","Exchange word from r/m16 with r16."
"XCHG r32, EAX","XCHGL EAX, r32","xchgl EAX, r32","90+rd","V","V","","operand32","","Y","32","Exchange EAX with r32."
"XCHG r32, r/m32","XCHGL r/m32, r32","xchgl r/m32, r32","87 /r","V","V","","operand32","","Y","32","Exchange doubleword from r/m32 with r32."
"XCHG r64, RAX","XCHGQ RAX, r64","xchgq RAX, r64","REX.W 90+rd","N.E.","V","","","","Y","64","Exchange RAX with r64."
"XCHG r64, r/m64","XCHGQ r/m64, r64","xchgq r/m64, r64","REX.W 87 /r","N.E.","V","","","","Y","64","Exchange quadword from r/m64 with r64."
"XCHG r8, r/m8","XCHGB r/m8, r8","xchgb r/m8, r8","86 /r","V","V","","","","Y","8","Exchange byte from r/m8 with r8 (byte register)."
"XCHG r8, r/m8","XCHGB r/m8, r8","xchgb r/m8, r8","REX 86 /r","N.E.","V","","pseudo64","","Y","8","Exchange byte from r/m8 with r8 (byte register)."
"XEND","XEND","xend","NP 0F 01 D5","V","V","RTM","","","","","Specifies the end of an RTM code region."
"XGETBV","XGETBV","xgetbv","NP 0F 01 D0","V","V","","","","","","Reads an XCR specified by ECX into EDX:EAX."
"XLATB","XLAT","xlat","D7","V","V","","ignoreREXW","","","","Set AL to memory byte DS:[(E)BX + unsigned AL]."
"XLATB","XLAT","xlat","REX.W D7","N.E.","V","","pseudo","","","","Set AL to memory byte [RBX + unsigned AL]."
"XOR AL, imm8","XORB imm8, AL","xorb imm8, AL","34 ib","V","V","","","","Y","8","AL XOR imm8."
"XOR AX, imm16","XORW imm16, AX","xorw imm16, AX","35 iw","V","V","","operand16","","Y","16","AX XOR imm16."
"XOR EAX, imm32","XORL imm32, EAX","xorl imm32, EAX","35 id","V","V","","operand32","","Y","32","EAX XOR imm32."
"XOR RAX, imm32","XORQ imm32, RAX","xorq imm32, RAX","REX.W 35 id","N.E.","V","","","","Y","64","RAX XOR imm32 (sign-extended)."
"XOR r/m16, imm16","XORW imm16, r/m16","xorw imm16, r/m16","81 /6 iw","V","V","","operand16","","Y","16","r/m16 XOR imm16."
"XOR r/m16, imm8","XORW imm8, r/m16","xorw imm8, r/m16","83 /6 ib","V","V","","operand16","","Y","16","r/m16 XOR imm8 (sign-extended)."
"XOR r/m16, r16","XORW r16, r/m16","xorw r16, r/m16","31 /r","V","V","","operand16","","Y","16","r/m16 XOR r16."
"XOR r/m32, imm32","XORL imm32, r/m32","xorl imm32, r/m32","81 /6 id","V","V","","operand32","","Y","32","r/m32 XOR imm32."
"XOR r/m32, imm8","XORL imm8, r/m32","xorl imm8, r/m32","83 /6 ib","V","V","","operand32","","Y","32","r/m32 XOR imm8 (sign-extended)."
"XOR r/m32, r32","XORL r32, r/m32","xorl r32, r/m32","31 /r","V","V","","operand32","","Y","32","r/m32 XOR r32."
"XOR r/m64, imm32","XORQ imm32, r/m64","xorq imm32, r/m64","REX.W 81 /6 id","N.E.","V","","","","Y","64","r/m64 XOR imm32 (sign-extended)."
"XOR r/m64, imm8","XORQ imm8, r/m64","xorq imm8, r/m64","REX.W 83 /6 ib","N.E.","V","","","","Y","64","r/m64 XOR imm8 (sign-extended)."
"XOR r/m64, r64","XORQ r64, r/m64","xorq r64, r/m64","REX.W 31 /r","N.E.","V","","","","Y","64","r/m64 XOR r64."
"XOR r/m8, imm8","XORB imm8, r/m8","xorb imm8, r/m8","80 /6 ib","V","V","","","","Y","8","r/m8 XOR imm8."
"XOR r/m8, imm8","XORB imm8, r/m8","xorb imm8, r/m8","REX 80 /6 ib","N.E.","V","","pseudo64","","Y","8","r/m8 XOR imm8."
"XOR r/m8, r8","XORB r8, r/m8","xorb r8, r/m8","30 /r","V","V","","","","Y","8","r/m8 XOR r8."
"XOR r/m8, r8","XORB r8, r/m8","xorb r8, r/m8","REX 30 /r","N.E.","V","","pseudo64","","Y","8","r/m8 XOR r8."
"XOR r16, r/m16","XORW r/m16, r16","xorw r/m16, r16","33 /r","V","V","","operand16","","Y","16","r16 XOR r/m16."
"XOR r32, r/m32","XORL r/m32, r32","xorl r/m32, r32","33 /r","V","V","","operand32","","Y","32","r32 XOR r/m32."
"XOR r64, r/m64","XORQ r/m64, r64","xorq r/m64, r64","REX.W 33 /r","N.E.","V","","","","Y","64","r64 XOR r/m64."
"XOR r8, r/m8","XORB r/m8, r8","xorb r/m8, r8","32 /r","V","V","","","","Y","8","r8 XOR r/m8."
"XOR r8, r/m8","XORB r/m8, r8","xorb r/m8, r8","REX 32 /r","N.E.","V","","pseudo64","","Y","8","r8 XOR r/m8."
"XORPD xmm1, xmm2/m128","XORPD xmm2/m128, xmm1","xorpd xmm2/m128, xmm1","66 0F 57 /r","V","V","SSE2","","","","","Return the bitwise logical XOR of packed double precision floating-point values in xmm1 and xmm2/mem."
"XORPS xmm1, xmm2/m128","XORPS xmm2/m128, xmm1","xorps xmm2/m128, xmm1","NP 0F 57 /r","V","V","SSE","","","","","Return the bitwise logical XOR of packed single- precision floating-point values in xmm1 and xmm2/mem."
"XRELEASE","XRELEASE","xrelease","F3","V","V","HLE","pseudo","","","","A hint used with an “XRELEASE-enabled“ instruction to end lock elision on the instruction memory operand address."
"XSAVEOPT mem","XSAVEOPT mem","xsaveopt mem","NP 0F AE /6","V","V","XSAVEOPT","operand16,operand32","","","","Save state components specified by EDX:EAX to mem, optimizing if possible."
"XSAVEOPT64 mem","XSAVEOPT64 mem","xsaveopt64 mem","NP REX.W 0F AE /6","V","V","XSAVEOPT","","","","","Save state components specified by EDX:EAX to mem, optimizing if possible."
"XSETBV","XSETBV","xsetbv","NP 0F 01 D1","V","V","","","","","","Write the value in EDX:EAX to the XCR specified by ECX."
"XTEST","XTEST","xtest","NP 0F 01 D6","V","V","HLE or RTM","","","","","Test if executing in a transactional region."
